{"paper_id": "u9hnCwX99I1", "title": "Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning", "keywords": "['Multi-Agent Reinforcement Learning']", "abstract": "We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully perform cooperative tasks with any communication level at execution time by taking advantage of information-sharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully decentralized), to a setting featuring full communication (fully centralized). To formalize our setting, we define a new class of multi-agent partially observable Markov decision processes (POMDPs) that we name hybrid-POMDPs, which explicitly models a communication process between the agents. We contribute MARO, an approach that combines an autoregressive predictive model to estimate missing agents' observations, and a dropout-based RL training scheme that simulates different communication levels during the centralized training phase. We evaluate MARO on standard scenarios and extensions of previous benchmarks tailored to emphasize the negative impact of partial observability in MARL. Experimental results show that our method consistently outperforms baselines, allowing agents to act with faulty communication while successfully exploiting shared information.", "ratings": "[5, 5, 5, 5]", "confidences": "[4, 4, 4, 5]", "decision": "Reject", "year": "2023"}
{"paper_id": "UOOmHiXetC", "title": "Structure and randomness in planning and reinforcement learning", "keywords": "reinforcement learning, uncertainty, model-based, MCTS", "abstract": "Planning in large state spaces inevitably needs to balance depth and breadth of the search. It has a crucial impact on planners performance and most manage this interplay implicitly. We present a novel method $\\textit{Shoot Tree Search (STS)}$, which makes it possible to control this trade-off more explicitly. Our algorithm can be understood as an interpolation between two celebrated search mechanisms: MCTS and random shooting. It also lets the user control the bias-variance trade-off, akin to $TD(n)$, but in the tree search context.              In experiments on challenging domains, we show that STS can get the best of both worlds consistently achieving higher scores.", "ratings": "[3.0, 4.0, 6.0, 3.0, 6.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "X4DOJ-wL2I", "title": "SDAC: Efficient Safe Reinforcement Learning with Low-Biased Distributional Actor-Critic", "keywords": "['Reinforcement learning', 'Safety', 'Distributional Critic']", "abstract": "To apply reinforcement learning (RL) to real-world practical applications, agents are required to adhere to the safety guidelines of their respective domains. Safe RL can effectively handle the guidelines by maximizing returns while maintaining safety satisfaction. In this paper, we develop a safe distributional RL method based on the trust region method which has the capability of satisfying safety constraints consistently. However, importance sampling required for the trust region method can hinder performance due to its significant variance, and policies may not meet the safety guidelines due to the estimation bias of distributional critics. Hence, we enhance safety performance through the following approaches. First, we propose novel surrogates for the trust region method expressed with Q-functions using the reparameterization trick. Second, we utilize distributional critics trained with a target distribution where bias-variance can be traded off. In addition, if an initial policy violates safety constraints, there can be no policy satisfying safety constraints within the trust region. Thus, we propose a gradient integration method which is guaranteed to find a policy satisfying multiple constraints from an unsafe initial policy. From extensive experiments, the proposed method shows minimal constraint violations while achieving high returns compared to existing safe RL methods. Furthermore, we demonstrate the benefit of safe RL for problems in which the reward function cannot be easily specified.", "ratings": "[8, 5, 3, 6]", "confidences": "[3, 3, 4, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "UXwlFxVWks", "title": "Divergent representations of ethological visual inputs emerge from supervised, unsupervised, and reinforcement learning", "keywords": "['reinforcement learning', 'transfer learning', 'representations', 'dimensionality', 'sparsity', 'RSA']", "abstract": "Artificial neural systems trained using reinforcement, supervised, and unsupervised learning all acquire internal representations of high dimensional input. To what extent these representations depend on the different learning objectives is largely unknown. Here we compare the representations learned by eight different convolutional neural networks, each with identical ResNet architectures and trained on the same family of egocentric images, but embedded within different learning systems. Specifically, the representations are trained to guide action in a compound reinforcement learning task; to predict one or a combination of three task-related targets with supervision; or using one of three different unsupervised objectives. Using representational similarity analysis, we find that the network trained with reinforcement learning differs most from the other networks. Through further analysis using metrics inspired by the neuroscience literature, we find that the model trained with reinforcement learning has a high-dimensional representation wherein individual images are represented with very different patterns of neural activity. These representations seem to arise in order to guide long-term behavior and goal-seeking in the RL agent. Our results provide insights into how the properties of neural representations are influenced by objective functions and can inform transfer learning approaches.", "ratings": "[3.0, 3.0, 3.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "jlVNBPEDynH", "title": "Neuro-algorithmic Policies for Discrete Planning", "keywords": "planning, reinforcement learning, combinatorial optimization, control, imitation learning", "abstract": "Although model-based and model-free approaches to learning control of systems have achieved impressive results on standard benchmarks, most have been shown to be lacking in their generalization capabilities. These methods usually require sampling an exhaustive amount of data from different environment configurations.              We introduce a neuro-algorithmic policy architecture with the ability to plan consisting of a model working in unison with a shortest path solver to predict trajectories with low way-costs. These policies can be trained end-to-end by blackbox differentiation. We show that this type of hybrid architectures generalize well to unseen environment configurations.              https://sites.google.com/view/neuro-algorithmic", "ratings": "[4.0, 3.0, 3.0, 7.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "_zHHAZOLTVh", "title": "A Maximum Mutual Information Framework for Multi-Agent Reinforcement Learning", "keywords": "Multi-agent reinforcement learning, coordination, mutual information", "abstract": "In this paper, we propose a maximum mutual information (MMI) framework for multi-agent reinforcement learning (MARL) to enable multiple agents to learn coordinated behaviors by regularizing the accumulated return with the mutual information between actions. By introducing a latent variable to induce nonzero mutual information between actions and applying a variational bound, we derive a tractable lower bound on the considered MMI-regularized objective function. Applying policy iteration to maximize the derived lower bound, we propose a practical algorithm named variational maximum mutual information multi-agent actor-critic (VM3-AC), which follows centralized learning with decentralized execution (CTDE). We evaluated VM3-AC for several games requiring coordination, and numerical results show that VM3-AC outperforms MADDPG and other MARL algorithms in multi-agent tasks requiring coordination.", "ratings": "[6.0, 6.0, 5.0, 3.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "xbx7Hxjbd79", "title": "COLA: Consistent Learning with Opponent-Learning Awareness", "keywords": "['Differentiable games', 'multi-agent reinforcement learning', 'general-sum games', 'lola']", "abstract": "Optimization problems with multiple, interdependent losses, such as Generative Adversarial Networks (GANs) or multi-agent RL, are commonly formalized as differentiable games.          Learning with Opponent-Learning Awareness (LOLA) introduced opponent shaping to this setting. More specifically, LOLA introduced an augmented learning rule that accounts for the agent's influence on the anticipated learning step of the other agents. However, the original LOLA formulation is inconsistent because LOLA models other agents as naive learners rather than LOLA agents.          In previous work, this inconsistency was stated to be the root cause of LOLA's failure to preserve stable fixed points (SFPs). We provide a counterexample by investigating cases where Higher-Order LOLA (HOLA) converges.          Furthermore, we show that, contrary to claims made, Competitive Gradient Descent (CGD) does not solve the consistency problem.         Next, we propose a new method called Consistent LOLA (COLA), which learns update functions that are consistent under mutual opponent shaping. Lastly, we empirically compare the performance and consistency of HOLA, LOLA, and COLA on a set of general-sum learning games.", "ratings": "[3.0, 8.0, 6.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "Su_HbZ0Sdz", "title": "Feasible Adversarial Robust Reinforcement Learning for Underspecified Environments", "keywords": "['reinforcement learning', 'robust rl', 'sim-to-real', 'game-theory', 'psro']", "abstract": "Robust reinforcement learning (RL) considers the problem of learning policies that perform well in the worst case among a set of possible environment parameter values. In real-world environments, choosing the set of possible values for robust RL can be a difficult task. When that set is specified too narrowly, the agent will be left vulnerable to reasonable parameter values unaccounted for. When specified too broadly, the agent will be too cautious. In this paper, we propose Feasible Adversarial Robust RL (FARR), a novel problem formulation and objective for automatically determining the set of environment parameter values over which to be robust. FARR implicitly defines the set of feasible parameter values as those on which an agent could achieve a benchmark reward given enough training resources. By formulating this problem as a two-player zero-sum game, optimizing the FARR objective jointly produces an adversarial distribution over parameter values with feasible support and a policy robust over this feasible parameter set. We demonstrate that approximate Nash equilibria for this objective can be found using a variation of the PSRO algorithm. Furthermore, we show that an optimal agent trained with FARR is more robust to feasible adversarial parameter selection than with existing minimax, domain-randomization, and regret objectives in a parameterized gridworld and three MuJoCo control environments.", "ratings": "[8, 5, 3, 3]", "confidences": "[4, 4, 5, 5]", "decision": "Reject", "year": "2023"}
{"paper_id": "CPfjKI8Yzx", "title": "Robust Imitation via Decision-Time Planning", "keywords": "imitation learning, reinforcement learning, inverse reinforcement learning", "abstract": "The goal of imitation learning is to mimic expert behavior from demonstrations, without access to an explicit reward signal. A popular class of approaches infers the (unknown) reward function via inverse reinforcement learning (IRL) followed by maximizing this reward function via reinforcement learning (RL). The policies learned via these approaches are however very brittle in practice and deteriorate quickly even with small test-time perturbations due to compounding errors. We propose Imitation with Planning at Test-time (IMPLANT), a new algorithm for imitation learning that utilizes decision-time planning to correct for compounding errors of any base imitation policy. In contrast to existing approaches, we retain both the imitation policy and the rewards model at decision-time, thereby benefiting from the learning signal of the two components. Empirically, we demonstrate that IMPLANT significantly outperforms benchmark imitation learning approaches on standard control environments and excels at zero-shot generalization when subject to challenging perturbations in test-time dynamics.", "ratings": "[4.0, 4.0, 6.0, 3.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "f6CQliwyra", "title": "A Free Lunch from the Noise: Provable and Practical Exploration for Representation Learning", "keywords": "['representation learning', 'reinforcement learning']", "abstract": "Representation learning lies at the heart of the empirical success of deep learning for dealing with the curse of dimensionality. However, the power of representation learning has not been fully exploited yet in reinforcement learning (RL), due to i), the trade-off between expressiveness and tractability; and ii), the coupling between exploration and representation learning. In this paper, we first reveal the fact that under some noise assumption in the stochastic control model, we can obtain the linear spectral feature of its corresponding Markov transition operator in closed-form for free. Based on this observation, we propose Spectral Dynamics Embedding (SPEDE), which breaks the trade-off and completes optimistic exploration for representation learning by exploiting the structure of the noise. We provide rigorous theoretical analysis of SPEDE, and demonstrate the practical superior performance over the existing state-of-the-art empirical algorithms on several benchmarks.", "ratings": "[5.0, 8.0, 5.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "rSwTMomgCz", "title": "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices", "keywords": "meta-reinforcement learning, reinforcement learning, exploration", "abstract": "The goal of meta-reinforcement learning (meta-RL) is to build agents that can quickly learn new tasks by leveraging prior experience on related tasks. Learning a new task often requires both exploring to gather task-relevant information and exploiting this information to solve the task. In principle, optimal exploration and exploitation can be learned end-to-end by simply maximizing task performance. However, such meta-RL approaches struggle with local optima due to a chicken-and-egg problem: learning to explore requires good exploitation to gauge the exploration's utility, but learning to exploit requires information gathered via exploration. Optimizing separate objectives for exploration and exploitation can avoid this problem, but prior meta-RL exploration objectives yield suboptimal policies that gather information irrelevant to the task. We alleviate both concerns by constructing an exploitation objective that automatically identifies task-relevant information and an exploration objective to recover only this information. This avoids local optima in end-to-end training, without sacrificing optimal exploration. Empirically, DREAM substantially outperforms existing approaches on complex meta-RL problems, such as sparse-reward 3D visual navigation.", "ratings": "[5.0, 4.0, 6.0, 7.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "ZN3s7fN-bo", "title": "Interactive Visualization for Debugging RL", "keywords": "Reinforcement Learning, Interpretability, Visualization", "abstract": "Visualization tools for supervised learning (SL) allow users to interpret, introspect, and gain an intuition for the successes and failures of their models. While reinforcement learning (RL) practitioners ask many of the same questions while debugging agent policies, existing tools aren't a great fit for the RL setting as these tools address challenges typically found in the SL regime. Whereas SL involves a static dataset, RL often entails collecting new data in challenging environments with partial observability, stochasticity, and non-stationary data distributions. This necessitates the creation of alternate visual interfaces to help us better understand agent policies trained using RL. In this work, we design and implement an interactive visualization tool for debugging and interpreting RL. Our system identifies and addresses important aspects missing from existing tools such as (1) visualizing alternate state representations (different from those seen by the agent) that researchers could use while debugging RL policies; (2) interactive interfaces tailored to metadata stored while training RL agents (3) a conducive workflow designed around RL policy debugging. We provide an example workflow of how this system could be used, along with ideas for future extensions.", "ratings": "[6.0, 3.0, 4.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "nlWgE3A-iS", "title": "ReaPER: Improving Sample Efficiency in Model-Based Latent Imagination", "keywords": "model-based reinforcement learning, visual control, sample efficiency", "abstract": "Deep Reinforcement Learning (DRL) can distill behavioural policies from sensory input that solve complex tasks, however, the policies tend to be task-specific and sample inefficient, requiring a large number of interactions with the environment that may be costly or impractical for many real world applications. Model-based DRL (MBRL) can allow learned behaviours and dynamics from one task to be translated to a new task in a related environment, but still suffer from low sample efficiency. In this work we introduce ReaPER, an algorithm that addresses the sample efficiency challenge in model-based DRL, we illustrate the power of the proposed solution on the DeepMind Control benchmark. Our improvements are driven by sparse , self-supervised, contrastive model representations and efficient use of past experience. We empirically analyze each novel  component of ReaPER and analyze how they contribute to sample efficiency. We also illustrate how other standard alternatives fail to improve upon previous methods. Code will be made available.", "ratings": "[4.0, 5.0, 6.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "lpxeg8dhJ-", "title": "Training Equilibria in Reinforcement Learning", "keywords": "['theory', 'reinforcement learning', 'learning dynamics', 'partial observability', 'MDP', 'POMDP', 'markov decision processes']", "abstract": "In partially observable environments, reinforcement learning algorithms such as policy gradient and Q-learning may have multiple equilibria---policies that are stable under further training---and can converge to policies that are strictly suboptimal. Prior work blames insufficient exploration, but suboptimal equilibria can arise despite full exploration and other favorable circumstances like a flexible policy parametrization. We show theoretically that the core problem is that in partially observed environments, an agent's past actions induce a distribution on hidden states. Equipping the policy with memory helps it model the hidden state and leads to convergence to a higher reward equilibrium, \\emph{even when there exists a memoryless optimal policy}. Experiments show that policies with insufficient memory tend to learn to use the environment as auxiliary memory,and parameter noise helps policies escape suboptimal equilibria. ", "ratings": "[3, 3, 6, 5]", "confidences": "[3, 4, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "eVzy-BWKY6Z", "title": "Edge Rewiring Goes Neural: Boosting Network Resilience via Policy Gradient", "keywords": "['network resilience', 'neural combinatorial optimization', 'graph neural networks', 'reinforcement learning']", "abstract": "Improving the resilience of a network protects the system from natural disasters and malicious attacks.         This is typically achieved by introducing new edges, which however may reach beyond the maximum number of connections a node could sustain.         Many studies then resort to the degree-preserving operation of rewiring, which swaps existing edges $AC, BD$ to new edges $AB, CD$.         A significant line of studies focuses on this technique for theoretical and practical results while leaving three limitations: network utility loss, local optimality, and transductivity.          In this paper, we propose ResiNet, a reinforcement learning (RL)-based framework to discover Resilient Network topologies against various disasters and attacks.          ResiNet is objective agnostic which allows the utility to be balanced by incorporating it into the objective function.         The local optimality, typically seen in greedy algorithms, is addressed by casting the cumulative resilience gain into a sequential decision process of step-wise rewiring.         The transductivity, which refers to the necessity to run a computationally intensive optimization for each input graph, is lifted by our variant of RL with auto-regressive permutation-invariant variable action space.         ResiNet is armed by our technical innovation, Filtration enhanced GNN (FireGNN), which distinguishes graphs with minor differences.         It is thus possible for ResiNet to capture local structure changes and adapt its decision among consecutive graphs, which is known to be infeasible for GNN.         Extensive experiments demonstrate that with a small number of rewiring operations, ResiNet achieves a near-optimal resilience gain on multiple graphs while balancing the utility, with a large margin compared to existing approaches.", "ratings": "[3.0, 6.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "VbCVU10R7K", "title": "Offline policy selection under Uncertainty", "keywords": "Off-policy selection, reinforcement learning, Bayesian inference", "abstract": "The presence of uncertainty in policy evaluation  significantly complicates the process of policy ranking and selection in real-world settings. We formally consider offline policy selection as learning preferences over a set of policy prospects given a fixed experience dataset. While one can select or rank policies based on point estimates of their policy values or high-confidence intervals, access to the full distribution over one's belief of the policy value enables more flexible selection algorithms under a wider range of downstream evaluation metrics. We propose BayesDICE for estimating this belief distribution in terms of posteriors of distribution correction ratios derived from stochastic constraints (as opposed to explicit likelihood, which is not available). Empirically, BayesDICE is highly competitive to existing state-of-the-art approaches in confidence interval estimation. More importantly, we show how the belief distribution estimated by BayesDICE may be used to rank policies with respect to any arbitrary downstream policy selection metric, and we empirically demonstrate that this selection procedure significantly outperforms existing approaches, such as ranking policies according to mean or high-confidence lower bound value estimates.", "ratings": "[6.0, 6.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "rVdLv-uzYup", "title": "Joint Perception and Control as Inference with an Object-based Implementation", "keywords": "model-based reinforcement learning, perception modeling, object-based reinforcement learning", "abstract": "Existing model-based reinforcement learning methods often study perception modeling and decision making separately. We introduce joint Perception and Control as Inference (PCI), a general framework to combine perception and control for partially observable environments through Bayesian inference. Based on the fact that object-level inductive biases are critical in human perceptual learning and reasoning, we propose Object-based Perception Control (OPC), an instantiation of PCI which manages to facilitate control using automatic discovered object-based representations. We develop an unsupervised end-to-end solution and analyze the convergence of the perception model update. Experiments in a high-dimensional pixel environment demonstrate the learning effectiveness of our object-based perception control approach. Specifically, we show that OPC achieves good perceptual grouping quality and outperforms several strong baselines in accumulated rewards.", "ratings": "[4.0, 4.0, 5.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "TJzkxFw-mGm", "title": "Near-Optimal Regret Bounds for Model-Free RL in Non-Stationary Episodic MDPs", "keywords": "reinforcement learning, non-stationary environment, model-free approach, regret analysis", "abstract": "We consider model-free reinforcement learning (RL) in non-stationary Markov decision processes (MDPs). Both the reward functions and the state transition distributions are allowed to vary over time, either gradually or abruptly, as long as their cumulative variation magnitude does not exceed certain budgets. We propose an algorithm, named Restarted Q-Learning with Upper Confidence Bounds (RestartQ-UCB), for this setting, which adopts a simple restarting strategy and an extra optimism term. Our algorithm outperforms the state-of-the-art (model-based) solution in terms of dynamic regret. Specifically, RestartQ-UCB with Freedman-type bonus terms achieves a dynamic regret of $\\widetilde{O}(S^{\\frac{1}{3}} A^{\\frac{1}{3}} \\Delta^{\\frac{1}{3}} H T^{\\frac{2}{3}})$, where $S$ and $A$ are the numbers of states and actions, respectively, $\\Delta>0$ is the variation budget, $H$ is the number of steps per episode, and $T$ is the total number of steps. We further show that our algorithm is near-optimal by establishing an information-theoretical lower bound of $\\Omega(S^{\\frac{1}{3}} A^{\\frac{1}{3}} \\Delta^{\\frac{1}{3}} H^{\\frac{2}{3}} T^{\\frac{2}{3}})$, which to the best of our knowledge is the first impossibility result in non-stationary RL in general.", "ratings": "[7.0, 4.0, 4.0, 7.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "xl2-MIX2DCD", "title": "Reward Learning with Trees: Methods and Evaluation", "keywords": "['reinforcement learning', 'reward learning', 'alignment', 'human-agent interaction', 'explainable AI', 'XAI', 'interpretability', 'decision trees']", "abstract": "Recent efforts to learn reward functions from human feedback have tended to use deep neural networks, whose lack of transparency hampers our ability to explain agent behaviour or verify alignment. We explore the merits of learning intrinsically interpretable tree models instead. We develop a recently proposed method for learning reward trees from preference labels, and show it to be broadly competitive with neural networks on challenging high-dimensional tasks, with good robustness to limited or corrupted data. Having found that reward tree learning can be done effectively in complex settings, we then consider why it should be used, demonstrating that the interpretable reward structure gives significant scope for traceability, verification and explanation.", "ratings": "[5, 6, 3]", "confidences": "[3, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "-5fSvp1ofdd", "title": "Memory of Unimaginable Outcomes in Experience Replay", "keywords": "['Transfer Multitask and Meta-learning', 'Robotics', 'Model-Based Reinforcement Learning', 'Batch/Offline RL', 'Deep RL', 'Continuous Action RL']", "abstract": "Model-based reinforcement learning (MBRL) applies a single-shot dynamics model to imagined actions to select those with best expected outcome. The dynamics model is an unfaithful representation of the environment physics, and its capacity to predict the outcome of a future action varies as it is trained iteratively. An experience replay buffer collects the outcomes of all actions executed in the environment and is used to iteratively train the dynamics model. With growing experience, it is expected that the model becomes more accurate at predicting the outcome and expected reward of imagined actions. However, training times and memory requirements drastically increase with the growing collection of experiences. Indeed, it would be preferable to retain only those experiences that could not be anticipated by the model while interacting with the environment. We argue that doing so results in a lean replay buffer with diverse experiences that correspond directly to the model's predictive weaknesses at a given point in time. We propose strategies for: i) determining reliable predictions of the dynamics model with respect to the imagined actions, ii) retaining only the unimaginable experiences in the replay buffer, and iii) training further only when sufficient novel experience has been acquired. We show that these contributions lead to lower training times, drastic reduction of the replay buffer size, fewer updates to the dynamics model and reduction of catastrophic forgetting. All of which enable the effective implementation of continual-learning agents using MBRL.", "ratings": "[3, 3, 3, 3]", "confidences": "[3, 3, 4, 2]", "decision": "Reject", "year": "2023"}
{"paper_id": "C3ukgkqJuh0", "title": "Reinforcement learning for instance segmentation with high-level priors", "keywords": "['Instance Segmentation', 'Reinforcement Learning', 'Biomedical Imaging']", "abstract": "Instance segmentation is a fundamental computer vision problem which remains challenging despite impressive recent advances due to deep learning-based methods. Given sufficient training data, fully supervised methods can yield excellent performance, but annotation of groundtruth data remains a major bottleneck, especially for biomedical applications where it has to be performed by domain experts. The amount of labels required can be drastically reduced by using rules derived from prior knowledge to guide the segmentation. However, these rules are in general not differentiable and thus cannot be used with existing methods. Here, we revoke this requirement by using stateless actor critic reinforcement learning, which enables non-differentiable rewards. We formulate the instance segmentation problem as graph partitioning and the actor critic predicts the edge weights driven by the rewards, which are based on the conformity of segmented instances to high-level priors on object shape, position or size. The experiments on toy and real data demonstrate that a good set of priors is sufficient to reach excellent performance without any direct object-level supervision.", "ratings": "[5, 5, 5]", "confidences": "[4, 2, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "7hMenh--8g", "title": "Uncertainty Weighted Offline Reinforcement Learning", "keywords": "reinforcement learning, offline, batch reinforcement learning, off-policy, uncertainty estimation, dropout, actor-critic, bootstrap error", "abstract": "Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that models the epistemic uncertainty to detect OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.", "ratings": "[4.0, 6.0, 7.0, 8.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "PAKkOriJBd", "title": "Coordination Scheme Probing for Generalizable Multi-Agent Reinforcement Learning", "keywords": "['reinforcement learning', 'multi-agent reinforcement learning', 'agent modeling']", "abstract": "Coordinating with previously unknown teammates without joint learning is a crucial need for real-world multi-agent applications, such as human-AI interaction. An active research topic on this problem is ad hoc teamwork, which improves agents' coordination ability in zero-shot settings. However, previous works can only solve the problem of a single agent's coordination with different teams, which is not in line with arbitrary group-to-group coordination in complex multi-agent scenarios. Moreover, they commonly suffer from limited adaptation ability within an episode in a zero-shot setting. To address these problems, we introduce the Coordination Scheme Probing (CSP) approach that applies a disentangled scheme probing module to represent and classify the newly arrived teammates beforehand with limited pre-collected episodic data and makes multi-agent control accordingly. To achieve generalization, CSP learns a meta-policy with multiple sub-policies that follow distinguished coordination schemes in an end-to-end fashion and automatically reuses it to coordinate with unseen teammates. Empirically, we show that the proposed method achieves remarkable performance compared to existing ad hoc teamwork and policy generalization methods in various multi-agent cooperative scenarios.", "ratings": "[5, 6, 8, 3]", "confidences": "[3, 3, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "tyyNcEVrklJ", "title": "Decentralized Policy Optimization", "keywords": "['multi-agent reinforcement learning']", "abstract": "The study of decentralized learning or independent learning in cooperative multi-agent reinforcement learning has a history of decades. Recently empirical studies show that independent PPO (IPPO) can obtain good performance, close to or even better than the methods of centralized training with decentralized execution, in several benchmarks. However, decentralized actor-critic with convergence guarantee is still open. In this paper, we propose decentralized policy optimization (DPO), a decentralized actor-critic algorithm with monotonic improvement and convergence guarantee. We derive a novel decentralized surrogate for policy optimization such that the monotonic improvement of joint policy can be guaranteed by each agent independently optimizing the surrogate. In practice, this decentralized surrogate can be realized by two adaptive coefficients for policy optimization at each agent. Empirically, we compare DPO with IPPO in a variety of cooperative multi-agent tasks, covering discrete and continuous action spaces, and fully and partially observable environments. The results show DPO outperforms IPPO in most tasks, which can be the evidence for our theoretical results.", "ratings": "[3, 3, 3]", "confidences": "[5, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "dut7suZoRqv", "title": "SparRL: Graph Sparsification via Deep Reinforcement Learning", "keywords": "['graph sparsification', 'graph theory', 'machine learning', 'reinforcement learning']", "abstract": "Graph sparsification concerns data reduction where an edge-reduced graph of a similar structure is preferred. Existing methods are mostly sampling-based, which introduce high computation complexity in general and lack of flexibility for a different reduction objective. We present SparRL, the first general and effective reinforcement learning-based framework for graph sparsification. SparRL can easily adapt to different reduction goals and promise graph-size-independent complexity. Extensive experiments show that SparRL outperforms all prevailing sparsification methods in producing high-quality sparsified graphs concerning a variety of objectives. As graph representations are very versatile, SparRL carries the potential for a broad impact.", "ratings": "[3.0, 3.0, 6.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "d5IQ3k7ed__", "title": "Finding General Equilibria in Many-Agent Economic Simulations using Deep Reinforcement Learning", "keywords": "['reinforcement learning', 'economics', 'simulation', 'multi-agent RL', 'equilibrium']", "abstract": "Real economies can be seen as a sequential imperfect-information game with many heterogeneous, interacting strategic agents of various agent types, such as consumers, firms, and governments. Dynamic general equilibrium models are common economic tools to model the economic activity, interactions, and outcomes in such systems. However, existing analytical and computational methods struggle to find explicit equilibria when all agents are strategic and interact, while joint learning is unstable and challenging. Amongst others, a key reason is that the actions of one economic agent may change the reward function of another agent, e.g., a consumer's expendable income changes when firms change prices or governments change taxes. We show that multi-agent deep reinforcement learning (RL) can discover stable solutions that are $\\epsilon$-Nash equilibria for a meta-game over agent types, in economic simulations with many agents, through the use of structured learning curricula and efficient GPU-only simulation and training.Conceptually, our approach is more flexible and does not need unrealistic assumptions, e.g., market clearing, that are commonly used for analytical tractability. Our GPU implementation enables training and analyzing economies with a large number of agents within reasonable time frames, e.g., training completes within a day. We demonstrate our approach in real-business-cycle models, a representative family of DGE models, with 100 worker-consumers, 10 firms, and a government who taxes and redistributes. We validate the learned meta-game $\\epsilon$-Nash equilibria through approximate best-response analyses, show that RL policies align with economic intuitions, and that our approach is constructive, e.g., by explicitly learning a spectrum of meta-game $\\epsilon$-Nash equilibria in open economic models.", "ratings": "[5.0, 6.0, 3.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "i2baoZMYZ3", "title": "Continuous Control with Action Quantization from Demonstrations", "keywords": "['Deep Reinforcement Learning', 'Action Discretization', 'Learning from Demonstrations']", "abstract": "In Reinforcement Learning (RL), discrete actions, as opposed to continuous actions, result in less complex exploration problem and the immediate derivation of the maximum of the action-value function which is central to dynamic programming-based methods. In this paper, we propose a novel method: Action Quantization from Demonstrations (AQuaDem) to learn a discretization of continuous action spaces by leveraging the priors of  demonstrations. This dramatically reduces the exploration problem, since the actions faced by the agent not only are in a finite number but also are plausible in light of the demonstrator\u2019s behavior. By discretizing the action space we can apply any discrete action deep RL algorithm to the continuous control problem. We evaluate the proposed method on three different setups: RL with demonstrations, RL with play data --demonstrations of a human playing in an environment but not solving any specific task-- and Imitation Learning. For all three setups, we only consider human data, thus most challenging than synthetic data. We found that AQuaDem consistently outperforms state-of-the-art continuous control methods, both in terms of performance and sample efficiency.", "ratings": "[5.0, 5.0, 6.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "IhkSFe9YqMy", "title": "Experience Replay More When It's a Key Transition in Deep Reinforcement Learning", "keywords": "['Experience Replay More', 'Key Transitions', 'Sampling', 'Add Noise to Noise', 'Deep Reinforcement Learning']", "abstract": "We propose a experience replay mechanism in Deep Reinforcement Learning based on Add Noise to Noise (AN2N), which requires agent to replay more experience containing key state, abbreviated as Experience Replay More (ERM). In the AN2N algorithm, we refer to the states where exploring more as the key states. We found that how the transitions containing the key state participates in updating the policy and Q networks has a significant impact on the performance improvement of the deep reinforcement learning agent, and the problem of catastrophic forgetting in neural networks is further magnified in the AN2N algorithm. Therefore, we change the previous strategy of uniform sampling of experience transitions. We sample the transition used for experience replay according to whether the transition contains key states and whether it is the most recently generated, which is the core idea of the ERM algorithm. The experimental results show that this algorithm can significantly improve the performance of the agent. We combine the ERM algorithm with Deep Deterministic Policy Gradient (DDPG), Twin Delayed Deep Deterministic policy gradient (TD3) and Soft Actor-Critic (SAC), and evaluate algorithm on the suite of OpenAI gym tasks, SAC with ERM achieves a new state of the art, and DDPG with ERM can even exceed the average performance of SAC under certain random seeds, which is incredible.", "ratings": "[1.0, 3.0, 1.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "yql6px0bcT", "title": "Decentralized Cross-Entropy Method for Model-Based Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Cross-Entropy Method', 'Planning', 'Model-Based RL']", "abstract": "Cross-Entropy Method (CEM) is a popular approach to planning in model-based reinforcement learning.         It has so far always taken a \\textit{centralized} approach where the sampling distribution is updated \\textit{centrally} based on the result of a top-$k$ operation applied to \\textit{all samples}.         We show that such a \\textit{centralized} approach makes CEM vulnerable to local optima and impair its sample efficiency, even in a one-dimensional multi-modal optimization task.         In this paper, we propose \\textbf{Decent}ralized \\textbf{CEM (DecentCEM)} where an ensemble of CEM instances run independently from one another and each performs a local improvement of its own sampling distribution.         In the exemplar optimization task, the proposed decentralized approach DecentCEM finds the global optimum much more consistently than the existing CEM approaches that use either a single Gaussian distribution or a mixture of Gaussians.         Further, we extend the decentralized approach to sequential decision-making problems where we show in 13 continuous control benchmark environments that it matches or outperforms the state-of-the-art CEM algorithms in most cases, under the same budget of the total number of samples for planning.", "ratings": "[6.0, 6.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "TMYzh1hsHd", "title": "MA2QL: A Minimalist Approach to Fully Decentralized Multi-Agent Reinforcement Learning", "keywords": "['multi-agent reinforcement learning']", "abstract": "Decentralized learning has shown great promise for cooperative multi-agent reinforcement learning (MARL). However, non-stationarity remains a significant challenge in fully decentralized learning. In the paper, we tackle the non-stationarity problem in the simplest and fundamental way and propose multi-agent alternate Q-learning (MA2QL), where agents take turns to update their Q-functions by Q-learning. MA2QL is a minimalist approach to fully decentralized cooperative MARL but is theoretically grounded. We prove that when each agent guarantees $\\varepsilon$-convergence at each turn, their joint policy converges to a Nash equilibrium. In practice, MA2QL only requires minimal changes to independent Q-learning (IQL). We empirically evaluate MA2QL on a variety of cooperative multi-agent tasks. Results show MA2QL consistently outperforms IQL, which verifies the effectiveness of MA2QL, despite such minimal changes.", "ratings": "[3, 6, 3, 3]", "confidences": "[4, 4, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "blCpfjAeFkn", "title": "Addressing High-dimensional Continuous Action Space via Decomposed Discrete Policy-Critic", "keywords": "['reinforcement learning', 'continuous control', 'actor-critic', 'decomposed policy', 'discretized action']", "abstract": "Reinforcement learning (RL) methods for discrete action spaces like DQNs are being widely used in tasks such as Atari games. However, they encounter difficulties when addressing continuous control tasks, since discretizing continuous action space incurs the curse-of-dimensionality. To tackle continuous control tasks via discretized actions, we propose a decomposed discrete policy-critic (D2PC) architecture, which was inspired by multi-agent RL (MARL) and associates with each action dimension a discrete policy, while leveraging a single critic network to provide a shared evaluation. Building on D2PC, we advocate soft stochastic D2PC (SD2PC) and deterministic D2PC (D3PC) methods with a discrete stochastic or deterministic policy, which show comparable or superior training performances relative to even continuous actor-critic methods. Additionally, we design a mechanism that allows D3PC to interact with continuous actor-critic methods, contributing to the Q-policy-critic (QPC) algorithm, which inherits the training efficiency of discrete RL and the near-optimal final performance of continuous RL algorithms. Substantial experimental results on several continuous benchmark tasks validate our claims.", "ratings": "[3, 3, 3, 3, 5]", "confidences": "[4, 4, 5, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "oGzm2X0aek", "title": "Offline Adaptive Policy Leaning in Real-World Sequential Recommendation Systems", "keywords": "Reinforcement learning, Recommendation system", "abstract": "The training process of RL requires many trial-and-errors that are costly in real-world applications. To avoid the cost, a promising solution is to learn the policy from an offline dataset, e.g., to learn a simulator from the dataset, and train optimal policies in the simulator. By this approach, the quality of policies highly relies on the fidelity of the simulator. Unfortunately, due to the stochasticity and unsteadiness of the real-world and the unavailability of online sampling, the distortion of the simulator is inevitable. In this paper, based on the model learning technique, we propose a new paradigm to learn an RL policy from offline data in the real-world sequential recommendation system (SRS). Instead of increasing the fidelity of models for policy learning, we handle the distortion issue via learning to adapt to diverse simulators generated by the offline dataset. The adaptive policy is suitable to real-world environments where dynamics are changing and have stochasticity in the offline setting. Experiments are conducted in synthetic environments and a real-world ride-hailing platform. The results show that the method overcomes the distortion problem and produces robust recommendations in the unseen real-world.", "ratings": "[7.0, 7.0, 4.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "lo7GKwmakFZ", "title": "Average Reward Reinforcement Learning with Monotonic Policy Improvement", "keywords": "Reinforcement Learning, Average Reward, Policy Optimization, Model Free RL", "abstract": "In continuing control tasks, an agent\u2019s average reward per time step is a more natural performance measure compared to the commonly used discounting framework as it can better capture an agent\u2019s long-term behavior.  We derive a novel lower bound on the difference of the average rewards for two policies, where the lower bound depends on the average divergence between the policies.  We show that previous work based on the discounted return (Schulman et al., 2015; Achiam et al.,2017) result in a trivial lower bound in the average reward setting.  We develop an iterative procedure based on our lower bound which produces a sequence of monotonically improved policies for the average reward criterion. When combined with deep reinforcement learning methods, the procedure leads to scalable and efficient algorithms aimed at maximizing an agent\u2019s average reward performance. Empirically, we demonstrate the efficacy of our algorithms through a series of high-dimensional control tasks with long time horizons and show that discounting can lead to unsatisfactory performance on continuing control tasks.", "ratings": "[6.0, 6.0, 5.0, 6.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "0_ao8yS2eBw", "title": "Solving NP-Hard Problems on Graphs with Extended AlphaGo Zero", "keywords": "Graph neural network, Combinatorial optimization, Reinforcement learning", "abstract": "There have been increasing challenges to solve combinatorial optimization problems by machine learning.        Khalil et al. (NeurIPS 2017) proposed an end-to-end reinforcement learning framework, which automatically learns graph embeddings to construct solutions to a wide range of problems.       However, it sometimes performs poorly on graphs having different characteristics than training graphs.       To improve its generalization ability to various graphs, we propose a novel learning strategy based on AlphaGo Zero, a Go engine that achieved a superhuman level without the domain knowledge of the game.       We redesign AlphaGo Zero for combinatorial optimization problems, taking into account several differences from two-player games.       In experiments on five NP-hard problems such as {\\sc MinimumVertexCover} and {\\sc MaxCut}, our method, with only a policy network, shows better generalization than the previous method to various instances that are not used for training, including random graphs, synthetic graphs, and real-world graphs.       Furthermore, our method is significantly enhanced by a test-time Monte Carlo Tree Search which makes full use of the policy network and value network.       We also compare recently-developed graph neural network (GNN) models, with an interesting insight into a suitable choice of GNN models for each task.", "ratings": "[4.0, 5.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "PU35uLgRZkk", "title": "The Skill-Action Architecture: Learning Abstract Action Embeddings for Reinforcement Learning", "keywords": "Hierarchical Reinforcement Learning, Reinforcement Learning", "abstract": "The option framework, one of the most promising Hierarchical Reinforcement Learning (HRL) frameworks, is developed based on the Semi-Markov Decision Problem (SMDP) and employs a triple formulation of the option (i.e., an action policy, a termination probability, and an initiation set). These design choices, however, mean that the option framework: 1) has low sample efficiency, 2) cannot use more stable Markov Decision Problem (MDP) based learning algorithms, 3) represents abstract actions implicitly, and 4) is expensive to scale up. To overcome these problems, here we propose a simple yet effective MDP implementation of the option framework: the Skill-Action (SA) architecture. Derived from a novel discovery that the SMDP option framework has an MDP equivalence, SA hierarchically extracts skills (abstract actions) from primary actions and explicitly encodes these knowledge into skill context vectors (embedding vectors). Although SA is MDP formulated, skills can still be temporally extended by applying the attention mechanism to skill context vectors. Unlike the option framework, which requires $M$ action policies for $M$ skills, SA's action policy only needs one decoder to decode skill context vectors into primary actions. Under this formulation, SA can be optimized with any MDP based policy gradient algorithm. Moreover, it is sample efficient, cheap to scale up, and theoretically proven to have lower variance. Our empirical studies on challenging infinite horizon robot simulation games demonstrate that SA not only outperforms all baselines by a large margin, but also exhibits smaller variance, faster convergence, and good interpretability. A potential impact of SA is to pave the way for a large scale pre-training architecture in the reinforcement learning area.", "ratings": "[5.0, 4.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "d1oQqDvB7GQ", "title": "Raisin: Residual Algorithms for Versatile Offline Reinforcement Learning", "keywords": "['reinforcement learning', 'offline RL', 'residual algorithms', 'residual gradient']", "abstract": "The residual gradient algorithm (RG), gradient descent of the Mean Squared Bellman Error, brings robust convergence guarantees to bootstrapped value estimation. Meanwhile, the far more common semi-gradient algorithm (SG) suffers from well-known instabilities and divergence. Unfortunately, RG often converges slowly in practice. Baird (1995) proposed residual algorithms (RA), weighted averaging of RG and SG, to combine RG's robust convergence and SG's speed. RA works moderately well in the online setting. We find, however, that RA works disproportionately well in the offline setting. Concretely, we find that merely adding a variable residual component to SAC increases its score on D4RL gym tasks by a median factor of 54. We further show that using the minimum of ten critics lets our algorithm match SAC-$N$'s state-of-the-art returns using 50$\\times$ less compute and no additional hyperparameters. In contrast, TD3+BC with the same minimum-of-ten-critics trick does not match SAC-$N$'s returns on a handful of environments.", "ratings": "[6, 5, 6]", "confidences": "[3, 4, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "hypDstHla7", "title": "Network Reusability Analysis for Multi-Joint Robot Reinforcement Learning", "keywords": "Reinforcement Learning, Machine Learning, Robot Motion Learning, DQN, Robot Manipulator, Target Reaching, Network Pruning", "abstract": "Recent experiments indicate that pre-training of end-to-end Reinforcement Learning neural networks on general tasks can speed up the training process for a specific application. However, it remains open if these networks form general feature extractors that are reused and a hierarchical organization as apparent e.g. in Convolutional Neural Networks.  In this paper we analyze the intrinsic neuron activation in networks trained for target reaching of planar robot manipulators with increasing joint number. We analyze the individual neuron activity distribution in the network, introduce a pruning algorithm to reduce the network size, and with these dense networks we spot correlations of neuron activity patterns among networks trained for robot manipulators with different joint number. We show that the input and output network layers have more distinct neuron activation in contrast to hidden layers. Our pruning algorithm reduces the network size significantly, increases the euclidean distance of neuron activation, but keeps a high performance in training and evaluation. Our results demonstrate correlations of neuron activity among networks trained for robots with different complexity. Hereby, robots with small joint number difference show higher layer-wise correlations whereas more different robots mostly show correlations to the first input layer.", "ratings": "[5.0, 4.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "gtwVBChN8td", "title": "Deep Reinforcement Learning With Adaptive Combined Critics", "keywords": "overestimation, continuous control, deep reinforcement learning, policy improvement", "abstract": "The overestimation problem has long been popular in deep value learning, because function approximation errors may lead to amplified value estimates and suboptimal policies. There have been several methods to deal with the overestimation problem, however, further problems may be induced, for example, the underestimation bias and instability. In this paper, we focus on the overestimation issues on continuous control through deep reinforcement learning, and propose a novel algorithm that can minimize the overestimation, avoid the underestimation bias and retain the policy improvement during the whole training process. Specifically, we add a weight factor to adjust the influence of two independent critics, and use the combined value of weighted critics to update the policy. Then the updated policy is involved in the update of the weight factor, in which we propose a novel method to provide theoretical and experimental guarantee for future policy improvement. We evaluate our method on a set of classical control tasks, and the results show that the proposed algorithms are more computationally efficient and stable than several existing algorithms for continuous control.", "ratings": "[3.0, 5.0, 3.0, 3.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "9jXqR128vKs", "title": "In-Context Policy Iteration", "keywords": "['Reinforcement Learning', 'In-Context Learning', 'Foundation Models']", "abstract": "This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the \u201cfew-shot\u201d quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt content is the entire locus of learning. ICPI iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our algorithm using Codex Chen et al. (2021b), a language model with no prior knowledge of the domains on which we evaluate it.", "ratings": "[6, 5, 5, 6]", "confidences": "[4, 2, 4, 2]", "decision": "Reject", "year": "2023"}
{"paper_id": "7bns2VTdMAx", "title": "Deep Learning of Intrinsically Motivated Options in the Arcade Learning Environment", "keywords": "['reinforcement learning', 'intrinsic motivation', 'exploration', 'options', 'auxiliary task learning']", "abstract": "In Reinforcement Learning, Intrinsic Motivation motivates directed behaviors through a wide range of reward-generating methods. Depending on the task and environment, these rewards can be useful, might complement each other, but can also break down entirely, as seen with the noisy TV problem for curiosity. We therefore argue that scalability and robustness, among others, are key desirable properties of a method to incorporate intrinsic rewards, which a simple weighted sum of reward lacks. In a tabular setting, Explore Options let the agent call an intrinsically motivated policy in order to learn from its trajectories. We introduce Deep Explore Options, revising Explore Options within the Deep Reinforcement Learning paradigm to tackle complex visual problems. Deep Explore Options can naturally learn from several unrelated intrinsic rewards, ignore harmful intrinsic rewards, learn to balance exploration, but also isolate exploitative and exploratory behaviors for independent usage. We test Deep Explore Options on hard and easy exploration games of the Atari Suite, following a benchmarking study to ensure fairness. Our empirical results show that they achieve similar results than weighted sum baselines, while maintaining their key properties. ", "ratings": "[3, 3, 1, 1]", "confidences": "[2, 4, 5, 5]", "decision": "Reject", "year": "2023"}
{"paper_id": "mewtfP6YZ7", "title": "On Trade-offs of Image Prediction in Visual Model-Based Reinforcement Learning", "keywords": "world models, model based reinforcement learning, latent planning, model-based reinforcement learning, model predictive control, video prediction", "abstract": "Model-based reinforcement learning (MBRL) methods have shown strong sample efficiency and performance across a variety of tasks, including when faced with high-dimensional visual observations. These methods learn to predict the environment dynamics and expected reward from interaction and use this predictive model to plan and perform the task. However, MBRL methods vary in their fundamental design choices, and it there is no strong consensus in the literature on how these design decisions affect performance. In this paper, we study a number of design decisions for the predictive model in visual MBRL algorithms, focusing specifically on methods that use a predictive model for planning. We find that a range of design decisions that are often considered crucial, such as the use of latent spaces, have little effect on task performance. A big exception to this finding is that predicting future observations (i.e., images) leads to significant task performance improvement compared to only predicting rewards. We also empirically find that image prediction accuracy, somewhat surprisingly, correlates more strongly with downstream task performance than reward prediction accuracy. We show how this phenomenon is related to exploration and how some of the lower-scoring models on standard benchmarks (that require exploration) will perform the same as the best-performing models when trained on the same training data. Simultaneously, in the absence of exploration, models that fit the data better usually perform better on the down-stream task as well, but surprisingly, these are often not the same models that perform the best when learning and exploring from scratch. These findings suggest that performance and exploration place important and potentially contradictory requirements on the model.", "ratings": "[7.0, 6.0, 3.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "enhd0P_ERBO", "title": "Learning a Transferable Scheduling Policy for Various Vehicle Routing Problems based on Graph-centric Representation Learning", "keywords": "Vehicle Routing Problem, Multiple Traveling Salesmen Problem, Capacitated Vehicle Routing Problem, Reinforcement Learning, Graph Neural Network", "abstract": "Reinforcement learning has been used to learn to solve various routing problems. however, most of the algorithm is restricted to finding an optimal routing strategy for only a single vehicle. In addition, the trained policy under a specific target routing problem is not able to solve different types of routing problems with different objectives and constraints. This paper proposes an reinforcement learning approach to solve the min-max capacitated multi vehicle routing problem (mCVRP), the problem seeks to minimize the total completion time for multiple vehicles whose one-time traveling distance is constrained by their fuel levels to serve the geographically distributed customer nodes. The method represents the relationships among vehicles, customers, and fuel stations using relationship-specific graphs to consider their topological relationships and employ graph neural network (GNN) to extract the graph's embedding to be used to make a routing action. We train the proposed model using the random mCVRP instance with different numbers of vehicles, customers, and refueling stations. We then validate that the trained policy solve not only new mCVRP problems with different complexity (weak transferability but also different routing problems (CVRP, mTSP, TSP) with different objectives and constraints (storing transferability).", "ratings": "[5.0, 6.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "JHXjK94yH-y", "title": "Explore and Control with Adversarial Surprise", "keywords": "['reinforcement learning', 'intrinsic motivation', 'exploration', 'multi-agent']", "abstract": "Unsupervised reinforcement learning (RL) studies how to leverage environment statistics to learn useful behaviors without the cost of reward engineering. However, a central challenge in unsupervised RL is to extract behaviors that meaningfully affect the world and cover the range of possible outcomes, without getting distracted by inherently unpredictable, uncontrollable, and stochastic elements in the environment. To this end, we propose an unsupervised RL method designed for high-dimensional, stochastic environments based on an adversarial game between two policies (which we call Explore and Control) controlling a single body and competing over the amount of observation entropy the agent experiences. The Explore agent seeks out states that maximally surprise the Control agent, which in turn aims to minimize surprise, and thereby manipulate the environment to return to familiar and predictable states. The competition between these two policies drives them to seek out increasingly surprising parts of the environment while learning to gain mastery over them. We show formally that the resulting algorithm maximizes coverage of the underlying state in block MDPs with stochastic observations, providing theoretical backing to our hypothesis that this procedure avoids uncontrollable and stochastic distractions. Our experiments further demonstrate that Adversarial Surprise leads to the emergence of complex and meaningful skills, and outperforms state-of-the-art unsupervised reinforcement learning methods in terms of both exploration and zero-shot transfer to downstream tasks.", "ratings": "[5.0, 3.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "hF1WEiIYPNb", "title": "Query The Agent: Improving Sample Efficiency Through Epistemic Uncertainty Estimation", "keywords": "['goal-conditioned reinforcement learning', 'reinforcement learning', 'goal-conditioned', 'goal', 'model-free', 'sample efficiency', 'deep reinforcement learning']", "abstract": "Curricula for goal-conditioned reinforcement learning agents typically rely on poor estimates of the agent's epistemic uncertainty or fail to consider the agents' epistemic uncertainty altogether, resulting in poor sample efficiency. We propose a novel algorithm, Query The Agent (QTA), which significantly improves sample efficiency by estimating the agent's epistemic uncertainty throughout the state space and setting goals in highly uncertain areas. Encouraging the agent to collect data in highly uncertain states allows the agent to improve its estimation of the value function rapidly. QTA utilizes a novel technique for estimating epistemic uncertainty, Predictive Uncertainty Networks (PUN), to allow QTA to assess the agent's uncertainty in all previously observed states. We demonstrate that QTA offers decisive sample efficiency improvements over preexisting methods.", "ratings": "[5, 3, 5, 5]", "confidences": "[2, 4, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "FoRC6dIfO8u", "title": "Cyclophobic Reinforcement Learning", "keywords": "['Reinforcement learning', 'intrinsic rewards', 'exploration', 'transfer learning', 'objects']", "abstract": "In environments with sparse rewards finding a good inductive bias for exploration is crucial to the agent\u2019s success. However, there are two competing goals: novelty search and systematic exploration. While existing approaches such as curiousity- driven exploration find novelty, they sometimes do not systematically explore the whole state space, akin to depth-first-search vs breadth-first-search. In this paper, we propose a new intrinsic reward that is cyclophobic, i.e. it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the agent\u2019s cropped observations we are able to achieve excellent results in the MiniGrid and MiniHack environments. Both are particularly hard, as they require complex interactions with different objects in order to be solved. Detailed comparisons with previous approaches and thorough ablation studies show that our newly proposed cyclophobic reinforcement learning is vastly more efficient than other state of the art methods.", "ratings": "[5, 3, 3]", "confidences": "[3, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "bIwkmDnSeu", "title": "Unbiased learning with State-Conditioned Rewards in Adversarial Imitation Learning", "keywords": "Adversarial Learning, Imitation Learning, Inverse Reinforcement Learning, Reinforcement Learning, Transfer Learning", "abstract": "Adversarial imitation learning has emerged as a general and scalable framework for automatic reward acquisition. However, we point out that previous methods commonly exploited occupancy-dependent reward learning formulation. Despite the theoretical justification, the occupancy measures tend to cause issues in practice because of high variance and low vulnerability to domain shifts. Another reported problem is termination biases induced by provided rewarding and regularization schemes around terminal states. In order to deal with these issues, this work presents a novel algorithm called causal adversarial inverse reinforcement learning. The framework employs a dual discriminator architecture for decoupling state densities from rewards formulation. We investigate the reward shaping theory to deal with the finite horizon problem and address the reward function's optimality. The formulation draws a strong connection between adversarial learning and energy-based reinforcement learning; thus, the architecture is capable of recovering a reward function that induces a multi-modal policy. In experiments, we demonstrate that our approach outperforms prior methods in challenging continuous control tasks, even under significant variation in the environments.", "ratings": "[5.0, 4.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "Lb8ZnWW_In6", "title": "LEARNING DYNAMIC ABSTRACT REPRESENTATIONS FOR SAMPLE-EFFICIENT REINFORCEMENT LEARNING", "keywords": "['Sequential Decision-Making', 'Reinforcement Learning', 'Learning Abstract Representations']", "abstract": "In many real-world problems, the learning agent needs to learn a problem\u2019s abstractions and solution simultaneously. However, most such abstractions need to be designed and refined by hand for different problems and domains of application. This paper presents a novel top-down approach for constructing state abstractions while carrying out reinforcement learning. Starting with state variables and a simulator, it presents a novel domain-independent approach for dynamically computing an abstraction based on the dispersion of Q-values in abstract states as the agent continues acting and learning. Extensive empirical evaluation on multiple domains and problems shows that this approach automatically learns abstractions that are finely-tuned to the problem, yield powerful sample efficiency, and result in the RL agent significantly outperforming existing approaches.", "ratings": "[3, 5, 5]", "confidences": "[4, 3, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "_SKUm2AJpvN", "title": "Decoupling Representation Learning from Reinforcement Learning", "keywords": "reinforcement learning, representation learning, unsupervised learning", "abstract": "In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning.  To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss.  In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments.  Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others.  We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks.  Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation.  Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at \\url{hidden url}.", "ratings": "[6.0, 5.0, 5.0, 7.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "IUGwUr5_9wY", "title": "DISCO-DANCE: Learning to Discover Skills with Guidance", "keywords": "['Unsupervised skill discovery', 'Reinforcement Learning']", "abstract": "Unsupervised skill discovery (USD) allows agents to learn diverse and discriminable skills without access to pre-defined rewards, by maximizing the mutual information (MI) between skills and states reached by each skill. The most common problem of MI-based skill discovery is insufficient exploration, because each skill is heavily penalized when it deviates from its initial settlement. Recent works introduced an auxiliary reward to encourage the exploration of the agent via maximizing the state's epistemic uncertainty or entropy. However, we have discovered that the performance of these auxiliary rewards decreases as the environment becomes more challenging. Therefore, we introduce a new unsupervised skill discovery algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill which has the highest potential to reach the unexplored states, (2) guide other skills to follow the guide skill, then (3) the guided skills are diffused to maximize their discriminability in the unexplored states. Empirically, DISCO-DANCE substantially outperforms other USD baselines on challenging environments including two navigation benchmarks and a continuous control benchmark.", "ratings": "[5, 5, 3]", "confidences": "[3, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "2iKvo44-Bya", "title": "System Identification as a Reinforcement Learning Problem", "keywords": "['System Identification', 'Reinforcement Learning', 'Offline Reinforcement Learning', 'Forward Models']", "abstract": "System identification, also known as learning forward models, transfer functions, system dynamics, etc., has a long tradition both in science and engineering in different fields. Particularly, it is a recurring theme in Reinforcement Learning research, where forward models approximate the state transition function of a Markov Decision Process by learning a mapping function from current state and action to the next state. This problem is commonly defined as a Supervised Learning problem in a direct way. This common approach faces several difficulties due to the inherent complexities of the dynamics to learn, for example, delayed effects, high non-linearity, non-stationarity, partial observability and, more important, error accumulation when using bootstrapped predictions (predictions based on past predictions), over large time horizons. Here we explore the use of Reinforcement Learning in this problem. We elaborate on why and how this problem fits naturally and sound as a Reinforcement Learning problem, and present some experimental results that demonstrate RL is a promising technique to solve these kind of problems.", "ratings": "[6, 1, 3, 5]", "confidences": "[4, 4, 5, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "j97zf-nLhC", "title": "Zero-Shot Coordination via Semantic Relationships Between Actions and Observations", "keywords": "['multi-agent communication', 'multi-agent reinforcement learning', 'attention mechanism', 'zero-shot coordination']", "abstract": "An unaddressed challenge in zero-shot coordination is to take advantage of the semantic relationship between the features of an action and the features of observations. Humans take advantage of these relationships in highly intuitive ways. For instance in the absence of a shared-language, we might point to the object we desire or hold up fingers to indicate how many objects we want. To address this challenge, we investigate the effect of network architecture on the propensity of learning algorithms to make use of these relationships in human-compatible ways. We find that attention-based architectures that jointly process a featurized representation of the observation and the action, have a better inductive bias for exploiting semantic relationships for zero-shot coordination. Excitingly, in a set of diagnostic tasks, these agents produce highly human-compatible policies, without requiring the symmetry relationships of the problems to be hard-coded.", "ratings": "[6.0, 6.0, 6.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "tDG-zrQ8S1Q", "title": "SPRINT: Scalable Semantic Policy Pre-training via Language Instruction Relabeling", "keywords": "['reinforcement learning', 'language-guided RL', 'offline RL', 'policy pre-training']", "abstract": "We propose SPRINT, an approach for scalable offline policy pre-training based on natural language instructions. SPRINT pre-trains an agent\u2019s policy to execute a diverse set of semantically meaningful skills that it can leverage to learn new tasks faster. Prior work on offline pre-training required tedious manual definition of pre-training tasks or learned semantically meaningless skills via random goal-reaching. Instead, our approach SPRINT (Scalable Pre-training via Relabeling Language INsTructions) leverages natural language instruction labels on offline agent experience, collected at scale (e.g., via crowd-sourcing), to define a rich set of tasks with minimal human effort. Furthermore, by using natural language to define tasks, SPRINT can use pre-trained large language models to automatically expand the initial task set. By relabeling and aggregating task instructions, even across multiple training trajectories, we can learn a large set of new skills during pre-training. In experiments using a realistic household simulator, we show that agents pre-trained with SPRINT learn new long-horizon household tasks substantially faster than with previous pre-training approaches.", "ratings": "[5, 6, 6, 5]", "confidences": "[3, 4, 3, 2]", "decision": "Reject", "year": "2023"}
{"paper_id": "TlS3LBoDj3Z", "title": "QTRAN++: Improved Value Transformation for Cooperative Multi-Agent Reinforcement Learning", "keywords": "multi-agent reinforcement learning", "abstract": "QTRAN is a multi-agent reinforcement learning (MARL) algorithm capable of learning the largest class of joint-action value functions up to date. However, despite its strong theoretical guarantee, it has shown poor empirical performance in complex environments, such as Starcraft Multi-Agent Challenge (SMAC). In this paper, we identify the performance bottleneck of QTRAN and propose a substantially improved version, coined QTRAN++. Our gains come from (i) stabilizing the training objective of QTRAN, (ii) removing the strict role separation between the action-value estimators of QTRAN, and (iii) introducing a multi-head mixing network for value transformation. Through extensive evaluation, we confirm that our diagnosis is correct, and QTRAN++ successfully bridges the gap between empirical performance and theoretical guarantee. In particular, QTRAN++ newly achieves state-of-the-art performance in the SMAC environment. The code will be released.", "ratings": "[6.0, 7.0, 6.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "zzL_5WoI3I", "title": "An Adaptive Entropy-Regularization Framework for Multi-Agent Reinforcement Learning", "keywords": "['Multi-Agent Reinforcement Learning', 'Entropy Regularization', 'Exploration-Exploitation Tradeoff']", "abstract": "In this paper, we propose an adaptive entropy-regularization framework (ADER) for multi-agent reinforcement learning (RL) to learn the adequate amount of exploration for each agent based on the degree of required exploration. In order to handle instability arising from updating multiple entropy temperature parameters for multiple agents, we disentangle the soft value function into two types: one for pure reward and the other for entropy. By applying multi-agent value factorization to the disentangled value function of pure reward, we obtain a relevant metric to assess the necessary degree of exploration for each agent. Based on this metric, we propose the ADER algorithm based on maximum entropy RL, which controls the necessary level of exploration across agents over time by learning the proper target entropy for each agent. Experimental results show that the proposed scheme significantly outperforms current state-of-the-art multi-agent RL algorithms. ", "ratings": "[6, 8, 3]", "confidences": "[3, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "mYNfmvt8oSv", "title": "D2RL: Deep Dense Architectures in Reinforcement Learning", "keywords": "Deep Reinforcement learning, Policy architectures", "abstract": "While improvements in deep learning architectures have played a crucial role in improving the state of supervised and unsupervised learning in computer vision and natural language processing, neural network architecture choices for reinforcement learning remain relatively under-explored. We take inspiration from successful architectural choices in computer vision and generative modeling, and investigate the use of deeper networks and dense connections for reinforcement learning on a variety of simulated robotic learning benchmark environments. Our findings reveal that current methods benefit significantly from dense connections and deeper networks, across a suite of manipulation and locomotion tasks, for both proprioceptive and image-based observations. We hope that our results can serve as a strong baseline and further motivate future research into neural network architectures for reinforcement learning.", "ratings": "[5.0, 8.0, 4.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "lNrtNGkr-vw", "title": "Linear Representation Meta-Reinforcement Learning for Instant Adaptation", "keywords": "meta reinforcement learning, out-of-distribution, reinforcement learning", "abstract": "This paper introduces Fast Linearized Adaptive Policy (FLAP), a new meta-reinforcement learning (meta-RL) method that is able to extrapolate well to out-of-distribution tasks without the need to reuse data from training, and adapt almost instantaneously with the need of only a few samples during testing. FLAP builds upon the idea of learning a shared linear representation of the policy so that when adapting to a new task, it suffices to predict a set of linear weights. A separate adapter network is trained simultaneously with the policy such that during adaptation, we can directly use the adapter network to predict these linear weights instead of updating a meta-policy via gradient descent such as in prior Meta-RL algorithms like  MAML to obtain the new policy. The application of the separate feed-forward network not only speeds up the adaptation run-time significantly, but also generalizes extremely well to very different tasks that prior Meta-RL methods fail to generalize to. Experiments on standard continuous-control meta-RL benchmarks show FLAP presenting significantly stronger performance on out-of-distribution tasks with up to double the average return and up to 8X faster adaptation run-time speeds when compared to prior methods.", "ratings": "[7.0, 6.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "kOtkgUGAVTX", "title": "CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery", "keywords": "['unsupervised learning', 'reinforcement learning', 'exploration']", "abstract": "We introduce Contrastive Intrinsic Control (CIC) - an algorithm for unsupervised skill discovery that maximizes the mutual information between skills and state transitions. In contrast to most prior approaches, CIC uses a decomposition of the mutual information that explicitly incentivizes diverse behaviors by maximizing state entropy. We derive a novel lower bound estimate for the mutual information which combines a particle estimator for state entropy to generate diverse behaviors and contrastive learning to distill these behaviors into distinct skills.  We evaluate our algorithm on the Unsupervised Reinforcement Learning Benchmark, which consists of a long reward-free pre-training phase followed by a short adaptation phase to downstream tasks with extrinsic rewards. We find that CIC improves on prior unsupervised skill discovery methods by $91\\%$ and the next-leading overall exploration algorithm by $26\\%$ in terms of downstream task performance.", "ratings": "[3.0, 8.0, 8.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "V4AVDoFtVM", "title": "What About Taking Policy as Input of Value Function: Policy-extended Value Function Approximator", "keywords": "Reinforcement Learning, Value Function Approximation, Representation Learning", "abstract": "The value function lies in the heart of Reinforcement Learning (RL), which defines the long-term evaluation of a policy in a given state. In this paper, we propose Policy-extended Value Function Approximator (PeVFA) which extends the conventional value to be not only a function of state but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies in contrast to a conventional one with limited capacity for only one policy, inducing the new characteristic of \\emph{value generalization among policies}. From both the theoretical and empirical lens, we study value generalization along the policy improvement path (called local generalization), from which we derive a new form of Generalized Policy Iteration with PeVFA to improve the conventional learning process. Besides, we propose a framework to learn the representation of an RL policy,  studying several different approaches to learn an effective policy representation from policy network parameters and state-action pairs through contrastive learning and action prediction. In our experiments, Proximal Policy Optimization (PPO) with PeVFA significantly outperforms its vanilla counterpart in MuJoCo continuous control tasks, demonstrating the effectiveness of value generalization offered by PeVFA and policy representation learning.", "ratings": "[3.0, 5.0, 5.0, 7.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "nulUqBMpBb", "title": "Uncertainty-Driven Exploration for Generalization in Reinforcement Learning", "keywords": "['Deep reinforcement learning', 'exploration', 'generalization', 'procgen', 'crafter']", "abstract": "Value-based methods tend to outperform policy optimization methods when trained and tested in single environments; however, they significantly underperform when trained on multiple environments with similar characteristics and tested on new ones from the same distribution. We investigate the potential reasons behind the poor generalization performance of value-based methods and discover that exploration plays a crucial role in these settings. Exploration is helpful not only for finding optimal solutions to the training environments, but also for acquiring knowledge that helps generalization to other unseen environments. We show how to make value-based methods competitive with policy optimization methods in these settings by using uncertainty-driven exploration and distribtutional RL. Our algorithm is the first value-based method to achieve state-of-the-art on both Procgen and Crafter, two challenging benchmarks for generalization in RL. ", "ratings": "[6, 6, 5, 5]", "confidences": "[4, 4, 4, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "2G-vUJ7XcSB", "title": "On the Power of Pre-training for Generalization in RL: Provable Benefits and Hardness", "keywords": "['Reinforcement Learning', 'Generalization', 'Learning Theory']", "abstract": "Generalization in Reinforcement Learning (RL) aims to train an agent during training that generalizes to the target environment. In this work, we first point out that RL generalization is fundamentally different from the generalization in supervised learning, and fine-tuning on the target environment is necessary for good test performance. Therefore, we seek to answer the following question: how much can we expect pre-training over training environments to be helpful for efficient and effective fine-tuning? On one hand, we give a surprising result showing that asymptotically, the improvement from pre-training is at most a constant factor. On the other hand, we show that pre-training can be indeed helpful in the non-asymptotic regime by designing a policy collection-elimination (PCE) algorithm and proving a distribution-dependent regret bound that is independent of the state-action space. We hope our theoretical results can provide insight towards understanding pre-training and generalization in RL.", "ratings": "[8, 5, 5]", "confidences": "[3, 3, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "Ew0zR07CYRd", "title": "Bounded Myopic Adversaries for Deep Reinforcement Learning Agents", "keywords": "deep reinforcement learning, adversarial", "abstract": "Adversarial attacks against deep neural networks have been widely studied. Adversarial examples for deep reinforcement learning (DeepRL) have significant security implications, due to the deployment of these algorithms in many application domains. In this work we formalize an optimal myopic adversary for deep reinforcement learning agents. Our adversary attempts to find a bounded perturbation of the state which minimizes the value of the action taken by the agent. We show with experiments in various games in the Atari environment that our attack formulation achieves significantly larger impact as compared to the current  state-of-the-art. Furthermore, this enables us to lower the bounds by several orders of magnitude on the perturbation needed to efficiently achieve significant impacts on DeepRL agents.", "ratings": "[6.0, 6.0, 6.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "X6YPReSv5CX", "title": "Mixture of Step Returns in Bootstrapped DQN", "keywords": "Reinforcement Learning", "abstract": "The concept of utilizing multi-step returns for updating value functions has been adopted in deep reinforcement learning (DRL) for a number of years. Updating value functions with different backup lengths provides advantages in different aspects, including bias and variance of value estimates, convergence speed, and exploration behavior of the agent. Conventional methods such as TD-lambda leverage these advantages by using a target value equivalent to an exponential average of different step returns. Nevertheless, integrating step returns into a single target sacrifices the diversity of the advantages offered by different step return targets. To address this issue, we propose Mixture Bootstrapped DQN (MB-DQN) built on top of bootstrapped DQN, and uses different backup lengths for different bootstrapped heads. MB-DQN enables heterogeneity of the target values that is unavailable in approaches relying only on a single target value. As a result, it is able to maintain the advantages offered by different backup lengths. In this paper, we first discuss the motivational insights through a simple maze environment. In order to validate the effectiveness of MB-DQN, we perform experiments on the Atari 2600 benchmark environments and demonstrate the performance improvement of MB-DQN over a number of baseline methods. We further provide a set of ablation studies to examine the impacts of different design configurations of MB-DQN.", "ratings": "[5.0, 7.0, 4.0, 4.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "AT7jak63NNK", "title": "Meta-Reinforcement Learning Robust to Distributional Shift via Model Identification and Experience Relabeling", "keywords": "Meta-Reinforcement Learning, Meta Learning, Reinforcement Learning", "abstract": "Reinforcement learning algorithms can acquire policies for complex tasks autonomously. However, the number of samples required to learn a diverse set of skills can be prohibitively large. While meta-reinforcement learning methods have enabled agents to leverage prior experience to adapt quickly to new tasks, their performance depends crucially on how close the new task is to the previously experienced tasks. Current approaches are either not able to extrapolate well, or can do so at the expense of requiring extremely large amounts of data for on-policy meta-training. In this work, we present model identification and experience relabeling (MIER), a meta-reinforcement learning algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time. Our method is based on a simple insight: we recognize that dynamics models can be adapted efficiently and consistently with off-policy data, more easily than policies and value functions. These dynamics models can then be used to continue training policies and value functions for out-of-distribution tasks without using meta-reinforcement learning at all, by generating synthetic experience for the new task.", "ratings": "[5.0, 4.0, 5.0, 6.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "Ti2i204vZON", "title": "Learning Representations for Pixel-based Control: What Matters and Why?", "keywords": "['Reinforcement Learning', 'Representation Learning', 'Pixel-based Control']", "abstract": "Learning representations for pixel-based control has garnered significant attention recently in reinforcement learning. A wide range of methods have been proposed to enable efficient learning, leading to sample complexities similar to those in the full state setting. However, moving beyond carefully curated pixel data sets (centered crop, appropriate lighting, clear background, etc.) remains challenging. In this paper, we adopt a more difficult setting, incorporating background distractors, as a first step towards addressing this challenge. We present a simple baseline approach that can learn meaningful representations with no metric-based learning, no data augmentations, no world-model learning, and no contrastive learning. We then analyze when and why previously proposed methods are likely to fail or reduce to the same performance as the baseline in this harder setting and why we should think carefully about extending such methods beyond the well-curated environments. Our results show that finer categorization of benchmarks on the basis of characteristics like the density of reward, planning horizon of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms. Based on these observations, we propose different metrics to consider when evaluating an algorithm on benchmark tasks. We hope such a data-centric view can motivate researchers to rethink representation learning when investigating how to best apply RL to real-world tasks.", "ratings": "[3.0, 6.0, 3.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "dN_iVr6iNuU", "title": "Preventing Value Function Collapse in Ensemble Q-Learning by Maximizing Representation Diversity", "keywords": "Ensemble Q-Learning, Representation Diversity, Reinforcement Learning", "abstract": "The first deep RL algorithm, DQN, was limited by the overestimation bias of the learned Q-function. Subsequent algorithms proposed techniques to reduce this problem, without fully eliminating it. Recently, the Maxmin and Ensemble Q-learning algorithms used the different estimates provided by ensembles of learners to reduce the bias. Unfortunately, these learners can converge to the same point in the parametric or representation space, falling back to the classic single neural network DQN. In this paper, we describe a regularization technique to maximize diversity in the representation space in these algorithms. We propose and compare five regularization functions inspired from economics theory and consensus optimization. We show that the resulting approach significantly outperforms the Maxmin and Ensemble Q-learning algorithms as well as non-ensemble baselines.", "ratings": "[6.0, 5.0, 5.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "PcR6Lir5mxu", "title": "Planning With Uncertainty: Deep Exploration in Model-Based Reinforcement Learning", "keywords": "['Reinforcement learning', 'exploration', 'uncertainty', 'planning']", "abstract": "Deep model-based reinforcement learning has shown super-human performance in many challenging domains. Low sample efficiency and limited exploration remain however as leading obstacles in the field. In this paper, we demonstrate deep exploration in model-based RL by incorporating epistemic uncertainty into planning trees, circumventing the standard approach of propagating uncertainty through value learning. We evaluate this approach with the state of the art model-based RL algorithm MuZero, and extend its training process to stabilize learning from explicitly-exploratory decisions. Our results demonstrate that planning with uncertainty is able to achieve effective deep exploration with standard uncertainty estimation mechanisms, and with it significant gains in sample efficiency.", "ratings": "[5, 3, 3, 3]", "confidences": "[3, 4, 4, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "HP-tcf48fT", "title": "Learning to Search for Fast Maximum Common Subgraph Detection", "keywords": "graph matching, maximum common subgraph, graph neural network, reinforcement learning, search", "abstract": "Detecting the Maximum Common Subgraph (MCS) between two input graphs is fundamental for applications in biomedical analysis, malware detection, cloud computing, etc. This is especially important in the task of drug design, where the successful extraction of common substructures in compounds can reduce the number of experiments needed to be conducted by humans. However, MCS computation is NP-hard, and state-of-the-art MCS solvers rely on heuristics in search which in practice cannot find good solution for large graph pairs under a limited search budget. Here we propose GLSearch, a Graph Neural Network based model for MCS detection, which learns to search. Our model uses a state-of-the-art branch and bound algorithm as the backbone search algorithm to extract subgraphs by selecting one node pair at a time. In order to make better node selection decision at each step, we replace the node selection heuristics with a novel task-specific Deep Q-Network (DQN), allowing the search process to find larger common subgraphs faster. To enhance the training of DQN, we leverage the search process to provide supervision in a pre-training stage and guide our agent during an imitation learning stage. Therefore, our framework allows search and reinforcement learning to mutually benefit each other. Experiments on synthetic and real-world large graph pairs demonstrate that our model outperforms state-of-the-art MCS solvers and neural graph matching network models.", "ratings": "[7.0, 5.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "26WnoE4hjS", "title": "Measuring and mitigating interference in reinforcement learning", "keywords": "Reinforcement Learning, Representation Learning", "abstract": "Catastrophic interference is common in many network-based learning systems, and many proposals exist for mitigating it. But, before we overcome interference we must understand it better. In this work, we first provide a definition and novel measure of interference for value-based control methods such as Fitted Q Iteration and DQN. We systematically evaluate our measure of interference, showing that it correlates with forgetting, across a variety of network architectures. Our new interference measure allows us to ask novel scientific questions about commonly used deep learning architectures and develop new learning algorithms. In particular we show that updates on the last layer result in significantly higher interference than updates internal to the network. Lastly, we introduce a novel online-aware representation learning algorithm to minimize interference, and we empirically demonstrate that it improves stability and has lower interference.", "ratings": "[5.0, 4.0, 6.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "cbtV7xGO9pS", "title": "TEAC: Intergrating Trust Region and Max Entropy Actor Critic for Continuous Control", "keywords": "Reinforcement Learning, Trust region methods, Maximum Entropy Reinforcement Learning, Deep Reinforcement Learning", "abstract": "Trust region methods and maximum entropy methods are two state-of-the-art branches used in reinforcement learning (RL) for the benefits of stability and exploration in continuous environments, respectively. This paper proposes to integrate both branches in a unified framework, thus benefiting from both sides. We first transform the original RL objective to a constraint optimization problem and then proposes trust entropy actor critic (TEAC), an off-policy algorithm to learn stable and sufficiently explored policies for continuous states and actions. TEAC trains the critic by minimizing the refined Bellman error and updates the actor by minimizing KL-divergence loss derived from the closed-form solution to the Lagrangian.        We prove that the policy evaluation and policy improvement in TEAC is guaranteed to converge.       Extensive experiments on the tasks in the MuJoCo environment show that TEAC outperforms state-of-the-art solutions in terms of efficiency and effectiveness.", "ratings": "[5.0, 5.0, 5.0, 7.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "5-X1XzdAWcC", "title": "Efficient Exploration using Model-Based Quality-Diversity with Gradients", "keywords": "['Quality-Diversity', 'Exploration', 'Reinforcement Learning']", "abstract": "Exploration is a key challenge in Reinforcement Learning, especially in long-horizon, deceptive and sparse-reward environments. For such applications, population-based approaches have proven effective. Methods such as Quality-Diversity deals with this by encouraging novel solutions and producing a diversity of behaviours. However, these methods are driven by either undirected sampling (i.e. mutations) or use approximated gradients (i.e. Evolution Strategies) in the parameter space, which makes them highly sample-inefficient. In this paper, we propose a model-based Quality-Diversity approach, relying on gradients and learning in imagination. Our approach optimizes all members of a population simultaneously to maintain both performance and diversity efficiently by leveraging the effectiveness of QD algorithms as good data generators to train deep models. We demonstrate that it maintains the divergent search capabilities of population-based approaches while significantly improving their sample efficiency (5 times faster) and quality of solutions (2 times more performant).", "ratings": "[6, 6, 3, 5]", "confidences": "[3, 4, 4, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "e-ZdxsIwweR", "title": "Robust Constrained Reinforcement Learning for Continuous Control with Model Misspecification", "keywords": "reinforcement learning, constraints, robustness", "abstract": "Many real-world physical control systems are required to satisfy constraints upon deployment. Furthermore, real-world systems are often subject to effects such as non-stationarity, wear-and-tear, uncalibrated sensors and so on. Such effects effectively perturb the system dynamics and can cause a policy trained successfully in one domain to perform poorly when deployed to a perturbed version of the same domain. This can affect a policy's ability to maximize future rewards as well as the extent to which it satisfies constraints. We refer to this as constrained model misspecification. We present an algorithm with theoretical guarantees that mitigates this form of misspecification, and showcase its performance in multiple Mujoco tasks from the Real World Reinforcement Learning (RWRL) suite.", "ratings": "[5.0, 5.0, 4.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "wE-3ly4eT5G", "title": "FactoredRL: Leveraging Factored Graphs for Deep Reinforcement Learning", "keywords": "reinforcement learning, factored mdp, factored rl", "abstract": "We propose a simple class of deep reinforcement learning (RL) methods, called FactoredRL, that can leverage factored environment structures to improve the sample efficiency of existing model-based and model-free RL algorithms. In tabular and linear approximation settings, the factored Markov decision process literature has shown exponential improvements in sample efficiency by leveraging factored environment structures. We extend this to deep RL algorithms that use neural networks. For model-based algorithms, we use the factored structure to inform the state transition network architecture and for model-free algorithms we use the factored structure to inform the Q network or the policy network architecture.  We demonstrate that doing this significantly improves sample efficiency in both discrete and continuous state-action space settings.", "ratings": "[6.0, 6.0, 6.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "ZvvxYyjfvZc", "title": "Correcting Momentum in Temporal Difference Learning", "keywords": "Momentum, Reinforcement Learning, Temporal Difference, Deep Reinforcement Learning", "abstract": "A common optimization tool used in deep reinforcement learning is momentum, which consists in accumulating and discounting past gradients, reapplying them at each iteration. We argue that, unlike in supervised learning, momentum in Temporal Difference (TD) learning accumulates gradients that become doubly stale: not only does the gradient of the loss change due to parameter updates, the loss itself changes due to bootstrapping. We first show that this phenomenon exists, and then propose a first-order correction term to momentum. We show that this correction term improves sample efficiency in policy evaluation by correcting target value drift. An important insight of this work is that deep RL methods are not always best served by directly importing techniques from the supervised setting.", "ratings": "[6.0, 6.0, 6.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "WZ2L6D8IHoc", "title": "Contextual Symbolic Policy For Meta-Reinforcement Learning", "keywords": "['meta learning', 'reinforcement learning', 'context variables', 'symbolic policy']", "abstract": "Context-based Meta-Reinforcement Learning (Meta-RL), which conditions the RL agent on the context variables, is a powerful method for learning a generalizable agent. Current context-based Meta-RL methods often construct their contextual policy with a neural network (NN) and directly take the context variables as a part of the input. However, the NN-based policy contains tremendous parameters which possibly result in overfitting, the difficulty of deployment and poor interpretability. To improve the generation ability, efficiency and interpretability, we propose a novel Contextual Symbolic Policy (CSP) framework, which generates contextual policy with a symbolic form based on the context variables for unseen tasks in meta-RL. Our key insight is that the symbolic expression is capable of capturing complex relationships by composing various operators and has a compact form that helps strip out irrelevant information. Thus, the CSP learns to produce symbolic policy for meta-RL tasks and extract the essential common knowledge to achieve higher generalization ability. Besides, the symbolic policies with a compact form are efficient to be deployed and easier to understand. In the implementation, we construct CSP as a gradient-based framework to learn the symbolic policy from scratch in an end-to-end and differentiable way. The symbolic policy is represented by a symbolic network composed of various symbolic operators. We also employ a path selector to decide the proper symbolic form of the policy and a parameter generator to produce the coefficients of the symbolic policy. Empirically, we evaluate the proposed CSP method on several Meta-RL tasks and demonstrate that the contextual symbolic policy achieves higher performance and efficiency and shows the potential to be interpretable.", "ratings": "[5, 5, 3, 5]", "confidences": "[4, 2, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "rQYyXqHPgZR", "title": "Success-Rate Targeted Reinforcement Learning by Disorientation Penalty", "keywords": "reinforcement learning, undiscounted return, success rate", "abstract": "Current reinforcement learning generally uses discounted return as its learning objective. However, real-world tasks may often demand a high success rate, which can be quite different from optimizing rewards. In this paper, we explicitly formulate the success rate as an undiscounted form of return with {0, 1}-binary reward function. Unfortunately, applying traditional Bellman updates to value function learning can be problematic for learning undiscounted return, and thus not suitable for optimizing success rate. From our theoretical analysis, we discover that values across different states tend to converge to the same value, resulting in the agent wandering around those states without making any actual progress. This further leads to reduced learning efficiency and inability to complete a task in time. To combat the aforementioned issue, we propose a new method, which introduces Loop Penalty (LP) into value function learning, to penalize disoriented cycling behaviors in the agent's decision-making. We demonstrate the effectiveness of our proposed LP on three environments, including grid-world cliff-walking, Doom first-person navigation and robot arm control, and compare our method with Q-learning, Monte-Carlo and Proximal Policy Optimization (PPO). Empirically, LP improves the convergence of training and achieves a higher success rate.", "ratings": "[4.0, 4.0, 3.0, 2.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "Rw_vo-wIAa", "title": "Multi-agent Policy Optimization with Approximatively Synchronous Advantage Estimation", "keywords": "multi-agent reinforcement learning, policy optimization, advantage estimation, credit assignment", "abstract": "Cooperative multi-agent tasks require agents to deduce their own contributions with shared global rewards, known as the challenge of credit assignment. General methods for policy based multi-agent reinforcement learning to solve the challenge introduce differentiate value functions or advantage functions for individual agents. In multi-agent system, polices of different agents need to be evaluated jointly. In order to update polices synchronously, such value functions or advantage functions also need synchronous evaluation. However, in current methods, value functions or advantage functions use counter-factual joint actions which are evaluated asynchronously, thus suffer from natural estimation bias. In this work, we propose the approximatively synchronous advantage estimation. We first derive the marginal advantage function, an expansion from single-agent advantage function to multi-agent system. Further more, we introduce a policy approximation for synchronous advantage estimation, and break down the multi-agent policy optimization problem into multiple sub-problems of single-agent policy optimization. Our method is compared with baseline algorithms on StarCraft multi-agent challenges, and shows the best performance on most of the tasks.", "ratings": "[4.0, 3.0, 5.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "YPHIlC3K4J", "title": "Neural Discrete Reinforcement Learning", "keywords": "['Deep Reinforcement Learning', 'Representation Learning', 'Action Space']", "abstract": "Designing effective action spaces for complex environments is a fundamental and challenging problem in reinforcement learning (RL). Some recent works have revealed that naive RL algorithms utilizing well-designed handcrafted discrete action spaces can achieve promising results even when dealing with high-dimensional continuous or hybrid decision-making problems. However, elaborately designing such action spaces requires comprehensive domain knowledge. In this paper, we systemically analyze the advantages of discretization for different action spaces and then propose a unified framework, Neural Discrete Reinforcement Learning (NDRL), to automatically learn how to effectively discretize almost arbitrary action spaces. Specifically, we propose the Action Discretization Variational AutoEncoder (AD-VAE), an action representation learning method that can learn compact latent action spaces while maintain the essential properties of original environments, such as boundary actions and the relationship between different action dimensions. Moreover, we uncover a key issue that parallel optimization of the AD-VAE and online RL agents is often unstable. To address it, we further design several techniques to adapt RL agents to learned action representations, including latent action remapping and ensemble Q-learning. Quantitative experiments and visualization results demonstrate the efficiency and stability of our proposed framework for complex action spaces in various environments. ", "ratings": "[5, 3, 3, 5]", "confidences": "[3, 4, 4, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "uEBrNNEfceE", "title": "Safe Linear-Quadratic Dual Control with Almost Sure Performance Guarantee", "keywords": "['reinforcement learning']", "abstract": "This paper considers the linear-quadratic dual control problem where the system parameters need to be identified and the control objective needs to be optimized in the meantime. Contrary to existing works on data-driven linear-quadratic regulation, which typically provide error or regret bounds within a certain probability, we propose an online algorithm that guarantees the asymptotic optimality of the controller in the almost sure sense. Our dual control strategy consists of two parts: a switched controller with time-decaying exploration noise and Markov parameter inference based on the cross-correlation between the exploration noise and system output. Central to the almost sure performance guarantee is a safe switched control strategy that falls back to a known conservative but stable controller when the actual state deviates significantly from the target state. We prove that this switching strategy rules out any potential destabilizing controllers from being applied, while the performance gap between our switching strategy and the optimal linear state feedback is exponentially small. Under our dual control scheme, the parameter inference error scales as $O(T^{-1/4+\\epsilon})$, while the suboptimality gap of control performance scales as $O(T^{-1/2+\\epsilon})$, where $T$ is the number of time steps, and $\\epsilon$ is an arbitrarily small positive number. Simulation results on an industrial process example are provided to illustrate the effectiveness of our proposed strategy.", "ratings": "[5.0, 5.0, 8.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "1usJZBGNrZ", "title": "Offline Reinforcement Learning with Closed-Form Policy Improvement Operators", "keywords": "['Offline Reinforcement Learning algorithms', 'Deep Reinforcement Learning']", "abstract": "Behavior constrained policy optimization has been demonstrated to be a successful paradigm for tackling Offline Reinforcement Learning. By exploiting historical transitions, a policy is trained to maximize a learned value function while constrained by the behavior policy to avoid a significant distributional shift. In this paper, we propose our closed-form policy improvement operators. We make a novel observation that the behavior constraint naturally motivates the use of first-order Taylor approximation, leading to a linear approximation of the policy objective. Additionally, as practical datasets are usually collected by heterogeneous policies, we model the behavior policies as a Gaussian Mixture and overcome the induced optimization difficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality, giving rise to a closed-form policy improvement operator. We instantiate an offline RL algorithm with our novel policy improvement operator and empirically demonstrate its effectiveness over state-of-the-art algorithms on the standard D4RL benchmark.", "ratings": "[6, 6, 5]", "confidences": "[3, 2, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "3_NvTLGjDKy", "title": "Unified neural representation model for physical and conceptual spaces", "keywords": "['Neuroscience', 'Grid cell', 'Concept cell', 'Spatial navigation', 'Reinforcement learning', 'Word embedding']", "abstract": "The spatial processing system of the brain uses grid-like neural representations (grid cells) for supporting vector-based navigation. Experiments also suggest that neural representations for concepts (concept cells) exist in the human brain, and conceptual inference relies on navigation in conceptual spaces. We propose a unified model called ``disentangled successor information (DSI)'' that explains neural representations for both physical and conceptual spaces. DSI generates grid-like representations in a 2-dimensional space that highly resemble those observed in the brain. Moreover, the same model creates concept-specific representations from linguistic inputs, corresponding to concept cells. Mathematically, DSI vectors approximate value functions for navigation and word vectors obtained by word embedding methods, thus enabling both spatial navigation and conceptual inference based on vector-based calculation. Our results suggest that a single principle can explain computation of physical and conceptual spaces in the human brain.", "ratings": "[6, 3, 3, 8]", "confidences": "[4, 4, 3, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "tVrRejrC-RZ", "title": "Robust Exploration via Clustering-based Online Density Estimation", "keywords": "['exploration', 'representation learning', 'density estimation', 'reinforcement learning']", "abstract": "Intrinsic motivation is a critical ingredient in reinforcement learning to enable progress when rewards are sparse. However, many existing approaches that measure the novelty of observations are brittle, or rely on restrictive assumptions about the environment which limit generality. We propose to decompose the exploration problem into two orthogonal sub-problems: (i) finding the right representation (metric) for exploration (ii) estimating densities in this representation space. To address (ii), we introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method that estimates visitation counts for clusters of states that are similar according to the metric induced by any arbitrary representation learning technique. We adapt classical clustering algorithms to design a new type of memory that allows RECODE to keep track of the history of interactions over thousands of episodes, thus effectively tracking global visitation counts. This is in contrast to existing non-parametric approaches, that can only store the recent history, typically the current episode. The generality of RECODE allows us to easily address (i) by leveraging both off-the-shelf and novel representation learning techniques. In particular, we introduce a novel generalization of the action-prediction representation that leverages multi-step predictions and that we find to be better suited to a suite of challenging 3D-exploration tasks in DM-HARD-8. We show experimentally that our approach can work with a variety of RL agents, and obtain state-of-the-art performance on Atari and DM-HARD-8.", "ratings": "[3, 3, 3]", "confidences": "[3, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "Py8WbvKH_wv", "title": "DRIBO: Robust Deep Reinforcement Learning via Multi-View Information Bottleneck", "keywords": "['Representation Learning', 'Deep Reinforcement Learning', 'Information Bottleneck']", "abstract": "Deep reinforcement learning (DRL) agents are often sensitive to visual changes that were unseen in their training environments. To address this problem, we leverage the sequential nature of RL to learn robust representations that encode only task-relevant information from observations based on the unsupervised multi-view setting. Specifically, we introduce a novel contrastive version of Multi-View Information Bottleneck (MIB) objective for temporal data. We train RL agents from pixels with this auxiliary objective to learn robust representations that can compress away task-irrelevant information and are predictive of task-relevant dynamics. This approach enables us to train high-performance policies that are robust to visual distractions and can generalize well to unseen environments. We demonstrate that our approach can achieve SOTA performance on diverse visual control tasks on the DeepMind Control Suite when the background is replaced with natural videos. In addition, we show that our approach outperforms well-established baselines for generalization to unseen environments on the Procgen benchmark.", "ratings": "[5.0, 5.0, 6.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "S2UB9PkrEjF", "title": "Universal Value Density Estimation for Imitation Learning and Goal-Conditioned Reinforcement Learning", "keywords": "Imitation Learning, Reinforcement Learning, Universal Value Functions", "abstract": "This work considers two distinct settings: imitation learning and goal-conditioned reinforcement learning. In either case, effective solutions require the agent to reliably reach a specified state (a goal), or set of states (a demonstration). Drawing a connection between probabilistic long-term dynamics and the desired value function, this work introduces an approach that utilizes recent advances in density estimation to effectively learn to reach a given state. We develop a unified view on the two settings and show that the approach can be applied to both. In goal-conditioned reinforcement learning, we show it to circumvent the problem of sparse rewards while addressing hindsight bias in stochastic domains. In imitation learning, we show that the approach can learn from extremely sparse amounts of expert data and achieves state-of-the-art results on a common benchmark.", "ratings": "[6.0, 4.0, 5.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "tvwNdOKhuF5", "title": "Superior Performance with Diversified Strategic Control in FPS Games Using General Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Hindsight Experience Replay', 'FPS Games']", "abstract": "This paper offers an overall solution for first-person shooter (FPS) games to achieve superior performance using general reinforcement learning (RL). We introduce an agent in ViZDoom that can surpass previous top agents ranked in the open ViZDoom AI Competitions by a large margin. The proposed framework consists of a number of generally applicable techniques, including hindsight experience replay (HER) based navigation, hindsight proximal policy optimization (HPPO), rule-guided policy search (RGPS), prioritized fictitious self-play (PFSP), and diversified strategic control (DSC). The proposed agent outperforms existing agents by taking advantage of diversified and human-like strategies, instead of larger neural networks, more accurate frag skills, or hand-craft tricks, etc. We provide comprehensive analysis and experiments to elaborate the effect of each component in affecting the agent performance, and demonstrate that the proposed and adopted techniques are important to achieve superior performance in general end-to-end FPS games. The proposed methods can contribute to other games and real-world tasks which also require spatial navigation and diversified behaviors.", "ratings": "[3.0, 3.0, 3.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "GJkTaYTmzVS", "title": "Play to Grade: Grading Interactive Coding Games as Classifying Markov Decision Process", "keywords": "Deep Reinforcement Learning, Education, Automated Grading, Program Testing", "abstract": "Contemporary coding education often present students with the task of developing programs that have user interaction and complex dynamic systems, such as mouse based games. While pedagogically compelling, grading such student programs requires dynamic user inputs, therefore they are difficult to grade by unit tests. In this paper we formalize the challenge of grading interactive programs as a task of classifying Markov Decision Processes (MDPs). Each student's program fully specifies an MDP where the agent needs to operate and decide, under reasonable generalization, if the dynamics and reward model of the input MDP conforms to a set of latent MDPs. We demonstrate that by experiencing a handful of latent MDPs millions of times, we can use the agent to sample trajectories from the input MDP and use a classifier to determine membership. Our method drastically reduces the amount of data needed to train an automatic grading system for interactive code assignments and present a challenge to state-of-the-art reinforcement learning generalization methods. Together with Code.org, we curated a dataset of 700k student submissions, one of the largest dataset of anonymized student submissions to a single assignment. This Code.org assignment had no previous solution for automatically providing correctness feedback to students and as such this contribution could lead to meaningful improvement in educational experience.", "ratings": "[5.0, 3.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "RmB-zwXOIVC", "title": "Imitation with Neural Density Models", "keywords": "Imitation Learning, Reinforcement Learning, Density Estimation, Density Model, Maximum Entropy RL, Mujoco", "abstract": "We propose a new framework for Imitation Learning (IL) via density estimation of the expert's occupancy measure followed by Maximum Occupancy Entropy Reinforcement Learning (RL) using the density as a reward. Our approach maximizes a non-adversarial model-free RL objective that provably lower bounds reverse Kullback\u2013Leibler divergence between occupancy measures of the expert and imitator. We present a practical IL algorithm, Neural Density Imitation (NDI), which obtains state-of-the-art demonstration efficiency on benchmark control tasks.", "ratings": "[5.0, 6.0, 8.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "gcjxr_g48GU", "title": "LPMARL: Linear Programming based Implicit Task Assignment for Hierarchical Multi-agent Reinforcement Learning", "keywords": "['Linear programming', 'Multi-agent reinforcement learning', 'Hierarchical multi-agent reinforcement learning', 'Implicit deep learning']", "abstract": "Training a multi-agent reinforcement learning (MARL) model with sparse reward is notoriously difficult because the terminal reward is induced by numerous interactions among agents. In this study, we propose linear programming-based hierarchical MARL (LPMARL) to learn effective coperative strategy among agents. LPMARL is composed of two hierarchical decision-making schemes: (1) solving an agent-task assignment problem and (2) solving a local cooperative game among agents that are assigned to the same task. For the first step, LPMARL formulates the agent-task assignment problem as linear programming (LP) using the state-dependent cost parameters generated by a graph neural network (GNN). Solving the LP can be considered as assigning tasks to agents, which decomposes the original problem into a set of task-dependent sub-problems. After solving the formulated LP, LPMARL employs a general MARL strategy to derive a lower-level policy to solve each sub-task in a cooperative manner. We train the LP-parameter generating GNN layer and the low-level MARL policy network, which are the essential components for making hierarchical decisions, in an end-to-end manner using the implicit function theorem. We empirically demonstrate that LPMARL learns an optimal agent-task allocation and the subsequent local cooperative control policy among agents in sub-groups for solving various mixed cooperative-competitive environments.", "ratings": "[6, 6, 5, 5]", "confidences": "[3, 3, 4, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "NOApNZTiTNU", "title": "Aggressive Q-Learning with Ensembles: Achieving Both High Sample Efficiency and High Asymptotic Performance", "keywords": "['deep reinforcement learning', 'off-policy', 'model-free', 'sample efficiency', 'ensembles']", "abstract": "Recently, Truncated Quantile Critics (TQC), using distributional representation of critics, was shown to provide state-of-the-art asymptotic training performance on all environments from the MuJoCo continuous control benchmark suite. Also recently, Randomized Ensemble Double Q-Learning (REDQ), using a high update-to-data ratio and target randomization, was shown to achieve high sample efficiency that is competitive with state-of-the-art model-based methods. In this paper, we propose a novel model-free algorithm, Aggressive Q-Learning with Ensembles (AQE), which improves the sample-efficiency performance of REDQ and the asymptotic performance of TQC, thereby providing overall state-of-the-art performance during all stages of training. Moreover, AQE is very simple, requiring neither distributional representation of critics nor target randomization.", "ratings": "[5.0, 3.0, 5.0, 6.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "mfJepDyIUcQ", "title": "Safety Verification of Model Based Reinforcement Learning Controllers", "keywords": "Reachable set, state constraints, safety verification, model-based reinforcement learning", "abstract": "Model-based reinforcement learning (RL) has emerged as a promising tool for developing controllers for real world systems (e.g., robotics, autonomous driving, etc.).  However, real systems often have constraints imposed on their state space which must be satisfied to ensure the safety of the system and its environment. Developing a verification tool for RL algorithms is challenging because the non-linear structure of neural networks impedes analytical verification of such models or controllers.  To this end, we present a novel safety verification framework for model-based RL controllers using reachable set analysis.  The proposed framework can efficiently handle models and controllers which are represented using neural networks. Additionally, if a controller fails to satisfy the safety constraints in general, the proposed framework can also be used to identify the subset of initial states from which the controller can be safely executed.", "ratings": "[5.0, 7.0, 7.0, 3.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "KdcLdLuIjQT", "title": "Goal Randomization for Playing Text-based Games without a Reward Function", "keywords": "['Text-based games', 'Deep reinforcement learning', 'Generalization']", "abstract": "Playing text-based games requires language understanding and sequential decision making. The objective of a reinforcement learning agent is to behave so as to maximise the sum of a suitable scalar reward function. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose a goal randomization method that uses random basic goals to train a policy in the absence of the reward of environments. Specifically, through simple but effective goal generation, our method learns to continuously propose challenging -- yet temporal and achievable -- goals that allow the agent to learn general skills for acting in a new environment, independent of the task to be solved. In a variety of text-based games, we show that this simple method results in competitive performance for agents. We also show that our method can learn policies that generalize across different text-based games. In further, we demonstrate an interesting result that our method works better than one of state-of-the-art agents GATA, which uses environment rewards for some text-based games.", "ratings": "[5.0, 5.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "74cDdRwm4NV", "title": "Learning to Shape Rewards using a Game of Two Partners", "keywords": "['Reinforcement learning', 'Reward Shaping', 'Markov game', 'Sparse rewards']", "abstract": "Reward shaping (RS) is a powerful method in reinforcement learning (RL) for  overcoming the problem of sparse or uninformative rewards. However, RS typically  relies on manually engineered shaping-reward functions whose construction is time consuming and error-prone. It also requires domain knowledge which runs contrary  to the goal of autonomous learning. We introduce Reinforcement Learning Optimal  Shaping Algorithm (ROSA), an automated RS framework in which the shaping reward function is constructed in a novel Markov game between two agents. A  reward-shaping agent (Shaper) uses switching controls to determine which states to add shaping rewards and their optimal values while the other agent (Controller) learns the optimal policy for the task using these shaped rewards. We prove that ROSA, which easily adopts existing RL algorithms, learns to construct a shaping reward function that is tailored to the task thus ensuring efficient convergence to high performance policies. We demonstrate ROSA\u2019s congenial properties in three carefully designed experiments and show its superior performance against state-of-the-art RS algorithms in challenging sparse reward environments.", "ratings": "[3.0, 6.0, 5.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "Frt6LTRFhui", "title": "General Policy Evaluation and Improvement by Learning to Identify Few But Crucial States", "keywords": "['Reinforcement Learning', 'Off-Policy Reinforcement Learning']", "abstract": "Learning to evaluate and improve policies is a core problem of Reinforcement Learning (RL). Traditional RL algorithms learn a value function defined for a single policy. A recently explored competitive alternative is to learn a single value function for many policies. Here we combine the actor-critic architecture of Parameter-Based Value Functions and the policy embedding of Policy Evaluation Networks to learn a single value function for evaluating (and thus helping to improve) any policy represented by a deep neural network (NN). The method yields competitive experimental results. In continuous control problems with infinitely many states, our value function minimizes its prediction error by simultaneously learning a small set of `probing states' and a mapping from actions produced in probing states to the policy's return. The method extracts crucial abstract knowledge about the environment in form of very few states sufficient to fully specify the behavior of many policies. A policy improves solely by changing actions in probing states, following the gradient of the value function's predictions. Surprisingly, it is possible to clone the behavior of a near-optimal policy in Swimmer-v3 and Hopper-v3 environments only by knowing how to act in 3 and 5 such learned states, respectively. Remarkably, our value function trained to evaluate NN policies is also invariant to changes of the policy architecture: we show that it allows for zero-shot learning of linear policies competitive with the best policy seen during training.", "ratings": "[3, 5, 6, 5]", "confidences": "[4, 4, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "7yuU9VeIpde", "title": "Memory-Constrained Policy Optimization", "keywords": "['reinforcement learning', 'trust region policy optimization', 'on-policy', 'policy gradient', 'neural network']", "abstract": "We introduce a new constrained optimization method for policy gradient reinforcement learning, which uses two trust regions to regulate each policy update. In addition to using the proximity of one single old policy as the first trust region as done by prior works, we propose to form a second trust region through the construction of another virtual policy that represents a wide range of past policies. We then enforce the new policy to stay closer to the virtual policy, which is beneficial in case the old policy performs badly. More importantly, we propose a mechanism to automatically build the virtual policy from a memory buffer of past policies, providing a new capability for dynamically selecting appropriate trust regions during the optimization process. Our proposed method, dubbed as Memory-Constrained Policy Optimization (MCPO), is examined on a diverse suite of environments including robotic locomotion control, navigation with sparse rewards and Atari games, consistently demonstrating competitive performance against recent on-policy constrained policy gradient methods.", "ratings": "[5.0, 8.0, 5.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "33nhOe3cTd", "title": "Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions", "keywords": "['Computer Go', 'Monte-Carlo Tree Search', 'Reinforcement learning', 'Adaptive', 'Acceleration']", "abstract": "One of the most important AI research questions is to trade off computation versus performance, since \"perfect rational\" exists in theory but it is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant improvement of performance in varieties of challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that mimics the human behavior that spends adequate amounts of time to think about different questions. Inspired by this, we propose a strategy that converges to the ground truth MCTS search results with much less computation. We give theoretical bounds of the V-MCTS and evaluate the performance in $9 \\times 9$ Go board games and Atari games. Experiments show that our method can achieve similar performances as the original search algorithm while requiring less than $50\\%$ number of search times on average.         We believe that this approach is a viable alternative for tasks with limited time and resources.", "ratings": "[5.0, 3.0, 8.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "pC00NfsvnSK", "title": "Improving zero-shot generalization in offline reinforcement learning using generalized similarity functions", "keywords": "['reinforcement learning', 'representation learning', 'self-supervised learning', 'offline RL', 'generalized value function', 'generalization']", "abstract": "Reinforcement learning (RL) agents are widely used for solving complex sequential decision making tasks, but still exhibit difficulty in generalizing to scenarios not seen during training. While prior online approaches demonstrated that using additional signals beyond the reward function can lead to better generalization capabilities in RL agents, i.e. using self-supervised learning (SSL), they struggle in the offline RL setting, i.e. learning from a static dataset. We show that the performance of online algorithms for generalization in RL can be hindered in the offline setting due to poor estimation of similarity between observations. We propose a new theoretically-motivated framework called Generalized Similarity Functions (GSF), which uses contrastive learning to train an offline RL agent to aggregate observations based on the similarity of their expected future behavior, where we quantify this similarity using generalized value functions. We show that GSF is general enough to recover existing SSL objectives while also improving zero-shot generalization performance on a complex offline RL benchmark, offline Procgen.", "ratings": "[5.0, 6.0, 6.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "RuC5ilX2m6O", "title": "Local Patch AutoAugment with Multi-Agent Collaboration", "keywords": "['Automatic Data Augmentation', 'Multi-Agent Reinforcement Learning']", "abstract": "Data augmentation (DA) plays a critical role in improving the generalization of deep learning models. Recent works on automatically searching for DA policies from data have achieved great success. However, existing automated DA methods generally perform the search at the image level, which limits the exploration of diversity in local regions. In this paper, we propose a more fine-grained automated DA approach, dubbed Patch AutoAugment, to divide an image into a grid of patches and search for the joint optimal augmentation policies for the patches. We formulate it as a multi-agent reinforcement learning (MARL) problem, where each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. We show the effectiveness of our method on multiple benchmark datasets of image classification and fine-grained image recognition (e.g., CIFAR-10, CIFAR-100, ImageNet, CUB-200-2011, Stanford Cars and FGVC-Aircraft). Extensive experiments demonstrate that our method outperforms the state-of-the-art DA methods while requiring fewer computational resources.", "ratings": "[6.0, 5.0, 6.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "uFkGzn9RId8", "title": "The act of remembering: A study in partially observable reinforcement learning", "keywords": "Reinforcement Learning, Partial Observability, Memory Representations, External Memories, POMDPs.", "abstract": "Reinforcement Learning (RL) agents typically learn memoryless policies---policies that only consider the last observation when selecting actions. Learning memoryless policies is efficient and optimal in fully observable environments. However, some form of memory is necessary when RL agents are faced with partial observability. In this paper, we study a lightweight approach to tackle partial observability in RL. We provide the agent with an external memory and additional actions to control what, if anything, is written to the memory. At every step, the current memory state is part of the agent\u2019s observation, and the agent selects a tuple of actions: one action that modifies the environment and another that modifies the memory. When the external memory is sufficiently expressive, optimal memoryless policies yield globally optimal solutions. Unfortunately, previous attempts to use external memory in the form of binary memory have produced poor results in practice. Here, we investigate alternative forms of memory in support of learning effective memoryless policies. Our novel forms of memory outperform binary and LSTM-based memory in well-established partially observable domains.", "ratings": "[5.0, 6.0, 7.0, 7.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "6KZ_kUVCfTa", "title": "Non-Markovian Predictive Coding For Planning In Latent Space", "keywords": "representation learning, reinforcement learning, information theory", "abstract": "High-dimensional observations are a major challenge in the application of model-based reinforcement learning (MBRL) to real-world environments. In order to handle high-dimensional sensory inputs, existing MBRL approaches use representation learning to map high-dimensional observations into a lower-dimensional latent space that is more amenable to dynamics estimation and planning. Crucially, the task-relevance and predictability of the learned representations play critical roles in the success of planning in latent space. In this work, we present Non-Markovian Predictive Coding (NMPC), an information-theoretic approach for planning from high-dimensional observations with two key properties: 1) it formulates a mutual information objective that prioritizes the encoding of task-relevant components of the environment; and 2) it employs a recurrent neural network capable of modeling non-Markovian latent dynamics. To demonstrate NMPC\u2019s ability to prioritize task-relevant information, we evaluate our new model on a challenging modification of standard DMControl tasks where the DMControl background is replaced with natural videos, containing complex but irrelevant information to the planning task. Our experiments show that NMPC is superior to existing methods in the challenging complex-background setting while remaining competitive with current state-of-the-art MBRL models in the standard setting.", "ratings": "[5.0, 6.0, 6.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "Hyan74saltV", "title": "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration", "keywords": "['Reinforcement Learning', 'Control', 'Skills', 'Priors', 'Hierarchical Reinforcement Learning']", "abstract": "The ability to effectively reuse prior knowledge is a key requirement when building general and flexible Reinforcement Learning (RL) agents. Skill reuse is one of the most common approaches, but current methods have considerable limitations. For example, fine-tuning an existing policy frequently fails, as the policy can degrade rapidly early in training, particularly in sparse reward tasks. In a similar vein, distillation of expert behavior can lead to poor results when given sub-optimal experts. We compare several common approaches for skill transfer on multiple domains and in several different transfer settings, including under changes in task and system dynamics. We identify how existing methods can fail and introduce an alternative approach which sidesteps some of these problems. Our approach learns to sequence existing temporally-abstract skills for exploration but learns the final policy directly from the raw experience. This conceptual split enables rapid adaptation and thus efficient data collection but without constraining the final solution. Our approach significantly outperforms many classical methods across a suite of evaluation tasks and we use a broad set of ablations to highlight the importance of different components of our method.", "ratings": "[3, 8, 6, 5]", "confidences": "[4, 4, 3, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "LemVOgJ4yP", "title": "Learning to Cooperate and Communicate Over Imperfect Channels", "keywords": "['multi-agent systems', 'deep reinforcement learning', 'emergent communication', 'imperfect communication channels']", "abstract": "Information exchange in multi-agent systems improves the cooperation among agents, especially in partially observable settings. This can be seen as part of the problem in which the agents learn how to communicate and to solve a shared task simultaneously. In the real world, communication is often carried out over imperfect channels and this requires the agents to deal with uncertainty due to potential information loss. In this paper, we consider a cooperative multi-agent system where the agents act and exchange information in a decentralized manner using a limited and unreliable channel. To cope with such channel constraints, we propose a novel communication approach based on independent Q-learning. Our method allows agents to dynamically adapt how much information to share by sending messages of different size, depending on their local observations and the channel properties. In addition to this message size selection, agents learn to encode and decode messages to improve their policies. We show that our approach outperforms approaches without adaptive capabilities and discuss its limitations in different environments.", "ratings": "[5, 5, 3]", "confidences": "[3, 3, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "4JlwgTbmzXQ", "title": "EqR: Equivariant Representations for Data-Efficient Reinforcement Learning", "keywords": "['Equivariance', 'Invariance', 'Representation learning', 'Reinforcement learning', 'Symmetric MDPs', 'MDP homomorphism', 'Lie parameterization.']", "abstract": "We study different notions of equivariance as an inductive bias in Reinforcement Learning (RL) and propose new mechanisms for recovering representations that are equivariant to both an agent\u2019s action, and symmetry transformations of the state-action pairs. Whereas prior work on exploiting symmetries in deep RL can only incorporate predefined linear transformations, our approach allows for non-linear symmetry transformations of state-action pairs to be learned from the data itself. This is achieved through an equivariant Lie algebraic parameterization of state and action encodings, equivariant latent transition models, and the use of symmetry-based losses. We demonstrate the advantages of our learned equivariant representations for Atari games, in a data-efficient setting limited to 100k steps of interactions with the environment. Our method, which we call Equivariant representations for RL (EqR), outperforms many previous methods in a similar setting by achieving a median human-normalized score of 0.418, and surpassing human-level performance on 8 out of the 26 games.", "ratings": "[8.0, 8.0, 6.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "ioXEbG_Sf-a", "title": "Experience Replay with Likelihood-free Importance Weights", "keywords": "Experience Replay, Off-Policy Optimization, Deep Reinforcement Learning", "abstract": "The use of past experiences to accelerate temporal difference (TD) learning of value functions, or experience replay, is a key component in deep reinforcement learning. In this work, we propose to reweight experiences based on their likelihood under the stationary distribution of the current policy, and justify this with a contraction argument over the Bellman evaluation operator. The resulting TD objective encourages small approximation errors on the value function over frequently encountered states. To balance bias and variance in practice, we use a likelihood-free density ratio estimator between on-policy and off-policy experiences, and use the ratios as the prioritization weights. We apply the proposed approach empirically on three competitive methods, Soft Actor Critic (SAC), Twin Delayed Deep Deterministic policy gradient (TD3) and Data-regularized Q (DrQ), over 11 tasks from OpenAI gym and DeepMind control suite. We achieve superior sample complexity on 35 out of 45 method-task combinations compared to the best baseline and similar sample complexity on the remaining 10.", "ratings": "[6.0, 5.0, 7.0, 3.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "K1CNgCJtLLr", "title": "CrystalBox: Efficient Model-Agnostic Explanations for Deep RL Controllers", "keywords": "['explainability', 'reinforcement learning']", "abstract": "Practical adoption of Reinforcement Learning (RL) controllers is hindered by a lack of explainability. Particularly, in input-driven environments such as computer systems where the state dynamics are affected by external processes, explainability can serve as a key towards increased real-world deployment of RL controllers. In this work, we propose a novel framework, CrystalBox, for generating black-box post-hoc explanations for RL controllers in input-driven environments. CrystalBox is built on the principle of separation between policy learning and explanation computation. As the explanations are generated completely outside the training loop, CrystalBox is generalizable to a large family of input-driven RL controllers.To generate explanations, CrystalBox combines the natural decomposability of reward functions in systems environments with the explanatory power of decomposed returns. CrystalBox predicts these decomposed future returns using on policy Q-function approximations. Our design leverages two complementary approaches for this computation: sampling- and learning-based methods. We evaluate CrystalBox with RL controllers in real-world settings and demonstrate that it generates high-fidelity explanations. ", "ratings": "[3, 3, 5]", "confidences": "[4, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "U0jfsqmoV-4", "title": "Instruction-Following Agents with Jointly Pre-Trained Vision-Language Models", "keywords": "['reinforcement learning', 'pre-training', 'multimodal representation', 'representation learning', 'transformer']", "abstract": "Humans are excellent at understanding language and vision to accomplish a wide range of tasks. In contrast, creating general instruction-following embodied agents remains a difficult challenge. Prior work that uses pure language-only models lack visual grounding, making it difficult to connect language instructions with visual observations. On the other hand, methods that use pre-trained vision-language models typically come with divided language and visual representations, requiring designing specialized network architecture to fuse them together. We propose a simple yet effective model for robots to solve instruction-following tasks in vision-based environments. Our InstructRL method consists of a multimodal transformer that encodes visual observations and language instructions, and a policy transformer that predicts actions based on encoded representations. The multimodal transformer is pre-trained on millions of image-text pairs and natural language text, thereby producing generic cross-modal representations of observations and instructions. The policy transformer keeps track of the full history of observations and actions, and predicts actions autoregressively. We show that this unified transformer model outperforms all state-of-the-art pre-trained or trained-from-scratch methods in both single-task and multi-task settings. Our model also shows better model scalability and generalization ability than prior work.", "ratings": "[5, 6, 5]", "confidences": "[4, 4, 2]", "decision": "Reject", "year": "2023"}
{"paper_id": "mNLLDtkAy4X", "title": "Escaping Stochastic Traps with Aleatoric Mapping Agents", "keywords": "['Curiosity', 'Neuroscience', 'Acetylcholine', 'Uncertainty', 'Reinforcement learning', 'Intrinsic Rewards']", "abstract": "When extrinsic rewards are sparse, artificial agents struggle to explore an environment. Curiosity, implemented as an intrinsic reward for prediction errors, can improve exploration but fails when faced with action-dependent noise sources. We present aleatoric mapping agents (AMAs), a neuroscience inspired solution modeled on the cholinergic system of the mammalian brain. AMAs aim to explicitly ascertain which dynamics of the environment are unpredictable, regardless of whether those dynamics are induced by the actions of the agent. This is achieved by generating separate forward predictions for the mean and aleatoric uncertainty of future states with reducing intrinsic rewards for those states that are unpredictable. We show AMAs are able to effectively circumvent action-dependent stochastic traps that immobilise conventional curiosity driven agents.", "ratings": "[3.0, 6.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "m3DmIL7wHDW", "title": "The guide and the explorer: smart agents for resource-limited iterated batch reinforcement learning", "keywords": "['Model-based reinforcement learning', 'Dyna', 'exploration', 'planning', 'offline', 'growing batch', 'iterated batch']", "abstract": "Iterated (a.k.a growing) batch reinforcement learning (RL) is a growing subfield fueled by the demand from systems engineers for intelligent control solutions that they can apply within their technical and organizational constraints. Model-based RL (MBRL) suits this scenario well for its sample efficiency and modularity. Recent MBRL techniques combine efficient neural system models with classical planning (like model predictive control; MPC). In this paper we add two components to this classical setup. The first is a Dyna-style policy learned on the system model using model-free techniques. We call it the guide since it guides the planner. The second component is the explorer, a strategy to expand the limited knowledge of the guide during planning. Through a rigorous ablation study we show that combination of these two ingredients is crucial for optimal performance and better data efficiency. We apply this approach with an off-policy guide and a heating explorer to improve the state of the art of benchmark systems addressing both discrete and continuous action spaces.", "ratings": "[3, 3, 5, 6]", "confidences": "[3, 3, 3, 2]", "decision": "Reject", "year": "2023"}
{"paper_id": "OZgVHzdKicb", "title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "keywords": "Reinforcement Learning, Goal Reaching, Bayesian Classification, Reward Inference", "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "ratings": "[5.0, 4.0, 5.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "DvMDIEFtyjV", "title": "CLUTR: Curriculum Learning via Unsupervised Task Representation Learning", "keywords": "['reinforcement learning', 'curriculum learning']", "abstract": "Reinforcement Learning (RL) algorithms are often known for sample inefficiency and difficult generalization. Recently, Unsupervised Environment Design (UED) emerged as a new paradigm for zero-shot generalization by simultaneously learning a task distribution and agent policies on the sampled tasks. This is a non-stationary process where the task distribution evolves along with agent policies; creating an instability over time. While past works demonstrated the potential of such approaches, sampling effectively from the task space remains an open challenge, bottlenecking these approaches. To this end, we introduce CLUTR: a novel curriculum learning algorithm that decouples task representation and curriculum learning into a two-stage optimization. It first trains a recurrent variational autoencoder on randomly generated tasks to learn a latent task manifold. Next, a teacher agent creates a curriculum by optimizing a minimax REGRET-based objective on a set of latent tasks sampled from this manifold. By keeping the task manifold fixed, we show that CLUTR successfully overcomes the non-stationarity problem and improves stability. Our experimental results show CLUTR outperforms PAIRED, a principled and popular UED method, in terms of generalization and sample efficiency in the challenging CarRacing and navigation environments: showing an 18x improvement on the F1 CarRacing benchmark. CLUTR also performs comparably to the non-UED state-of-the-art for CarRacing, outperforming it in nine of the 20 tracks. CLUTR also achieves a 33% higher solved rate than PAIRED on a set of 18 out-of-distribution navigation tasks.", "ratings": "[3, 5, 5]", "confidences": "[4, 2, 5]", "decision": "Reject", "year": "2023"}
{"paper_id": "JI2TGOehNT0", "title": "Combining Imitation and Reinforcement Learning with Free Energy Principle", "keywords": "Imitation, Reinforcement Learning, Free Energy Principle", "abstract": "Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs are often introduced as separate problems, but a more realistic problem setting is how to merge the techniques so that the agent can reduce exploration costs by partially imitating experts at the same time it maximizes its return. Even when the experts are suboptimal (e.g. Experts learned halfway with other RL methods or human-crafted experts), it is expected that the agent outperforms the suboptimal experts\u2019 performance. In this paper, we propose to address the issue by using and theoretically extending Free Energy Principle, a unified brain theory that explains perception, action and model learning in a Bayesian probabilistic way. Our results show that our approach achieves at least equivalent or better performance than standard IL or RL in visual control tasks with sparse rewards.", "ratings": "[5.0, 5.0, 6.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "qbRv1k2AcH", "title": "Learning to Reason in Large Theories without Imitation", "keywords": "reinforcement learning, thoerem proving, exploration, mathematics", "abstract": "In this paper, we demonstrate how to do automated higher-order logic theorem proving in the presence of a large knowledge base of potential premises without learning from human proofs. We augment the exploration of premises based on a simple tf-idf (term frequency-inverse document frequency) based lookup in a deep reinforcement learning scenario. Our experiments show that our theorem prover trained with this exploration mechanism but no human proofs, dubbed DeepHOL Zero, outperforms provers that are trained only on human proofs. It approaches the performance of a prover trained by a combination of imitation and reinforcement learning. We perform multiple experiments to understand the importance of the underlying assumptions that make our exploration approach work, thus explaining our design choices.", "ratings": "[4.0, 6.0, 6.0, 6.0, 6.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "0W1TQ_hoMFN", "title": "Policy Architectures for Compositional Generalization in Control", "keywords": "['Reinforcement Learning', 'Imitation Learning', 'Compositionality']", "abstract": "Several tasks in control, robotics, and planning can be specified through desired goal configurations for entities in the environment. Learning goal-conditioned policies is a natural paradigm to solve such tasks. However, learning and generalizing on complex tasks can be challenging due to variations in number of entities or compositions of goals. To address this challenge, we introduce the Entity-Factored Markov Decision Process (EFMDP), a formal framework for modeling the entity-based compositional structure in control tasks. Geometrical properties of the EFMDP framework provide theoretical motivation for policy architecture design, particularly Deep Sets and popular relational mechanisms such as graphs and self attention. These structured policy architectures are flexible and can be trained end-to-end with standard reinforcement and imitation learning algorithms. We study and compare the learning and generalization properties of these architectures on a suite of simulated robot manipulation tasks, finding that they achieve significantly higher success rates with less data compared to standard multilayer perceptrons. Structured policies also enable broader and more compositional generalization, producing policies that extrapolate to different numbers of entities than seen in training, and stitch together (i.e. compose) learned skills in novel ways. Video results can be found at https://sites.google.com/view/comp-gen-anon.", "ratings": "[5, 6, 8, 3]", "confidences": "[3, 3, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "m1f8XUs-RQP", "title": "Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning", "keywords": "['reinforcement learning', 'lifelong learning', 'transfer learning', 'model-based reinforcement learning']", "abstract": "Learning models of the environment from pure interaction is often considered an essential component of building lifelong reinforcement learning agents. However, the common practice in model-based reinforcement learning is to learn models that model every aspect of the agent's environment, regardless of whether they are important in coming up with optimal decisions or not. In this paper, we argue that such models are not particularly well-suited for performing scalable and robust planning in lifelong reinforcement learning scenarios and we propose new kinds of models that only model the relevant aspects of the environment, which we call minimal value-equivalent partial models. After providing the formal definitions of these models, we provide theoretical results demonstrating the scalability advantages of performing planning with minimal value-equivalent partial models and then perform experiments to empirically illustrate our theoretical results. Finally, we provide some useful heuristics on how to learn such models with deep learning architectures and empirically demonstrate that models learned in such a way can allow for performing planning that is robust to distribution shifts and compounding model errors. Overall, both our theoretical and empirical results suggest that minimal value-equivalent partial models can provide significant benefits to performing scalable and robust planning in lifelong reinforcement learning scenarios. ", "ratings": "[5, 5, 3, 3]", "confidences": "[4, 4, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "bGC7Ai125lR", "title": "Towards Understanding How Machines Can Learn Causal Overhypotheses ", "keywords": "['causal reasoning', 'intervention', 'causal overhypotheses', 'Reinforcement learning', 'gpt-3']", "abstract": "Recent work in machine learning and cognitive science has suggested that understanding causal information is essential to the development of intelligence. One of the key challenges for current machine learning algorithms is modeling and understanding causal overhypotheses: transferable abstract hypotheses about sets of causal relationships. In contrast, even young children spontaneously learn causal overhypotheses, and use these to guide their exploration or to generalize to new situations. This has been demonstrated in a variety of cognitive science experiments using the \u201cblicket detector\u201d environment. We present a causal learning benchmark adapting the \u201cblicket\" environment for machine learning agents and evaluate a range of state-of-the-art methods in this environment. We find that although most agents have no problem learning causal structures seen during training, they are unable to learn causal overhypotheses from these experiences, and thus cannot generalize to new settings. ", "ratings": "[6, 3, 5]", "confidences": "[3, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "JYQYysrNT3M", "title": "Reinforcement Learning with Ex-Post Max-Min Fairness", "keywords": "['Reinforcement learning', 'fairness', 'regret minimization', 'multi-objective optimization', 'constrained Markov decision processes']", "abstract": "We consider reinforcement learning with vectorial rewards, where the agent receives a vector of $K\\geq 2$ different types of rewards at each time step. The agent aims to maximize the minimum total reward among the $K$ reward types. Different from existing works that focus on maximizing the minimum expected total reward, i.e. \\emph{ex-ante max-min fairness}, we maximize the expected minimum total reward, i.e. \\emph{ex-post max-min fairness}. Through an example and numerical experiments, we show that the optimal policy for the former objective generally does not converge to optimality under the latter, even as the number of time steps $T$ grows. Our main contribution is a novel algorithm, Online-ReOpt, that achieves near-optimality under our objective, assuming an optimization oracle that returns a near-optimal policy given any scalar reward. The expected objective value under Online-ReOpt is shown to converge to the asymptotic optimum as $T$ increases. Finally, we propose offline variants to ease the burden of online computation in Online-ReOpt, and we propose generalizations from the max-min objective to concave utility maximization.", "ratings": "[5.0, 3.0, 5.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "YqHW0o9wXae", "title": "Assisted Learning for Organizations with Limited Imbalanced Data", "keywords": "['Assisted learning', 'Deep learning', 'Reinforcement learning', 'Optimization', 'Heterogeneous learner', 'Imbalanced data']", "abstract": "We develop an assisted learning framework for assisting organization-level learners to improve their learning performance with limited and imbalanced data. In particular, learners at the organization level usually have sufficient computation resource, but are subject to stringent collaboration policy and information privacy. Their limited imbalanced data often cause biased inference and sub-optimal decision-making. In our assisted learning framework, an organizational learner purchases assistance service from a service provider and aims to enhance its model performance within a few assistance rounds. We develop effective stochastic training algorithms for assisted deep learning and assisted reinforcement learning. Different from existing distributed algorithms that need to frequently transmit gradients or models, our framework allows the learner to only occasionally share information with the service provider, and still achieve a near-oracle model as if all the data were centralized.", "ratings": "[3.0, 5.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "xxWl2oEvP2h", "title": "Rewriting by Generating: Learn Heuristics for Large-scale Vehicle Routing Problems", "keywords": "vehicle routing problem, reinforcement learning, optimization", "abstract": "The large-scale vehicle routing problem is defined based on the classical VRP with usually more than one thousand customers. It is of great importance to find an efficient and high-qualified solution. However, current algorithms of VRP including heuristics and RL based methods, only performs well on small-scale instances with usually no more than a hundred customers, and fails to solve large-scale VRP due to either high computation cost or the explosive exploration space that results in model divergence. Based on the classical idea of Divide-and-Conquer, we present a novel Rewriting-by-Generating(RBG) framework with hierarchical RL agents to solve the large-scale VRP. RBG contains a rewriter agent that refines the customer division and an elementary generator to generate regional solutions individually. Extensive experiments demonstrate our RBG framework has significant performance on large-scale VRP. RBG outperforms LKH3, the state-of-the-art method in solving VRP, by 2.43% with the problem size of N=2000 and could infer solutions about 100 times faster.", "ratings": "[7.0, 4.0, 6.0, 6.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "NrN8XarA2Iz", "title": "Learning to Dynamically Select Between Reward Shaping Signals", "keywords": "selection, automatic, reward, shaping, reinforcement learning", "abstract": "Reinforcement learning (RL) algorithms often have the limitation of sample complexity. Previous research has shown that the reliance on large amounts of experience can be mitigated through the presence of additional feedback. Automatic reward shaping is one approach to solving this problem, using automatic identification and modulation of shaping reward signals that are more informative about how agents should behave in any given scenario to learn and adapt faster. However, automatic reward shaping is still very challenging. To better study it, we break it down into two separate sub-problems: learning shaping reward signals in an application and learning how the signals can be adaptively used to provide a single reward feedback in the RL learning process. This paper focuses on the latter sub-problem. Unlike existing research, which tries to learn one shaping reward function from shaping signals, the proposed method learns to dynamically select the right reward signal to apply at each state, which is considerably more flexible. We further show that using an online strategy that seeks to match the learned shaping feedback with optimal value differences can lead to effective reward shaping and accelerated learning. The proposed ideas are verified through experiments in a variety of environments using different shaping reward paradigms.", "ratings": "[4.0, 4.0, 2.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "Krk0Gnft2Zc", "title": "Discrete State-Action Abstraction via the Successor Representation", "keywords": "['reinforcement learning', 'abstraction', 'successor representation', 'options', 'discrete', 'sparse reward', 'representation learning', 'intrinsic motivation']", "abstract": "While the difficulty of reinforcement learning problems is typically related to the complexity of their state spaces, Abstraction proposes that solutions often lie in simpler underlying latent spaces. Prior works have focused on learning either a continuous or dense abstraction, or require a human to provide one. Information-dense representations capture features irrelevant for solving tasks, and continuous spaces can struggle to represent discrete objects. In this work we automatically learn a sparse discrete abstraction of the underlying environment. We do so using a simple end-to-end trainable model based on the successor representation and max-entropy regularization. We describe an algorithm to apply our model, named Discrete State-Action Abstraction (DSAA), which computes an action abstraction in the form of temporally extended actions, i.e., Options, to transition between discrete abstract states. Empirically, we demonstrate the effects of different exploration schemes on our resulting abstraction, and show that it is efficient for solving downstream tasks.", "ratings": "[5, 3, 8, 3]", "confidences": "[4, 4, 4, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "ELmZduELxm", "title": "UTS: When Monotonic Value Factorisation Meets Non-monotonic and Stochastic Targets", "keywords": "['Value Decomposition', 'Multi-Agent Reinforcement Learning']", "abstract": "Extracting decentralised policies from joint action-values is an attractive way to exploit centralised learning. It is possible to apply monotonic value factorisation to guarantee consistency between the centralised and decentralised policies. However, the best strategy for training decentralised policies when the target joint action-values are non-monotonic and stochastic is still unclear. We propose a novel value factorisation method named uncertainty-based target shaping (UTS) to solve this problem. UTS employs networks that estimate the reward and the following state's embedding, where the large prediction error indicates that the target is stochastic. By replacing deterministic targets for the suboptimal with the best per-agent values, we enforce that all shaped targets become a subset of the space that can be represented by monotonic value factorisation. Empirical results show that UTS outperforms state-of-the-art baselines on multiple benchmarks, including matrix games, predator-prey, and challenging tasks in the StarCraft II micromanagement.", "ratings": "[6, 3, 3]", "confidences": "[4, 5, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "mF5tmqUfdsw", "title": "Zeroth-Order Actor-Critic", "keywords": "['reinforcement learning', 'zeroth-order optimization', 'actor-critic']", "abstract": "Evolution based zeroth-order optimization methods and policy gradient based first-order methods are two promising alternatives to solve reinforcement learning (RL) problems with complementary advantages. The former work with arbitrary policies, drive state-dependent and temporally-extended exploration, possess robustness-seeking property, but suffer from high sample complexity, while the latter are more sample efficient but restricted to differentiable policies and the learned policies are less robust. We propose Zeroth-Order Actor-Critic algorithm (ZOAC) that unifies these two methods into an on-policy actor-critic architecture to preserve the advantages from both. ZOAC conducts rollouts collection with timestep-wise perturbation in parameter space, first-order policy evaluation (PEV) and zeroth-order policy improvement (PIM) alternately in each iteration. The modified rollouts collection strategy and the introduced critic network help to reduce the variance of zeroth-order gradient estimators and improve the sample efficiency and stability of the algorithm. We evaluate our proposed method using two different types of policies, linear policies and neural networks, on a range of challenging continuous control benchmarks, where ZOAC outperforms zeroth-order and first-order baseline algorithms.", "ratings": "[6.0, 6.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "rgp4_59eC0", "title": "Representation Interference Suppression via Non-linear Value Factorization for Indecomposable Markov Games", "keywords": "['Multi-agent Reinforcement Learning']", "abstract": "Value factorization is an efficient approach for centralized training with decentralized execution in cooperative multi-agent reinforcement learning tasks. As the simplest implementation of value factorization, Linear Value Factorization (LVF) attracts wide attention. In this paper, firstly, we investigate the applicable conditions of LVF, which is important but usually neglected by previous works. We prove that due to the representation limitation, LVF is only perfectly applicable to an extremely narrow class of tasks, which we define as the decomposable Markov game. Secondly, to handle the indecomposable Markov game where the LVF is inapplicable, we turn to value factorization with complete representation capability (CRC) and explore the general form of the value factorization function that satisfies both Independent Global Max (IGM) and CRC conditions. A common problem of these value factorization functions is the representation interference among true Q values with shared local Q value functions. As a result, the policy could be trapped in local optimums due to the representation interference on the optimal true Q values. Thirdly, to address the problem, we propose a novel value factorization method, namely Q Factorization with Representation Interference Suppression (QFRIS). QFRIS adaptively reduces the gradients of the local Q value functions contributed by the non-optimal true Q values. Our method is evaluated on various benchmarks. Experimental results demonstrate the good convergence of QFIRS.", "ratings": "[1, 6, 5, 1]", "confidences": "[4, 2, 3, 5]", "decision": "Reject", "year": "2023"}
{"paper_id": "Gkbxt7ThQxU", "title": "Explicitly Maintaining Diverse Playing Styles in Self-Play", "keywords": "['Reinforcement learning', 'evolutionary algorithm', 'self-play', 'diverse playing styles', 'high skill levels']", "abstract": "Self-play has proven to be an effective training schema to obtain a high-level agent in complex games through iteratively playing against an opponent from its historical versions. However, its training process may prevent it from generating a well-generalised policy since the trained agent rarely encounters diversely-behaving opponents along its own historical path. In this paper, we aim to improve the generalisation of the policy by maintaining a population of agents with diverse playing styles and high skill levels throughout the training process. Specifically, we propose a bi-objective optimisation model to simultaneously optimise the agents' skill level and playing style. A feature of this model is that we do not regard the skill level and playing style as two objectives to maximise directly since they are not equally important (i.e., agents with diverse playing styles but low skill levels are meaningless). Instead, we create a meta bi-objective model to enable high-level agents with diverse playing styles more likely to be incomparable (i.e. Pareto non-dominated), thereby playing against each other through the training process. We then present an evolutionary algorithm working with the proposed model. Experiments in a classic table tennis game Pong and a commercial role-playing game Justice Online show that our algorithm can learn a well generalised policy and at the same time is able to provide a set of high-level policies with various playing styles.", "ratings": "[3, 6, 3]", "confidences": "[4, 4, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "Uqu9yHvqlRf", "title": "What Preserves the Emergence of Language?", "keywords": "emergence of language, reinforcement learning", "abstract": "The emergence of language is a mystery. One dominant theory is that cooperation boosts language to emerge. However, as a means of giving out information, language seems not to be an evolutionarily stable strategy. To ensure the survival advantage of many competitors, animals are selfish in nature. From the perspective of Darwinian, if an individual can obtain a higher benefit by deceiving the other party, why not deceive? For those who are cheated, once bitten and twice shy, cooperation will no longer be a good option. As a result, motivation for communication, as well as the emergence of language would perish. Then, what preserves the emergence of language? We aim to answer this question in a brand new framework of agent community, reinforcement learning, and natural selection. Empirically, we reveal that lying indeed dispels cooperation. Even with individual resistance to lying behaviors, liars can easily defeat truth tellers and survive during natural selection. However, social resistance eventually constrains lying and makes the emergence of language possible.", "ratings": "[6.0, 5.0, 3.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "BduNVoPyXBK", "title": "Task-driven Discovery of Perceptual Schemas for Generalization in Reinforcement Learning", "keywords": "['deep reinforcement learning', 'reinforcement learning', 'deep learning', 'compositional generalization', 'generalization', 'recurrent architecture']", "abstract": "Deep reinforcement learning (Deep RL) has recently seen significant progress in developing algorithms for generalization. However, most algorithms target a single type of generalization setting. In this work, we study generalization across three disparate task structures: (a) tasks composed of spatial and temporal compositions of regularly occurring object motions; (b) tasks composed of active perception of and navigation towards regularly occurring 3D objects; and (c) tasks composed of navigating through sequences of regularly occurring object-configurations. These diverse task structures all share an underlying idea of compositionality: task completion always involves combining reoccurring segments of task-oriented perception and behavior. We hypothesize that an agent can generalize within a task structure if it can discover representations that capture these reoccurring task-segments. For our tasks, this corresponds to representations for recognizing individual object motions, for navigation towards 3D objects, and for navigating through object-configurations. Taking inspiration from cognitive science, we term representations for reoccurring segments of an agent's experience, \"perceptual schemas\". We propose Composable Perceptual Schemas (CPS), which learns a composable state representation where perceptual schemas are distributed across multiple, relatively small recurrent \"subschema\" modules. Our main technical novelty is an expressive attention function that enables subschemas to dynamically attend to features shared across all positions in the agent's observation. Our experiments indicate our feature-attention mechanism enables CPS to generalize better than recurrent architectures that attend to observations with spatial attention.", "ratings": "[5.0, 6.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "RW_GTtTfHJ6", "title": "Causal Reinforcement Learning using Observational and Interventional Data", "keywords": "['reinforcement learning', 'causality', 'confounding']", "abstract": "Learning efficiently a causal model of the environment is a key challenge of model-based RL agents operating in POMDPs. We consider here a scenario where the learning agent has the ability to collect online experiences through direct interactions with the environment (interventional data), but also has access to a large collection of offline experiences, obtained by observing another agent interacting with the environment (observational data). A key ingredient, which makes this situation non-trivial, is that we allow the observed agent to act based on privileged information, hidden from the learning agent. We then ask the following questions: can the online and offline experiences be safely combined for learning a causal transition model ? And can we expect the offline experiences to improve the agent's performances ? To answer these, first we bridge the fields of reinforcement learning and causality, by importing ideas from the well-established causal framework of do-calculus, and expressing model-based reinforcement learning as a causal inference problem. Second, we propose a general yet simple methodology for safely leveraging offline data during learning. In a nutshell, our method relies on learning a latent-based causal transition model that explains both the interventional and observational regimes, and then inferring the standard POMDP transition model via deconfounding using the recovered latent variable. We prove our method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case), and we assess its effectiveness empirically on a series of synthetic toy problems.", "ratings": "[5.0, 5.0, 5.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "GOr80bgf52v", "title": "Factored World Models for Zero-Shot Generalization in Robotic Manipulation", "keywords": "['reinforcement learning', 'world models', 'robotic manipulation', 'zero-shot transfer']", "abstract": "World models for environments with many objects face a combinatorial explosion of states: as the number of objects increases, the number of possible arrangements grows exponentially. In this paper, we learn to generalize over robotic pick-and-place tasks using object-factored world models, which combat the combinatorial explosion by ensuring that predictions are equivariant to permutations of objects. We build on one such model, C-SWM, which we extend to overcome the assumption that each action is associated with one object. To do so, we introduce an action attention module to determine which objects are likely to be affected by an action. The attention module is used in conjunction with a residual graph neural network block that receives action information at multiple levels. Based on RGB images and parameterized motion primitives, our model can accurately predict the dynamics of a robot building structures from blocks of various shapes. Our model generalizes over training structures built in different positions. Moreover crucially, the learned model can make predictions about tasks not represented in training data. That is, we demonstrate successful zero-shot generalization to novel tasks. For example, we measure only 2.4% absolute decrease in our action ranking metric in the case of a block assembly task.", "ratings": "[6.0, 5.0, 5.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "-YAqAIsxr7v", "title": "OVD-Explorer: A General Information-theoretic Exploration Approach for Reinforcement Learning", "keywords": "['Exploration', 'Uncertainty', 'Reinforcement Learning']", "abstract": "Many exploration strategies are built upon the optimism in the face of the uncertainty (OFU) principle for reinforcement learning. However, without considering the aleatoric uncertainty, existing methods may over-explore the state-action pairs with large randomness and hence are non-robust. In this paper, we explicitly capture the aleatoric uncertainty from a distributional perspective and propose an information-theoretic exploration method named Optimistic Value Distribution Explorer (OVD-Explorer). OVD-Explorer follows the OFU principle, but more importantly, it avoids exploring the areas with high aleatoric uncertainty through maximizing the mutual information between policy and the upper bounds of policy's returns. Furthermore, to make OVD-Explorer tractable for continuous RL, we derive a closed form solution, and integrate it with SAC, which, to our knowledge, for the first time alleviates the negative impact on exploration caused by aleatoric uncertainty for continuous RL. Empirical evaluations on the commonly used Mujoco benchmark and a novel GridChaos task demonstrate that OVD-Explorer can alleviate over-exploration and outperform state-of-the-art methods.", "ratings": "[3.0, 6.0, 3.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "n5yBuzpqqw", "title": "Error Controlled Actor-Critic Method to Reinforcement Learning", "keywords": "reinforcement learning, actor-critic, function approximation, approximation error, KL divergence", "abstract": "In the reinforcement learning (RL) algorithms which incorporate function approximation methods, the approximation error of value function inevitably cause overestimation phenomenon and have a negative impact on the convergence of the algorithms. To mitigate the negative effects of approximation error, we propose a new actor-critic algorithm called Error Controlled Actor-critic which ensures confining the approximation error in value function. In this paper, we firstly present an analysis of how the approximation error can hinder the optimization process of actor-critic methods. Then, we derive an upper boundary of the approximation error of $Q$ function approximator, and found that the error can be lowered by placing restrictions on the KL-divergence between every two consecutive policies during the training phase of the policy. The results of experiments on a range of continuous control tasks from OpenAI gym suite demonstrate that the proposed actor-critic algorithm apparently reduces the approximation error and significantly outperforms other model-free RL algorithms.", "ratings": "[6.0, 3.0, 3.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "amRmtfpYgDt", "title": "Regioned Episodic Reinforcement Learning", "keywords": "Deep Reinforcement Learning, Episodic Memory, Sample Efficiency", "abstract": "Goal-oriented reinforcement learning algorithms are often good at exploration, not exploitation, while episodic algorithms excel at exploitation, not exploration. As a result, neither of these approaches alone can lead to a sample efficient algorithm in complex environments with high dimensional state space and delayed rewards. Motivated by these observations and shortcomings, in this paper, we introduce Regioned Episodic Reinforcement Learning (RERL) that combines the episodic and goal-oriented learning strengths and leads to a more sample efficient and ef- fective algorithm. RERL achieves this by decomposing the space into several sub-space regions and constructing regions that lead to more effective exploration and high values trajectories. Extensive experiments on various benchmark tasks show that RERL outperforms existing methods in terms of sample efficiency and final rewards.", "ratings": "[4.0, 5.0, 5.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "Tg9AvNbTUJo", "title": "$Q$-learning with regularization converges with non-linear non-stationary features", "keywords": "['Q-learning', 'Reinforcement Learning', 'Stochastic Approximation']", "abstract": "The deep $Q$-learning architecture is a neural network composed of non-linear hidden layers that learn features of states and actions and a final linear layer that learns the $Q$-values of the features. The parameters of both components can possibly diverge. Regularization of the updates is known to solve the divergence problem of fully linear architectures, where features are stationary and known a priori. We propose a deep $Q$-learning scheme that uses regularization of the final linear layer of architecture, updating it along a faster time-scale, and stochastic full-gradient descent updates for the non-linear features at a slower time-scale. We prove the proposed scheme converges with probability 1. Finally, we provide a bound on the error introduced by regularization of the final linear layer of the architecture.", "ratings": "[3, 6, 3]", "confidences": "[3, 2, 5]", "decision": "Reject", "year": "2023"}
{"paper_id": "P1zfguZHowl", "title": "Robust Losses for Learning Value Functions", "keywords": "['Reinforcement Learning']", "abstract": "Most value function learning algorithms in reinforcement learning are based on the mean squared (projected) Bellman error. However, squared errors are known to be sensitive to outliers, both skewing the solution of the objective and resulting in high-magnitude and high-variance gradients. Typical strategies to control these high-magnitude updates in RL involve clipping gradients, clipping rewards, rescaling rewards, and clipping errors. Clipping errors is related to using robust losses, like the Huber loss, but as yet no work explicitly formalizes and derives value learning algorithms with robust losses. In this work, we build on recent insights reformulating squared Bellman errors as a saddlepoint optimization problem, and propose a saddlepoint reformulation for a Huber Bellman error and Absolute Bellman error. We show that the resulting solutions have significantly lower error for certain problems and are otherwise comparable, in terms of both absolute and squared value error. We show that the resulting gradient-based algorithms are more robust, for both prediction and control, with less stepsize sensitivity.", "ratings": "[6.0, 6.0, 8.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "lTb1kzFA84J", "title": "Parameterized projected Bellman operator", "keywords": "['reinforcement learning', 'bellman operator', 'operator learning', 'approximate value iteration']", "abstract": "The Bellman operator is a cornerstone of reinforcement learning, widely used in a plethora of works, from value-based methods to modern actor-critic approaches. In problems with unknown models, the Bellman operator requires transition samples that strongly determine its behavior, as uninformative samples can result in negligible updates or long detours before reaching the fixed point. In this work, we introduce the novel idea of obtaining an approximation of the Bellman operator, which we call projected Bellman operator (PBO). Our PBO is a parametric operator on the parameter space of a given value function. Given the parameters of a value function, PBO outputs the parameters of a new value function and converges to a fixed point in the limit, as a standard Bellman operator. Notably, our PBO can approximate repeated applications of the true Bellman operator at once, as opposed to the sequential nature of the standard Bellman operator. We prove the important consequences of this finding for different classes of problems by analyzing PBO in terms of stability, convergence, and approximation error. Eventually, we propose an approximate value-iteration algorithm to show how PBO can overcome the limitations of classical methods, opening up multiple research directions as a novel paradigm in reinforcement learning.", "ratings": "[6, 3, 6, 5]", "confidences": "[3, 5, 4, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "w1w4dGJ4qV", "title": "The Benefits of Model-Based Generalization in Reinforcement Learning", "keywords": "['model-based reinforcement learning', 'generalization']", "abstract": "Model-Based Reinforcement Learning (RL) is widely believed to have the potential to improve sample efficiency by allowing an agent to synthesize large amounts of imagined experience. Experience Replay (ER) can be considered a simple kind of model, which has proved extremely effective at improving the stability and efficiency of deep RL. In principle, a learned parametric model could improve on ER by generalizing from real experience to augment the dataset with additional plausible experience. However, owing to the many design choices involved in empirically successful algorithms, it can be very hard to establish where the benefits are actually coming from. Here, we provide theoretical and empirical insight into when, and how, we can expect data generated by a learned model to be useful. First, we provide a general theorem motivating how learning a model as an intermediate step can narrow down the set of possible value functions more than learning a value function directly from data using the Bellman equation. Second, we provide an illustrative example showing empirically how a similar effect occurs in a more concrete setting with neural network function approximation. Finally, we provide extensive experiments showing the benefit of model-based learning for online RL in environments with combinatorial complexity, but factored structure that allows a learned model to generalize. In these experiments, we take care to control for other factors in order to isolate, insofar as possible, the benefit of using experience generated by a learned model relative to ER alone.", "ratings": "[8, 6, 5, 5]", "confidences": "[4, 4, 5, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "S9MPX7ejmv", "title": "Approximating Pareto Frontier through Bayesian-optimization-directed Robust Multi-objective Reinforcement Learning", "keywords": "Reinforcement Learning, Multi\u2013objective Optimization, Adversarial Machine Learning, Bayesian Optimization", "abstract": "Many real-word decision or control problems involve multiple conflicting objectives and uncertainties, which requires learned policies are not only Pareto optimal but also robust. In this paper, we proposed a novel algorithm to approximate a representation for robust Pareto frontier through Bayesian-optimization-directed robust multi-objective reinforcement learning (BRMORL). Firstly, environmental uncertainty is modeled as an adversarial agent over the entire space of preferences by incorporating zero-sum game into multi-objective reinforcement learning (MORL). Secondly, a comprehensive metric based on hypervolume and information entropy is presented to evaluate convergence, diversity and evenness of the distribution for Pareto solutions. Thirdly, the agent\u2019s learning process is regarded as a black-box, and the comprehensive metric we proposed is computed after each episode of training, then a Bayesian optimization (BO) algorithm is adopted to guide the agent to evolve towards improving the quality of the approximated Pareto frontier. Finally, we demonstrate the effectiveness of proposed approach on challenging high-dimensional multi-objective tasks across several environments, and show our scheme can produce robust policies under environmental uncertainty.", "ratings": "[3.0, 5.0, 5.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "O5Wr-xX0U2y", "title": "Deep Reinforcement Learning for Equal Risk Option Pricing and Hedging under Dynamic Expectile Risk Measures", "keywords": "['Deep reinforcement learning', 'risk averse Markov decision processes', 'expectile risk measures', 'derivative pricing']", "abstract": "Recently equal risk pricing, a framework for fair derivative pricing, was extended to consider coherent risk measures. However, all current implementations either employ a static risk measure or are based on traditional dynamic programming solution schemes that are impracticable in realistic settings: when the number of underlying assets is large  or only historical trajectories are available. This paper extends for the first time the deep deterministic policy gradient algorithm to the problem of solving a risk averse Markov decision process that models risk using a time consistent dynamic expectile risk measure. Our numerical experiments, which involve both a simple vanilla option and a more exotic basket option, confirm that the new ACRL algorithm can produce high quality hedging strategies that produce accurate prices in simple settings, and outperform the strategies produced using static risk measures when the risk is evaluated at later points of time.", "ratings": "[5.0, 6.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "vT0NSQlTA", "title": "Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via Latent Model Ensembles", "keywords": "Model-Based Reinforcement Learning, Deep Exploration, Continuous Visual Control, UCB, Latent Space, Ensembling", "abstract": "Learning complex behaviors through interaction requires coordinated long-term planning. Random exploration and novelty search lack task-centric guidance and waste effort on non-informative interactions. Instead, decision making should target samples with the potential to optimize performance far into the future, while only reducing uncertainty where conducive to this objective. This paper presents latent optimistic value exploration (LOVE), a strategy that enables deep exploration through optimism in the face of uncertain long-term rewards. We combine finite-horizon rollouts from a latent model with value function estimates to predict infinite-horizon returns and recover associated uncertainty through ensembling. Policy training then proceeds on an upper confidence bound (UCB) objective to identify and select the interactions most promising to improve long-term performance. We apply LOVE to continuous visual control tasks and demonstrate improved sample complexity on a selection of benchmarking tasks.", "ratings": "[5.0, 4.0, 6.0, 6.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "UECzHrGio7i", "title": "Robust Imitation Learning from Corrupted Demonstrations", "keywords": "['Robust Estimation', 'Imitation Learning', 'Reinforcement Learning']", "abstract": "We consider offline Imitation Learning  from corrupted demonstrations where a constant fraction of data can be noise or even arbitrary outliers. Classical approaches such as Behavior Cloning assumes that demonstrations are collected by an presumably optimal expert, hence         may fail drastically when learning from corrupted demonstrations. We propose a novel robust algorithm by minimizing a Median-of-Means (MOM) objective which guarantees the accurate estimation of policy, even in the presence of constant fraction of outliers.          Our theoretical analysis shows that our robust method in the corrupted setting enjoys nearly the same error scaling and sample complexity guarantees as the classical Behavior Cloning in the expert demonstration setting. Our experiments on continuous-control benchmarks validate that existing algorithms are fragile under corrupted demonstration while our method exhibits the predicted robustness and effectiveness.", "ratings": "[5.0, 3.0, 5.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "Gc4MQq-JIgj", "title": "RECONNAISSANCE FOR REINFORCEMENT LEARNING WITH SAFETY CONSTRAINTS", "keywords": "Reinforcement Learning, Safety constraints, Constrained Markov Decision Process", "abstract": "Practical reinforcement learning problems are often formulated as constrained Markov decision process (CMDP) problems, in which the agent has to maximize the expected return while satisfying a set of prescribed safety constraints. In this study, we consider a situation in which the agent has access to the generative model which provides us with a next state sample for any given state-action pair, and propose a model to solve a CMDP problem by decomposing the CMDP into a pair of MDPs; \\textit{reconnaissance} MDP (R-MDP) and \\textit{planning} MDP (P-MDP). In R-MDP, we train threat function, the Q-function analogue of danger that can determine whether a given state-action pair is safe or not. In P-MDP, we train a reward-seeking policy while using a fixed threat function to determine the safeness of each action. With the help of generative model, we can efficiently train the threat function by preferentially sampling rare dangerous events. Once the threat function for a baseline policy is computed, we can solve other CMDP problems with different reward and different danger-constraint without the need to re-train the model. We also present an efficient approximation method for the threat function that can greatly reduce the difficulty of solving R-MDP. We will demonstrate the efficacy of our method over classical approaches in benchmark dataset and complex collision-free navigation tasks.", "ratings": "[7.0, 5.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "W75l6XMzLq", "title": "Hindsight Curriculum Generation Based Multi-Goal Experience Replay", "keywords": "reinforcement learning, multi-goal task, experience replay", "abstract": "In multi-goal tasks with sparse rewards, it is challenging to learn from tons of experiences with zero rewards. Hindsight experience replay (HER), which replays past experiences with additional heuristic goals, has shown it possible for off-policy reinforcement learning (RL) to make use of failed experiences. However, the replayed experiences may not lead to well-explored state-action pairs, especially for a pseudo goal, which instead results in a poor estimate of the value function. To tackle the problem, we propose to resample hindsight experiences based on their likelihood under the current policy and the overall distribution. Based on the hindsight strategy, we introduce a novel multi-goal experience replay method that automatically generates a training curriculum, namely Hindsight Curriculum Generation (HCG). As the range of experiences expands, the generated curriculum strikes a dynamic balance between exploiting and exploring. We implement HCG with the vanilla Deep Deterministic Policy Gradient(DDPG), and experiments on several tasks with sparse binary rewards demonstrate that HCG improves sample efficiency of the state of the art.", "ratings": "[3.0, 4.0, 4.0, 3.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "vFvw8EzQNLy", "title": "Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning", "keywords": "['GPU-based simulation', 'off-policy learning', 'distributed training', 'reinforcement learning']", "abstract": "Reinforcement learning algorithms typically require tons of training data, resulting in long training time, especially on challenging tasks. With the recent advance in GPU-based simulation, such as Isaac Gym, data collection speed has been improved thousands of times on a commodity GPU. Most prior works have been using on-policy methods such as PPO to train policies in Isaac Gym due to its simpleness and effectiveness in scaling up. Off-policy methods are usually more sample-efficient but more challenging to be scaled up, resulting in a much longer wall-clock training time in practice. In this work, we presented a novel parallel $Q$-learning framework that not only gains better sample efficiency but also reduces the training wall-clock time compared to PPO. Different from prior works on distributed off-policy learning, such as Apex, our framework is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. We demonstrate the capability of scaling up $Q$ learning methods to tens of thousands of parallel environments. We also investigate various factors that can affect the policy learning training speed, including the number of parallel environments, exploration schemes, batch size, GPU models, etc.", "ratings": "[5, 3, 8, 5]", "confidences": "[2, 3, 3, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "-kfLEqppEm_", "title": "Convex Regularization in Monte-Carlo Tree Search", "keywords": "Monte-Carlo Tree Search, Entropy regularization, Reinforcement Learning", "abstract": "Monte-Carlo planning and Reinforcement Learning (RL) are essential to sequential decision making. The recent AlphaGo and AlphaZero algorithms have shown how to successfully combine these two paradigms in order to solve large scale sequential decision problems. These methodologies exploit a variant of the well-known UCT algorithm to trade off exploitation of good actions and exploration of unvisited states, but their empirical success comes at the cost of poor sample-efficiency and high computation time. In this paper, we overcome these limitations by studying the benefit of convex regularization in Monte-Carlo Tree Search (MCTS) to efficiently drive exploration and to improve policy updates, as already observed in RL. First, we introduce a unifying theory on the use of generic convex regularizers in MCTS, deriving the first regret analysis of regularized MCTS and showing that it guarantees exponential convergence rate. Second, we exploit our theoretical framework to introduce novel regularized backup operators for MCTS, based on the relative entropy of the policy update, and on the Tsallis entropy of the policy. Finally, we show how our framework can easily be incorporated in AlphaGo and AlphaZero, and we empirically show the superiority of convex regularization w.r.t. representative baselines, on well-known RL problems and across several Atari games.", "ratings": "[4.0, 8.0, 5.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "Fj1S0SV8p3U", "title": "Augmentation Curriculum Learning For Generalization in RL", "keywords": "['reinforcement learning', 'generalization', 'pixel-based RL', 'embodied learning']", "abstract": "Many Reinforcement Learning tasks rely solely on pixel-based observations of the environment. During deployment, these observations can fall victim to visual perturbations and distortions, causing the agent\u2019s policy to significantly degrade in performance. This motivates the need for robust agents that can generalize in the face of visual distribution shift. One common technique for doing this is to ap- ply augmentations during training; however, it comes at the cost of performance. We propose Augmentation Curriculum Learning a novel curriculum learning ap- proach that schedules augmentation into training into a weak augmentation phase and strong augmentation phase. We also introduce a novel visual augmentation strategy that proves to aid in the benchmarks we evaluate on. Our method achieves state-of-the-art performance on Deep Mind Control Generalization Benchmark.", "ratings": "[3, 5, 6, 5]", "confidences": "[4, 2, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "UYS38ssi1M", "title": "Learning GFlowNets from partial episodes for improved convergence and stability", "keywords": "['GFlowNets', 'probabilistic modeling', 'reinforcement learning']", "abstract": "Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD($\\lambda$) algorithm in reinforcement learning, we introduce subtrajectory balance or SubTB($\\lambda$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB($\\lambda$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before. We also perform a comparative analysis of stochastic gradient dynamics, shedding light on the bias-variance tradeoff in GFlowNet training and the advantages of subtrajectory balance.", "ratings": "[5, 5, 5]", "confidences": "[2, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "S0NsaRIxvQ", "title": "Adversarial Style Transfer for Robust Policy Optimization in Reinforcement Learning", "keywords": "['Deep Reinforcement Learning', 'Generalization in Reinforcement Learning']", "abstract": "This paper proposes an algorithm that aims to improve generalization for reinforcement learning agents by removing overfitting to confounding features. Our approach consists of a max-min game theoretic objective. A generator transfers the style of observation during reinforcement learning. An additional goal of the generator is to perturb the observation, which maximizes the agent's probability of taking a different action. In contrast, a policy network updates its parameters to minimize the effect of such perturbations, thus staying robust while maximizing the expected future reward. Based on this setup, we propose a practical deep reinforcement learning algorithm, Adversarial Robust Policy Optimization (ARPO), to find an optimal policy that generalizes to unseen environments. We evaluate our approach on visually enriched and diverse Procgen benchmarks. Empirically, we observed that our agent ARPO performs better in generalization and sample efficiency than a few state-of-the-art algorithms.", "ratings": "[5.0, 6.0, 8.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "1tfGKiwnJRJ", "title": "Risk-aware Bayesian RL for Cautious Exploration", "keywords": "['Reinforcement learning', 'Bayesian inference', 'Safe learning', 'Risk', 'Safety Specification']", "abstract": "This paper addresses the problem of maintaining safety during training in Reinforcement Learning (RL), such that the safety constraint violations are bounded at any point during learning. Whilst enforcing safety during training might limit the agent's exploration, we propose a new architecture that handles the trade-off between efficient progress in exploration and safety maintenance. As the agent's exploration progresses, we update Dirichlet-Categorical models of the transition probabilities of the Markov decision process that describes the agent's behavior within the environment by means of Bayesian inference. We then propose a way to approximate moments of the agent's belief about the risk associated with the agent's behavior originating from local action selection. We demonstrate that this approach can be easily coupled with RL, we provide rigorous theoretical guarantees, and we present experimental results to showcase the performance of the overall architecture.", "ratings": "[3, 5, 10, 3, 3]", "confidences": "[3, 3, 3, 5, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "FKotzp6PZJw", "title": "On the Estimation Bias in Double Q-Learning", "keywords": "Reinforcement learning, Q-learning, Estimation bias", "abstract": "Double Q-learning is a classical method for reducing overestimation bias, which is caused by taking maximum estimated values in the Bellman operator. Its variants in the deep Q-learning paradigm have shown great promise in producing reliable value prediction and improving learning performance. However, as shown by prior work, double Q-learning is not fully unbiased and still suffers from underestimation bias. In this paper, we show that such underestimation bias may lead to multiple non-optimal fixed points under an approximated Bellman operation. To address the concerns of converging to non-optimal stationary solutions, we propose a simple and effective approach as a partial fix for underestimation bias in double Q-learning. This approach leverages real returns to bound the target value. We extensively evaluate the proposed method in the Atari benchmark tasks and demonstrate its significant improvement over baseline algorithms.", "ratings": "[6.0, 3.0, 6.0, 6.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "eYm_Q5KLQr", "title": "Automatic Curriculum Generation for Reinforcement Learning in Zero-Sum Games", "keywords": "['multi-agent reinforcement learning', 'curriculum learning', 'zero-sum games']", "abstract": "Curriculum learning (CL), whose core idea is to train from easy to hard, is a popular technique to accelerate reinforcement learning (RL) training. It has also been a trend to automate the curriculum generation process. Automatic CL works primarily focus on goal-conditioned RL problems, where an explicit indicator of training progress, e.g., reward or success rate, can be used to prioritize the training tasks. However, such a requirement is no longer valid in zero-sum games: there are no goals for the agents, and the accumulative reward of the learning policy can constantly fluctuate throughout training. In this work, we present the first theoretical framework of automatic curriculum learning in the setting of zero-sum games and derive a surprisingly simple indicator of training progress, i.e., the Q-value variance, which can be directly approximated by computing the variance of value network ensembles. With such a progression metric, we further adopt a particle-based task sampler to generate initial environment configurations for training, which is particularly lightweight, computation-efficient, and naturally multi-modal. Combining these techniques with multi-agent PPO training, we obtain our final algorithm, Zero-sum Automatic Curriculum Learning (ZACL). We first evaluate ZACL in a 2D particle-world environment, where ZACL produces much stronger policies than popular RL methods for zero-sum games using the same amount of samples. Then we show in the challenging hide-and-seek environment that ZACL can lead to all four emergent phases using a single desktop computer, which is reported for the first time in the literature. The project website is at https://sites.google.com/view/zacl.", "ratings": "[3, 3, 5]", "confidences": "[4, 4, 5]", "decision": "Reject", "year": "2023"}
{"paper_id": "6ya8C6sCiD", "title": "Multi-Agent Language Learning: Symbolic Mapping", "keywords": "['emergent communication', 'multi-agent reinforcement learning']", "abstract": "The study of emergent communication has long been devoted to coax neural network agents to learn a language sharing similar properties with human language. In this paper, we try to find a natural way to help agents learn a compositional and symmetric language in complex settings like dialog games. Inspired by the theory that human language was originated from simple interactions, we hypothesize that language may evolve from simple tasks to difficult tasks. We propose a novel architecture called symbolic mapping as a basic component of the communication system of agent. We find that symbolic mapping learned in simple referential games can notably promote language learning in difficult tasks. Further, we explore vocabulary expansion, and show that with the help of symbolic mapping, agents can easily learn to use new symbols when the environment becomes more complex. All in all, we probe into how symbolic mapping helps language learning and find that a process from simplicity to complexity can serve as a natural way to help multi-agent language learning.", "ratings": "[3.0, 5.0, 6.0, 6.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "BIIwfP55pp", "title": "PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning", "keywords": "Meta-learning, Imitation Learning, Reinforcement Learning", "abstract": "Imitation learning is a natural way for a human to describe a task to an agent, and it can be combined with reinforcement learning to enable the agent to solve that task through exploration. However, traditional methods which combine imitation learning and reinforcement learning require a very large amount of interaction data to learn each new task, even when bootstrapping from a demonstration. One solution to this is to use meta reinforcement learning (meta-RL) to enable an agent to quickly adapt to new tasks at test time. In this work, we introduce a new method to combine imitation learning with meta reinforcement learning, Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). Dual inference strategies allow PERIL to precondition exploration policies on demonstrations, which greatly improves adaptation rates in unseen tasks. In contrast to pure imitation learning, our approach is capable of exploring beyond the demonstration, making it robust to task alterations and uncertainties. By exploiting the flexibility of meta-RL, we show how PERIL is capable of interpolating from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of meta-RL benchmarks under sparse rewards.", "ratings": "[4.0, 4.0, 3.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "axNDkxU9-6z", "title": "MDP Playground: Controlling Dimensions of Hardness in Reinforcement Learning", "keywords": "Reinforcement learning, Benchmarks, Efficiency, Reproducibility, Core issues, Algorithm analysis, Dimensions of hardness, OpenAI Gym", "abstract": "We present MDP Playground, an efficient benchmark for Reinforcement Learning (RL) algorithms with various dimensions of hardness that can be controlled independently to challenge algorithms in different ways and to obtain varying degrees of hardness in generated environments. We consider and allow control over a wide variety of key hardness dimensions, including delayed rewards, rewardable sequences, sparsity of rewards, stochasticity, image representations, irrelevant features, time unit, and action max. While it is very time consuming to run RL algorithms on standard benchmarks, we define a parameterised collection of fast-to-run toy benchmarks in OpenAI Gym by varying these dimensions. Despite their toy nature and low compute requirements, we show that these benchmarks present substantial challenges to current RL algorithms. Furthermore, since we can generate environments with a desired value for each of the dimensions, in addition to having fine-grained control over the environments' hardness, we also have the ground truth available for evaluating algorithms. Finally, we evaluate the kinds of transfer for these dimensions that may be expected from our benchmarks to more complex benchmarks. We believe that MDP Playground is a valuable testbed for researchers designing new, adaptive and intelligent RL algorithms and those wanting to unit test their algorithms.", "ratings": "[6.0, 4.0, 5.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "7DtgxVZGj-y", "title": "Contrastive Unsupervised Learning of World Model with Invariant Causal Features", "keywords": "['world models', 'causality', 'contrastive learning', 'model-based reinforcement learning', 'reinforcement learning', 'out-of-distribution generalisation', 'sim-to-real transfer', 'robot navigation']", "abstract": "In this paper we present a world model, which learns the causal features using invariance principle. We use contrastive unsupervised learning to learn the invariant causal features, which enforces invariance across augmentations of irrelevant parts or styles of the observation. Since the world model based reinforcement learning methods optimize representation learning and policy of the agent independently, contrastive loss collapses due to lack of supervisory signal to the representation learning module. We propose depth reconstruction as an auxiliary task to explicitly enforce the invariance and data augmentation as style intervention on the RGB space to mitigate this issue. Our design help us to leverage state-of-the-art unsupervised representation learning method to learn the world model with invariant causal features, which outperforms current state-of-the-art model-based as well as model-free reinforcement learning methods on out-of-distribution point navigation tasks on Gibson and iGibson dataset at 100k and 500k interaction step benchmarks. Further experiments on DeepMind control suite even without depth reconstruction, our proposed model performs on par with the state-of-the-art counterpart models.", "ratings": "[6, 3, 1, 3]", "confidences": "[4, 5, 3, 5]", "decision": "Reject", "year": "2023"}
{"paper_id": "NTCYXulK9qm", "title": "Co-Evolution As More Than a Scalable Alternative for Multi-Agent Reinforcement Learning", "keywords": "['reinforcement learning', 'multi-agent reinforcement learning', 'policy search', 'co-evolution', 'evolutionary algorithm']", "abstract": "In recent years, gradient based multi-agent reinforcement learning is growing in success. One contributing factor is the use of shared parameters for learning policy networks. While this approach scales well with the number of agents during execution it lacks this ambiguity for training as the number of produced samples grows linearly with the number of agents. For a very large number of agents, this could lead to an inefficient use of the circumstantial amount of produced samples. Moreover in single-agent reinforcement learning policy search with evolutionary algorithms showed viable success when sampling can be parallelized on a larger scale. The here proposed method does not only consider sampling in concurrent environments but further investigates sampling diverse parameters from the population in co-evolution in joint environments during training. This co-evolutionary policy search has shown to be capable of training a large number of agents. Beyond that, it has been shown to produce competitive results in smaller environments in comparison to gradient descent based methods. This surprising result make evolutionary algorithms a promising candidate for further research in the context of multi-agent reinforcement learning.", "ratings": "[1, 1, 3, 3]", "confidences": "[5, 3, 3, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "jK02XX9ZpJkt", "title": "CAMA: A New Framework for Safe Multi-Agent Reinforcement Learning  Using Constraint Augmentation", "keywords": "['Safe', 'Multi-agent Reinforcement Learning', 'Augmentation']", "abstract": "With the widespread application of multi-agent reinforcement learning (MARL) in real-life settings, the ability to meet safety constraints has become an urgent problem to solve. For example, it is necessary to avoid collisions to reach a common goal in controlling multiple drones. We address this problem by introducing the Constraint Augmented Multi-Agent framework --- CAMA. CAMA can serve as a plug-and-play module to the popular MARL algorithms, including centralized training, decentralized execution and independent learning frameworks. In our approach, we represent the safety constraint as the sum of discounted safety costs bounded by the predefined value, which we call the safety budget. Experiments demonstrate that CAMA can converge quickly to a high degree of constraint satisfaction and surpasses other state-of-the-art safety counterpart algorithms in both cooperative and competitive settings. ", "ratings": "[6, 5, 5, 5]", "confidences": "[3, 3, 2, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "PeT5p3ocagr", "title": "PGPS : Coupling Policy Gradient with Population-based Search", "keywords": "Reinforcement Learning, Population-based Search, Policy Gradient, Combining PG with PS", "abstract": "Gradient-based policy search algorithms (such as PPO, SAC or TD3) in deep reinforcement learning (DRL) have shown successful results on a range of challenging control tasks. However, they often suffer from flat or deceptive gradient problems. As an alternative to policy gradient methods, population-based evolutionary approaches have been applied to DRL. While population-based search algorithms show more robust learning in a broader range of tasks, they are usually inefficient in the use of samples. Recently, reported are a few attempts (such as CEMRL) to combine gradient with a population in searching optimal policy. This kind of hybrid algorithm takes advantage of both camps. In this paper, we propose yet another hybrid algorithm, which more tightly couples policy gradient with the population-based search. More specifically, we use the Cross-Entropy Method (CEM) for population-based search and Twin Delayed Deep Deterministic Policy Gradient (TD3) for policy gradient. In the proposed algorithm called Coupling Policy Gradient with Population-based Search (PGPS), a single TD3 agent, which learns by a gradient from all experiences generated by population, leads a population by providing its critic function Q as a surrogate to select better performing next-generation population from candidates. On the other hand, if the TD3 agent falls behind the CEM population, then the TD3 agent is updated toward the elite member of the CEM population using loss function augmented with the distance between the TD3 and the CEM elite. Experiments in a MuJoCo environment show that PGPS is robust to deceptive gradient and also outperforms the state-of-the-art algorithms.", "ratings": "[5.0, 3.0, 5.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "AvwF6IvT8et", "title": "Deep reinforced active learning for multi-class image classification", "keywords": "['Reinforcement learning', 'active learning', 'image classification']", "abstract": "High accuracy medical image classification can be limited by the costs of acquiring more data as well as the time and expertise needed to label existing images. In this paper, we apply active learning to medical image classification, a method which aims to maximise model performance on a minimal subset from a larger pool of data. We present a new active learning framework, based on deep reinforcement learning, to learn an active learning query strategy to label images based on predictions from a convolutional neural network. Our framework modifies the deep-Q network formulation, allowing us to pick data based additionally on geometric arguments in the latent space of the classifier, allowing for high accuracy multi-class classification in a batch-based active learning setting, enabling the agent to label datapoints that are both diverse and about which it is most uncertain. We apply our framework to two medical imaging datasets and compare with standard query strategies as well as the most recent reinforcement learning based active learning approach for image classification.", "ratings": "[3, 3, 3]", "confidences": "[4, 4, 5]", "decision": "Reject", "year": "2023"}
{"paper_id": "gZ2qq0oPvJR", "title": "Towards Finding Longer Proofs", "keywords": "automated reasoning, reinforcement learning, reasoning by analogy", "abstract": "We present a reinforcement learning (RL) based guidance system for automated theorem proving geared towards Finding Longer Proofs (FLoP). FLoP is a step towards learning to reason by analogy, reducing the dependence on large scale search in automated theorem provers. We use several simple, structured datasets with very long proofs to show that FLoP can successfully generalise a single training proof to a large class of related problems, implementing a simple form of analogical reasoning. On these benchmarks, FLoP is competitive with strong theorem provers despite using very limited search.", "ratings": "[4.0, 6.0, 8.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "jIu4hk04776", "title": "On the Geometry of Reinforcement Learning in Continuous State and Action Spaces", "keywords": "['geometry', 'deep reinforcement learning', 'manifold']", "abstract": "Advances in reinforcement learning have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens. Central to our work is the idea that the transition dynamics induce a low dimensional manifold of reachable states embedded in the high-dimensional nominal state space. We prove that, under certain conditions, the dimensionality of this manifold is at most the dimensionality of the action space plus one. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments.We further demonstrate the applicability of our result by learning a policy in this low dimensional representation. To do so we introduce an algorithm that learns a mapping to a low dimensional representation, as a narrow hidden layer of a deep neural network, in tandem with the policy using DDPG. Our experiments show that a policy learnt this way perform on par or better for four MuJoCo control suite tasks.", "ratings": "[5, 5, 8, 6]", "confidences": "[4, 3, 4, 2]", "decision": "Reject", "year": "2023"}
{"paper_id": "-uZp67PZ7p", "title": "Multi-Agent Reinforcement Learning with Shared Resource in Inventory Management", "keywords": "['Multi-Agent Reinforcement Learning', 'Inventory Management', 'Shared Resource', 'Decentralized Training Paradigm', 'Model-based RL']", "abstract": "We consider inventory management (IM) problem for a single store with a large number of SKUs (stock keeping units) in this paper, where we need to make replenishment decisions for each SKU to balance its supply and demand. Each SKU should cooperate with each other to maximize profits, as well as compete for shared resources e.g., warehouse spaces, budget etc. Co-existence of cooperation and competition behaviors makes IM a complicate game, hence IM can be naturally modelled as a multi-agent reinforcement learning (MARL) problem. In IM problem, we find that agents only interact indirectly with each other through some shared resources, e.g., warehouse spaces. To formally model MARL problems with above structure, we propose shared resource stochastic game along with an efficient algorithm to learn policies particularly for a large number of agents. By leveraging shared-resource structure, our method can greatly reduce model complexity and accelerate learning procedure compared with standard MARL algorithms, as shown by extensive experiments.", "ratings": "[6.0, 5.0, 5.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "buSCIu6izBY", "title": "Occupy & Specify: Investigations into a Maximum Credit Assignment Occupancy Objective for Data-efficient Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Intrinsic reward', 'MaxEnt', 'Probability matching', 'Motor control', 'Variational inference']", "abstract": "The capability to widely sample the state and action spaces is a key ingredient toward building effective reinforcement learning algorithms. The trade-off between exploration and exploitation generally requires the use of a data model, from which novelty bonuses are estimated and used to bias the return toward wider exploration. Surprisingly, little is known about the optimization objective followed when novelty (or entropy) bonuses are considered. Following the ``probability matching'' principle, we interpret here returns (cumulative rewards) as set points that fixate the occupancy of the state space, that is the frequency at which the different states are expected to be visited during trials. The circular dependence of the rewards sampling on the occupancy/policy makes it difficult to evaluate. We provide here a variational formulation for the matching objective, named MaCAO (Maximal Credit Assignment Occupancy) that interprets rewards as a log-likelihood on occupancy, that operates anticausally from the effects toward the causes. It is, broadly speaking, an estimation of the contribution of a state toward reaching a (future) goal. It is constructed so as to provide better convergence guaranties, with a complementary term serving as a regularizer, that, in principle, may reduce the greediness. In the absence of an explicit target occupancy, a uniform prior is used, making the regularizer consistent with a MaxEnt (Maximum Entropy) objective on states. Optimizing the entropy on states in known to be more tricky than optimizing the entropy on actions, because of an external sampling through the (unknown) environment, that prevents the propagation of a gradient. In our practical implementations, the MaxEnt regularizer is interpreted as a TD-error rather than a reward, making it possible to define an update in both the discrete and continuous cases. It is implemented on an actor-critic off-policy setup with a replay buffer, using gradient descent on a multi-layered neural network, and shown to provide significant increase in the sampling efficacy, that reflects in a reduced training time and higher returns on a set of classical motor learning benchmarks, in both the dense and the sparse rewards cases.", "ratings": "[3.0, 1.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "QnzSSoqmAvB", "title": "Playing Nondeterministic Games through Planning with a Learned Model", "keywords": "reinforcement learning, alphazero, muzero, mcts, planning, search", "abstract": "The MuZero algorithm is known for achieving high-level performance on traditional zero-sum two-player games of perfect information such as chess, Go, and shogi, as well as visual, non-zero sum, single-player environments such as the Atari suite. Despite lacking a perfect simulator and employing a learned model of environmental dynamics,  MuZero produces game-playing agents comparable to its predecessor AlphaZero. However, the current implementation of MuZero is restricted only to deterministic environments. This paper presents Nondeterministic MuZero (NDMZ), an extension of MuZero for nondeterministic, two-player, zero-sum games of perfect information. Borrowing from Nondeterministic Monte Carlo Tree Search and the theory of extensive-form games, NDMZ formalizes chance as a player in the game and incorporates it into the MuZero network architecture and tree search. Experiments show that NDMZ is capable of learning effective strategies and an accurate model of the game.", "ratings": "[3.0, 4.0, 6.0, 5.0, 7.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "psNSQsmd4JI", "title": "Containerized Distributed Value-Based Multi-Agent Reinforcement Learning", "keywords": "['Multi-agent reinforcement learning', 'Distributed reinforcement learning']", "abstract": "Multi-agent reinforcement learning tasks put a high demand on the volume of training samples. Different from its single-agent counterpart, distributed value-based multi-agent reinforcement learning faces the unique challenges of demanding data transfer, inter-process communication management, and high requirement of exploration. We propose a containerized learning framework to solve these problems. We pack several environment instances, a local learner and buffer, and a carefully designed multi-queue manager which avoids blocking into a container. Local policies of each container are encouraged to be as diverse as possible, and only trajectories with highest priority are sent to a global learner. In this way, we achieve a scalable, time-efficient, and diverse distributed MARL learning framework with high system throughput. To own knowledge, our method is the first to solve the challenging Google Research Football full game $\\mathtt{5\\_v\\_5}$. On the StarCraft II micromanagement benchmark, our method gets 4-18$\\times$ better results compared to state-of-the-art non-distributed MARL algorithms.", "ratings": "[5.0, 3.0, 5.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "Zca3NK3X8G", "title": "WaveCorr: Deep Reinforcement Learning with Permutation Invariant Policy Networks for Portfolio Management", "keywords": "['permutation invariance', 'portfolio management', 'deep reinforcement learning', 'policy network']", "abstract": "The problem of portfolio management represents an important and challenging class of dynamic decision making problems, where rebalancing decisions need to be made over time with the consideration of many factors such as investors\u2019 preferences, trading environment, and market conditions. In this paper, we present a new portfolio policy network architecture for deep reinforcement learning (DRL) that can exploit more effectively cross-asset dependency information and achieve better performance than  state-of-the-art architectures. In doing so, we introduce a new form of permutation invariance property for policy networks and derive general theory for verifying its applicability. Our portfolio policy network, named WaveCorr, is the first convolutional neural network architecture that preserves this invariance property when treating asset correlation information. Finally, in a set of experiments conducted using data from both Canadian (TSX) and American stock markets (S\\&P 500), WaveCorr consistently outperforms other architectures with an impressive 3\\%-25\\% absolute improvement in terms of average annual return, and up to more than 200\\% relative improvement in average Sharpe ratio. We also measured an improvement of a factor of up to 5 in the stability of performance under random choices of initial asset ordering and weights. The stability of the network has been found as particularly valuable by our industrial partner.", "ratings": "[8.0, 5.0, 5.0, 8.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "4Sp2v2DQcxX", "title": "Skill Machines: Temporal Logic Composition in Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Lifelong learning', 'Multi task learning', 'Transfer learning', 'Logical composition', 'Deep Reinforcement Learning']", "abstract": "A major challenge in reinforcement learning is specifying tasks in a manner that is both interpretable and verifiable. One common approach is to specify tasks through reward machines---finite state machines that encode the task to be solved. We introduce skill machines, a representation that can be learned directly from these reward machines that encode the solution to such tasks. We propose a framework where an agent first learns a set of base skills in a reward-free setting, and then combines these skills with the learned skill machine to produce composite behaviours specified by any regular language, such as linear temporal logics. This provides the agent with the ability to map from complex logical task specifications to near-optimal behaviours zero-shot. We demonstrate our approach in both a tabular and high-dimensional video game environment, where an agent is faced with several of these complex, long-horizon tasks. Our results indicate that the agent is capable of satisfying extremely complex task specifications, producing near optimal performance with no further learning. Finally, we demonstrate that the performance of skill machines can be improved with regular off-policy reinforcement learning algorithms when optimal behaviours are desired.", "ratings": "[6, 6, 5, 6]", "confidences": "[4, 3, 3, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "8CjVaaSSVxg", "title": "Learning Predictive Communication by Imagination in Networked System Control", "keywords": "Reinforcement Learning, Multi-agent Reinforcement Learning, Networked System Control", "abstract": "Dealing with multi-agent control in networked systems is one of the biggest challenges in Reinforcement Learning (RL) and limited success has been presented compared to recent deep reinforcement learning in single-agent domain. However, obstacles remain in addressing the delayed global information where each agent learns a decentralized control policy based on local observations and messages from connected neighbors. This paper first considers delayed global information sharing by combining the delayed global information and latent imagination of farsighted states in differentiable communication. Our model allows an agent to imagine its future states and communicate that with its neighbors. The predictive message sent to the connected neighbors reduces the delay in global information.  On the tasks of networked multi-agent traffic control, experimental results show that our model helps stabilize the training of each local agent and outperforms existing algorithms for networked system control.", "ratings": "[5.0, 4.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "E3Ys6a1NTGT", "title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "keywords": "deep learning, reinforcement learning", "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "ratings": "[7.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "jXe91kq3jAq", "title": "Skill Transfer via Partially Amortized Hierarchical Planning", "keywords": "Model-based Reinforcement Learning, Partial Amortization, Planning", "abstract": "To quickly solve new tasks in complex environments, intelligent agents need to build up reusable knowledge. For example, a learned world model captures knowledge about the environment that applies to new tasks. Similarly, skills capture general behaviors that can apply to new tasks. In this paper, we investigate how these two approaches can be integrated into a single reinforcement learning agent. Specifically, we leverage the idea of partial amortization for fast adaptation at test time. For this, actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning. We demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another, as compared to competitive baselines. Videos are available at: \\url{https://sites.google.com/view/partial-amortization/home}", "ratings": "[6.0, 7.0, 5.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "Xv_s64FiXTv", "title": "Learning to Represent Action Values as a Hypergraph on the Action Vertices", "keywords": "reinforcement learning, action representation learning, multidimensional action space, relational inductive bias", "abstract": "Action values are ubiquitous in reinforcement learning (RL) methods, with the sample complexity of such methods relying heavily on how fast a good estimator for action value can be learned. Viewing this problem through the lens of representation learning, naturally good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to RL, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multidimensional action spaces is a key ingredient for learning good representations of action. In order to test this, we set forth the action hypergraph networks framework---a class of functions for learning action representations with a relational inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks, which we dub hypergraph Q-networks. We show the effectiveness of our approach on a myriad of domains: illustrative prediction problems under minimal confounding effects, Atari 2600 games, and physical control benchmarks.", "ratings": "[7.0, 5.0, 8.0, 6.0, 8.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "jHefDGsorp5", "title": "Molecule Optimization by Explainable Evolution", "keywords": "Molecule Design, Explainable Model, Evolutionary Algorithm, Reinforcement Learning, Graph Generative Model", "abstract": "Optimizing molecules for desired properties is a fundamental yet challenging task in chemistry, material science and drug discovery. In this paper, we develop a novel algorithm for optimizing molecule properties via an Expectation Maximization~(EM)-like explainable evolutionary process. Our algorithm is designed to mimic human experts in the process of searching for desirable molecules and alternate between two stages: the first stage on explainable local search which identifies rationales, \\ie{}, critical subgraph patterns accounting for desired molecular properties, and the second stage on molecule completion which explores the larger space of molecules containing good rationales. We test our method against various baselines on a real-world multi-property optimization task where each method is given the same number of queries to the property oracle. We show that our evolution-by-explanation algorithm is 79\\% better than the best baseline in terms of a generic metric combining aspects such as success rate, novelty and diversity. Human expert evaluation on optimized molecules shows that 60\\% of top molecules obtained from our methods are deemed as successful ones.", "ratings": "[8.0, 7.0, 6.0, 7.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "hcQHRHKfN_", "title": "Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization", "keywords": "['diverse behavior', 'deep reinforcement learning', 'multi-agent reinforcement learning']", "abstract": "We present Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones. To encourage the learning policy to consistently converge towards a previously undiscovered local optimum, RSPO switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process. When a sampled trajectory is sufficiently distinct, RSPO performs standard policy optimization with extrinsic rewards.  For trajectories with high likelihood under existing policies, RSPO utilizes an intrinsic diversity reward to promote exploration. Experiments show that RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent particle-world tasks and MuJoCocontinuous control to multi-agent stag-hunt games and StarCraftII challenges.", "ratings": "[8.0, 8.0, 5.0, 8.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "aBO5SvgSt1", "title": "Mirror Descent Policy Optimization", "keywords": "['Reinforcement Learning', 'Policy Optimization']", "abstract": "Mirror descent (MD), a well-known first-order method in constrained convex optimization, has recently been shown as an important tool to analyze trust-region algorithms in reinforcement learning (RL). However, there remains a considerable gap between such theoretically analyzed algorithms and the ones used in practice. Inspired by this, we propose an efficient RL algorithm, called {\\em mirror descent policy optimization} (MDPO). MDPO iteratively updates the policy by {\\em approximately} solving a trust-region problem, whose objective function consists of two terms: a linearization of the standard RL objective and a proximity term that restricts two consecutive policies to be close to each other. Each update performs this approximation by taking multiple gradient steps on this objective function. We derive {\\em on-policy} and {\\em off-policy} variants of MDPO, while emphasizing important design choices motivated by the existing theory of MD in RL. We highlight the connections between on-policy MDPO and two popular trust-region RL algorithms: TRPO and PPO, and show that explicitly enforcing the trust-region constraint is in fact {\\em not} a necessity for high performance gains in TRPO. We then show how the popular soft actor-critic (SAC) algorithm can be derived by slight modifications of off-policy MDPO. Overall, MDPO is derived from the MD principles, offers a unified approach to viewing a number of popular RL algorithms, and performs better than or on-par with TRPO, PPO, and SAC in a number of continuous and discrete control tasks.", "ratings": "[6.0, 8.0, 6.0, 5.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "JYtwGwIL7ye", "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models", "keywords": "['reward misspecification', 'reinforcement learning', 'reward hacking', 'alignment', 'ml safety']", "abstract": "Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied. To understand reward hacking, we construct four RL environments with different misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space noise. Typically, more capable agents are able to better exploit reward misspecifications, causing them to attain higher proxy reward and lower true reward. Moreover, we find instances of \\emph{phase transitions}: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To encourage further research on reward misspecification, address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.", "ratings": "[6.0, 6.0, 6.0, 8.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "ZSKRQMvttc", "title": "Accelerated Policy Learning with Parallel Differentiable Simulation", "keywords": "['Robot Control', 'Policy Learning', 'Differentiable Simulation', 'Reinforcement Learning']", "abstract": "Deep reinforcement learning can generate complex control policies, but requires large amounts of training data to work effectively. Recent work has attempted to address this issue by leveraging differentiable simulators. However, inherent problems such as local minima and exploding/vanishing numerical gradients prevent these methods from being generally applied to control tasks with complex contact-rich dynamics, such as humanoid locomotion in classical RL benchmarks. In this work, we present SHAC, a short-horizon actor-critic method that successfully leverages parallel differentiable simulation to accelerate policy learning. Our method alleviates problems with local minima through a smooth critic function, avoids vanishing/exploding gradients through a truncated learning window, and allows many physical environments to be run in parallel. We evaluate our method on classical RL control tasks, and show substantial improvements in sample efficiency and wall-clock time over state-of-the-art RL and differentiable simulation-based algorithms. In addition, we demonstrate the scalability of our method by applying it to the challenging high-dimensional problem of muscle-actuated locomotion with a large action space, achieving a greater than 17x reduction in training time over the best-performing established RL algorithm. More visual results are provided at: https://sites.google.com/view/shac", "ratings": "[8.0, 6.0, 8.0, 8.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "jbIYfq4Tr-", "title": "On the Robustness of Safe Reinforcement Learning under Observational Perturbations", "keywords": "['Safe reinforcement learning', 'deep reinforcement learning', 'state robust reinforcement learning']", "abstract": "Safe reinforcement learning (RL) trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality, we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective observational adversarial attackers in the safe RL setting. We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and propose two new approaches - one maximizes the cost and the other maximizes the reward. One interesting and counter-intuitive finding is that the maximum reward attack is strong, as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a robust training framework for safe RL and evaluate it via comprehensive experiments. This paper provides a pioneer work to investigate the safety and robustness of RL under observational attacks for future safe RL studies. Code is available at: \\url{https://github.com/liuzuxin/safe-rl-robustness}", "ratings": "[6, 6, 6, 6]", "confidences": "[4, 3, 4, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "XOh5x-vxsrV", "title": "Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL", "keywords": "['reinforcement learning', 'representation learning', 'self-supervised learning', 'procgen']", "abstract": "A highly desirable property of a reinforcement learning (RL) agent -- and a major difficulty for deep RL approaches -- is the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not seen during training. Many promising approaches to this challenge consider RL as a process of training two functions simultaneously: a complex nonlinear encoder that maps high-dimensional observations to a latent representation space, and a simple linear policy over this space. We posit that a superior encoder for zero-shot generalization in RL can be trained by using solely an auxiliary SSL objective if the training process encourages the encoder to map behaviorally similar observations to similar representations, as reward-based signal can cause overfitting in the encoder (Raileanu et al., 2021). We propose Cross-Trajectory Representation Learning (CTRL), a method that runs within an RL agent and conditions its encoder to recognize behavioral similarity in observations by applying a novel SSL objective to pairs of trajectories from the agent's policies. CTRL can be viewed as having the same effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use of rewards and associated overfitting risks. Our experiments ablate various components of CTRL and demonstrate that in combination with PPO it achieves better generalization performance on the challenging Procgen benchmark suite (Cobbe et al., 2020).", "ratings": "[6.0, 3.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "6qZC7pfenQm", "title": "Improving Deep Policy Gradients with Value Function Search", "keywords": "['Deep Reinforcement Learning', 'Deep Policy Gradients']", "abstract": "Deep Policy Gradient (PG) algorithms employ value networks to drive the learning of parameterized policies and reduce the variance of the gradient estimates. However, value function approximation gets stuck in local optima and struggles to fit the actual return, limiting the variance reduction efficacy and leading policies to sub-optimal performance. This paper focuses on improving value approximation and analyzing the effects on Deep PG primitives such as value prediction, variance reduction, and correlation of gradient estimates with the true gradient. To this end, we introduce a Value Function Search that employs a population of perturbed value networks to search for a better approximation. Our framework does not require additional environment interactions, gradient computations, or ensembles, providing a computationally inexpensive approach to enhance the supervised learning task on which value networks train. Crucially, we show that improving Deep PG primitives results in improved sample efficiency and policies with higher returns using common continuous control benchmark domains.", "ratings": "[5, 6, 5, 5]", "confidences": "[4, 4, 4, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "W3Wf_wKmqm9", "title": "C-Learning: Horizon-Aware Cumulative Accessibility Estimation", "keywords": "reinforcement learning, goal reaching, Q-learning", "abstract": "Multi-goal reaching is an important problem in reinforcement learning needed to achieve algorithmic generalization. Existing methods suffer from the fundamental limitation of learning a single way to reach each goal from each state. This does not allow for any trade-off between reaching the goal quickly and reaching it reliably. We introduce the concept of cumulative accessibility functions to address this limitation. We show that these functions obey a recurrence relation which enables learning from offline interactions, and that they allow us to balance the speed / reliability trade-off at test time. In addition, these functions are monotonic in the planning horizon which decreases the complexity of the learning task. We propose a specially tailored replay buffer that allows efficient learning, validate our approach in a variety of experiments, and demonstrate that it allows the speed / reliability trade-off while outperforming state-of-the-art goal reaching algorithms in planning tasks. Additional visualizations can be found at https://sites.google.com/view/learning-cae/", "ratings": "[5.0, 6.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "V6BjBgku7Ro", "title": "Planning from Pixels using Inverse Dynamics Models", "keywords": "model based reinforcement learning, deep reinforcement learning, multi-task learning, deep learning, goal-conditioned reinforcement learning", "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "ratings": "[6.0, 6.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "--gvHfE3Xf5", "title": "Meta-Learning of Compositional Task Distributions in Humans and Machines", "keywords": "meta-learning, human cognition, reinforcement learning, compositionality", "abstract": "Modern machine learning systems struggle with sample efficiency and are usually trained with enormous amounts of data for each task. This is in sharp contrast with humans, who often learn with very little data. In recent years, meta-learning, in which one trains on a family of tasks (i.e. a task distribution), has emerged as an approach to improving the sample complexity of machine learning systems and to closing the gap between human and machine learning. However, in this paper, we argue that current meta-learning approaches still differ significantly from human learning. We argue that humans learn over tasks by constructing compositional generative models and using these to generalize, whereas current meta-learning methods are biased toward the use of simpler statistical patterns. To highlight this difference, we construct a new meta-reinforcement learning task with a compositional task distribution. We also introduce a novel approach to constructing a ``null task distribution'' with the same statistical complexity as the compositional distribution but without explicit compositionality. We train a standard meta-learning agent, a recurrent network trained with model-free reinforcement learning, and compare it with human performance across the two task distributions. We find that humans do better in the compositional task distribution whereas the agent does better in the non-compositional null task distribution -- despite comparable statistical complexity. This work highlights a particular difference between human learning and current meta-learning models, introduces a task that displays this difference, and paves the way for future work on human-like meta-learning.", "ratings": "[6.0, 6.0, 7.0, 7.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "NmZXv4467ai", "title": "Decision Transformer under Random Frame Dropping", "keywords": "['Decision Transformer', 'Reinforcement Learning', 'Frame Dropping']", "abstract": "Controlling agents remotely with deep reinforcement learning~(DRL) in the real world is yet to come. One crucial stepping stone is to devise RL algorithms that are robust in the face of dropped information from corrupted communication or malfunctioning sensors. Typical RL methods usually require considerable online interaction data that are costly and unsafe to collect in the real world. Furthermore, when applying to the frame dropping scenarios, they perform unsatisfactorily even with moderate drop rates. To address these issues, we propose Decision Transformer under Random Frame Dropping~(DeFog), an offline RL algorithm that enables agents to act robustly in frame dropping scenarios without online interaction. DeFog first randomly masks out data in the offline datasets and explicitly adds the time span of frame dropping as inputs. After that, a finetuning stage on the same offline dataset with a higher mask rate would further boost the performance. Empirical results show that DeFog outperforms strong baselines under severe frame drop rates like 90\\%, while maintaining similar returns under non-frame-dropping conditions in the regular MuJoCo control benchmarks and the Atari environments. Our approach offers a robust and deployable solution for controlling agents in real-world environments with limited or unreliable data.", "ratings": "[6, 6, 6]", "confidences": "[4, 3, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "w2Z2OwVNeK", "title": "Plan-Based Asymptotically Equivalent Reward Shaping", "keywords": "reinforcement learning, reward shaping, plan-based reward shaping, robotics, robotic manipulation", "abstract": "In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Asymptotically Equivalent Reward Shaping (ASEQ-RS). ASEQ-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of asymptotic equivalence. Being less restrictive, ASEQ-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based ASEQ-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.", "ratings": "[6.0, 7.0, 7.0, 3.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "uR9LaO_QxF", "title": "Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation", "keywords": "Deep Reinforcement Learning, Memory, Transformers, Distillation", "abstract": "Many real-world applications such as robotics provide hard constraints on power and compute that limit the viable model complexity of Reinforcement Learning (RL) agents. Similarly, in many distributed RL settings, acting is done on un-accelerated hardware such as CPUs, which likewise restricts model size to prevent intractable experiment run times. These \"actor-latency\" constrained settings present a major obstruction to the scaling up of model complexity that has recently been extremely successful in supervised learning. To be able to utilize large model capacity while still operating within the limits imposed by the system during acting, we develop an \"Actor-Learner Distillation\" (ALD) procedure that leverages a continual form of distillation that transfers learning progress from a large capacity learner model to a small capacity actor model. As a case study, we develop this procedure in the context of partially-observable environments, where transformer models have had large improvements over LSTMs recently, at the cost of significantly higher computational complexity. With transformer models as the learner and LSTMs as the actor, we demonstrate in several challenging memory environments that using Actor-Learner Distillation recovers the clear sample-efficiency gains of the transformer learner model while maintaining the fast inference and reduced total training time of the LSTM actor model.", "ratings": "[8.0, 7.0, 7.0, 5.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "Qun8fv4qSby", "title": "Transient Non-stationarity and Generalisation in Deep Reinforcement Learning", "keywords": "Reinforcement Learning, Generalization", "abstract": "Non-stationarity can arise in Reinforcement Learning (RL) even in stationary environments. For example, most RL algorithms collect new data throughout training, using a non-stationary behaviour policy. Due to the transience of this non-stationarity, it is often not explicitly addressed in deep RL and a single neural network is continually updated. However, we find evidence that neural networks exhibit a memory effect, where these transient non-stationarities can permanently impact the latent representation and adversely affect generalisation performance. Consequently, to improve generalisation of deep RL agents, we propose Iterated Relearning (ITER). ITER augments standard RL training by repeated knowledge transfer of the current policy into a freshly initialised network, which thereby experiences less non-stationarity during training. Experimentally, we show that ITER improves performance on the challenging generalisation benchmarks ProcGen and Multiroom.", "ratings": "[5.0, 5.0, 7.0, 8.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "Sq0-tgDyHe4", "title": "Local Feature Swapping for Generalization in Reinforcement Learning", "keywords": "['Reinforcement learning', 'Generalization', 'Regularization']", "abstract": "Over the past few years, the acceleration of computing resources and research in Deep Learning has led to significant practical successes in a range of tasks, including in particular in computer vision. Building on these advances, reinforcement learning has also seen a leap forward with the emergence of agents capable of making decisions directly from visual observations. Despite these successes, the over-parametrization of neural architectures leads to memorization of the data used during training and thus to a lack of generalization.         Reinforcement learning agents based on visual inputs also suffer from this phenomenon by erroneously correlating rewards with unrelated visual features such as background elements. To alleviate this problem, we introduce a new regularization layer consisting of channel-consistent local permutations (CLOP) of the feature maps. The proposed permutations induce robustness to spatial correlations and help prevent overfitting behaviors in RL. We demonstrate, on the OpenAI Procgen Benchmark, that RL agents trained with the CLOP layer exhibit robustness to visual changes and better generalization properties than agents trained using other state-of-the-art regularization techniques.", "ratings": "[8.0, 8.0, 8.0, 6.0, 8.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "b39dQt_uffW", "title": "Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation", "keywords": "['safety', 'reinforcement learning', 'safe reinforcement learning', 'constrained Markov decision process', 'partially observable Markov decision process', 'MDP', 'POMDP']", "abstract": "We address the problem of safe reinforcement learning from pixel observations. Inherent challenges in such settings are (1) a trade-off between reward optimization and adhering to safety constraints, (2) partial observability, and (3) high-dimensional observations. We formalize the problem in a constrained, partially observable Markov decision process framework, where an agent obtains distinct reward and safety signals. To address the curse of dimensionality, we employ a novel safety critic using the stochastic latent actor-critic (SLAC) approach. The latent variable model predicts rewards and safety violations, and we use the safety critic to train safe policies. Using well-known benchmark environments, we demonstrate competitive performance over existing approaches regarding computational requirements, final reward return, and satisfying the safety constraints. ", "ratings": "[5, 6, 8, 6]", "confidences": "[3, 3, 3, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "EBn0uInJZWh", "title": "Model-Based Offline Meta-Reinforcement Learning with Regularization", "keywords": "['offline reinforcement learning', 'model-based reinforcement learning', 'behavior policy', 'Meta-reinforcement learning']", "abstract": "Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL  could be outperformed  by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated  between \"exploring\" the out-of-distribution state-actions by following the meta-policy and \"exploiting\" the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we propose a model-based offline $\\text{\\bf Me}$ta-RL approach with $\\text{\\bf r}$egularized $\\text{\\bf P}$olicy $\\text{\\bf O}$ptimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block  of MerPO, using conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Our experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.", "ratings": "[6.0, 6.0, 6.0, 8.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "c87d0TS4yX", "title": "Orchestrated Value Mapping for Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Value Mapping', 'Reward Decomposition']", "abstract": "We present a general convergent class of reinforcement learning algorithms that is founded on two distinct principles: (1) mapping the value function into a different space via arbitrary functions from a broad class and (2) linearly decomposing the reward signal into multiple channels. The first principle enables asserting specific properties on the value function that can enhance learning. The second principle, on the other hand, allows for the value function to be represented as a composition of multiple utility functions. This can be leveraged for various purposes, including dealing with highly varying reward scales, incorporating a priori knowledge about the sources of reward, and ensemble learning. Combining the two principles yields a general blueprint for instantiating convergent algorithms by orchestrating diverse mapping functions over multiple reward channels. This blueprint generalizes and subsumes algorithms such as classical Q-learning, Logarithmic Q-learning, and Q-Decomposition. Moreover, our convergence proof for this general class relaxes certain required assumptions in some existing algorithms. Using our theory we discuss several interesting configurations as special cases. Finally, to illustrate the potential of the design space that our theory opens up, we instantiate a particular algorithm and evaluate its performance on the suite of Atari 2600 games.", "ratings": "[6.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "mQpmZVzXK1h", "title": "Latent Variable Representation for Reinforcement Learning", "keywords": "['Latent Variable Model', 'Markov Decision Processes', 'Reinforcement Learning']", "abstract": "Deep latent variable models have achieved significant empirical successes in model-based reinforcement learning (RL) due to their expressiveness in modeling complex transition dynamics. On the other hand, it remains unclear theoretically and empirically how latent variable models may facilitate learning, planning, and exploration to improve the sample efficiency of RL. In this paper, we provide a representation view of the latent variable models for state-action value functions, which allows both tractable variational learning algorithm and effective implementation of the optimism/pessimism principle in the face of uncertainty for exploration. In particular, we propose a computationally efficient planning algorithm with UCB exploration by incorporating kernel embeddings of latent variable models. Theoretically, we establish the sample complexity of the proposed approach in the online and offline settings. Empirically, we demonstrate superior performance over current state-of-the-art algorithms across various benchmarks.", "ratings": "[6, 8, 6, 3]", "confidences": "[4, 3, 3, 2]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "QFYnKlBJYR", "title": "Reinforcement Learning with Random Delays", "keywords": "Reinforcement Learning, Deep Reinforcement Learning", "abstract": "Action and observation delays commonly occur in many Reinforcement Learning applications, such as remote control scenarios. We study the anatomy of randomly delayed environments, and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.", "ratings": "[8.0, 6.0, 6.0, 3.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "ZOcX-eybqoL", "title": "Generalisation in Lifelong Reinforcement Learning through Logical Composition", "keywords": "['Reinforcement Learning', 'Lifelong learning', 'Multi task learning', 'Transfer learning', 'Logical composition', 'Deep Reinforcement Learning']", "abstract": "We leverage logical composition in reinforcement learning to create a framework that enables an agent to autonomously determine whether a new task can be immediately solved using its existing abilities, or whether a task-specific skill should be learned. In the latter case, the proposed algorithm also enables the agent to learn the new task faster by generating an estimate of the optimal policy. Importantly, we provide two main theoretical results: we give bounds on the performance of the transferred policy on a new task, and we give bounds on the necessary and sufficient number of tasks that need to be learned throughout an agent's lifetime to generalise over a distribution. We verify our approach in a series of experiments, where we perform transfer learning both after learning a set of base tasks, and after learning an arbitrary set of tasks. We also demonstrate that as a side effect of our transfer learning approach, an agent can produce an interpretable Boolean expression of its understanding of the current task. Finally, we demonstrate our approach in the full lifelong setting where an agent receives tasks from an unknown distribution and, starting from zero skills, is able to quickly generalise over the task distribution after learning only a few tasks---which are sub-logarithmic in the size of the task space.", "ratings": "[6.0, 6.0, 8.0, 5.0, 5.0, 5.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "20gBzEzgtiI", "title": "Performance Bounds for Model and Policy Transfer in Hidden-parameter MDPs", "keywords": "['Reinforcement learning', 'Meta learning', 'Transfer learning', 'Theory']", "abstract": "In the Hidden-Parameter MDP (HiP-MDP) framework, a family of reinforcement learning tasks is generated by varying hidden parameters specifying the dynamics and reward function for each individual task. HiP-MDP is a natural model for families of tasks in which meta- and lifelong-reinforcement learning approaches can succeed. Given a learned context encoder that infers the hidden parameters from previous experience, most existing algorithms fall into two categories: $\\textit{model transfer}$ and $\\textit{policy transfer}$, depending on which function the hidden parameters are used to parameterize. We characterize the robustness of model and policy transfer algorithms with respect to hidden parameter estimation error. We first show that the value function of HiP-MDPs is Lipschitz continuous under certain conditions. We then derive regret bounds for both settings through the lens of Lipschitz continuity. Finally, we empirically corroborate our theoretical analysis by experimentally varying the hyper-parameters governing the Lipschitz constants of two continuous control problems; the resulting performance is consistent with our predictions.", "ratings": "[6, 8, 5]", "confidences": "[2, 3, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "dCOL0inGl3e", "title": "Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication", "keywords": "['certifiable robustness', 'reinforcement learning', 'multi-agent system', 'adversarial communication', 'adversarial attack']", "abstract": "Communication is important in many multi-agent reinforcement learning (MARL) problems for agents to share information and make good decisions. However, when deploying trained communicative agents in a real-world application where noise and potential attackers exist, the safety of communication-based policies becomes a severe issue that is underexplored. Specifically, if communication messages are manipulated by malicious attackers, agents relying on untrustworthy communication may take unsafe actions that lead to catastrophic consequences. Therefore, it is crucial to ensure that agents will not be misled by corrupted communication, while still benefiting from benign communication. In this work, we consider an environment with $N$ agents, where the attacker may arbitrarily change the communication from any $C<\\frac{N-1}{2}$ agents to a victim agent. For this strong threat model, we propose a certifiable defense by constructing a message-ensemble policy that aggregates multiple randomly ablated message sets. Theoretical analysis shows that this message-ensemble policy can utilize benign communication while being certifiably robust to adversarial communication, regardless of the attacking algorithm. Experiments in multiple environments verify that our defense significantly improves the robustness of trained policies against various types of attacks.", "ratings": "[6, 8, 8]", "confidences": "[4, 2, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "4FBUihxz5nm", "title": "A Mixture-of-Expert Approach to RL-based Dialogue Management", "keywords": "['Reinforcement learning']", "abstract": "Despite recent advancements in language models (LMs), their application to dialogue management (DM) problems and ability to carry on rich conversations remain a challenge. We use reinforcement learning (RL) to develop a dialogue agent that avoids being short-sighted (outputting generic utterances) and maximizes overall user satisfaction. Most existing RL approaches to DM train the agent at the word-level, and thus, have to deal with a combinatorially complex action space even for a medium-size vocabulary. As a result, they struggle to produce a successful and engaging dialogue even if they are warm-started with a pre-trained LM. To address this issue, we develop a RL-based DM using a novel mixture of expert language model (MoE-LM) that consists of (i) a LM capable of learning diverse semantics for conversation histories, (ii) a number of specialized LMs (or experts) capable of generating utterances corresponding to a particular attribute or personality, and (iii) a RL-based DM that performs dialogue planning with the utterances generated by the experts. Our MoE approach provides greater flexibility to generate sensible utterances with different intents and allows RL to focus on conversational-level DM. We compare it with SOTA baselines on open-domain dialogues and demonstrate its effectiveness both in terms of the diversity and sensibility of the generated utterances and the overall DM performance. ", "ratings": "[8, 6, 3, 6, 8]", "confidences": "[3, 3, 3, 4, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "Rcmk0xxIQV", "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning", "keywords": "Multi-agent reinforcement learning, Value factorization, Dueling structure", "abstract": "We explore value-based multi-agent reinforcement learning (MARL) in the popular paradigm of centralized training with decentralized execution (CTDE). CTDE has an important concept, Individual-Global-Max (IGM) principle, which requires the consistency between joint and local action selections to support efficient local decision-making. However, in order to achieve scalability, existing MARL methods either limit representation expressiveness of their value function classes or relax the IGM consistency, which may suffer from instability risk or lead to poor performance. This paper presents a novel MARL approach, called duPLEX dueling multi-agent Q-learning (QPLEX), which takes a duplex dueling network architecture to factorize the joint value function. This duplex dueling structure encodes the IGM principle into the neural network architecture and thus enables efficient value function learning. Theoretical analysis shows that QPLEX achieves a complete IGM function class. Empirical experiments on StarCraft II micromanagement tasks demonstrate that QPLEX significantly outperforms state-of-the-art baselines in both online and offline data collection settings, and also reveal that QPLEX achieves high sample efficiency and can benefit from offline datasets without additional online exploration.", "ratings": "[7.0, 6.0, 6.0, 4.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "sciA_xgYofB", "title": "Impossibly Good Experts and How to Follow Them", "keywords": "['Imitation Learning', 'Reinforcement Learning', 'Experts', 'Distillation']", "abstract": "We consider the sequential decision making problem of learning from an expert that has access to more information than the learner. For many problems this extra information will enable the expert to achieve greater long term reward than any policy without this privileged information access. We call these experts ``Impossibly Good'' because no learning algorithm will be able to reproduce their behavior. However, in these settings it is reasonable to attempt to recover the best policy possible given the agent's restricted access to information. We provide a set of necessary criteria on the expert that will allow a learner to recover the optimal policy in the reduced information space from the expert's advice alone. We also provide a new approach called Elf Distillation (Explorer Learning from Follower) that can be used in cases where these criteria are not met and environmental rewards must be taken into account. We show that this algorithm performs better than a variety of strong baselines on a challenging suite of Minigrid and Vizdoom environments.", "ratings": "[6, 6, 6]", "confidences": "[3, 3, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "a4COps0uokg", "title": "User-Interactive Offline Reinforcement Learning", "keywords": "['Offline RL', 'Reinforcement Learning', 'User', 'Model-based', 'Adaptive']", "abstract": "Offline reinforcement learning algorithms still lack trust in practice due to the risk that the learned policy performs worse than the original policy that generated the dataset or behaves in an unexpected way that is unfamiliar to the user. At the same time, offline RL algorithms are not able to tune their most important hyperparameter - the proximity of the learned policy to the original policy. We propose an algorithm that allows the user to tune this hyperparameter at runtime, thereby addressing both of the above mentioned issues simultaneously. This allows users to start with the original behavior and grant successively greater deviation, as well as stopping at any time when the policy deteriorates or the behavior is too far from the familiar one.", "ratings": "[10, 6, 3, 8]", "confidences": "[4, 3, 4, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "xQAjSr64PTc", "title": "EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model", "keywords": "['Reinforcement Learning', 'Unsupervised RL', 'Model-based RL']", "abstract": "Unsupervised reinforcement learning (URL) poses a promising paradigm to learn useful behaviors in a task-agnostic environment without the guidance of extrinsic rewards to facilitate the fast adaptation of various downstream tasks. Previous works focused on the pre-training in a model-free manner while lacking the study of transition dynamics modeling that leaves a large space for the improvement of sample efficiency in downstream tasks. To this end, we propose an Efficient Unsupervised Reinforcement Learning Framework with Multi-choice Dynamics model (EUCLID), which introduces a novel model-fused paradigm to jointly pre-train the dynamics model and unsupervised exploration policy in the pre-training phase, thus better leveraging the environmental samples and improving the downstream task sampling efficiency. However, constructing a generalizable model which captures the local dynamics under different behaviors remains a challenging problem. We introduce the multi-choice dynamics model that covers different local dynamics under different behaviors concurrently, which uses different heads to learn the state transition under different behaviors during unsupervised pre-training and selects the most appropriate head for prediction in the downstream task. Experimental results in the manipulation and locomotion domains demonstrate that EUCLID achieves state-of-the-art performance with high sample efficiency, basically solving the state-based URLB benchmark and reaching a mean normalized score of 104.0\u00b11.2% in downstream tasks with 100k fine-tuning steps, which is equivalent to DDPG\u2019s performance at 2M interactive steps with 20\u00d7 more data. More visualization videos are released on our homepage.", "ratings": "[6, 6, 6, 6]", "confidences": "[4, 4, 4, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "psh0oeMSBiF", "title": "COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks", "keywords": "['certified robustness', 'poisoning attacks', 'reinforcement learning']", "abstract": "As reinforcement learning (RL) has achieved near human-level performance in a variety of tasks, its robustness has raised great attention when applied to safety-critical domains such as autonomous driving. Recent studies have explored the test-time attacks in RL and corresponding defenses, while the robustness of RL against training-time attacks remains largely unanswered. In this work, we focus on certifying the robustness of offline RL in the presence of poisoning attacks, where a subset of training trajectories could be arbitrarily manipulated. We propose the first certification framework COPA to certify the number of poisoning trajectories that can be tolerated regarding different certification criteria. Given the complex structure of RL, we propose two certification criteria: per-state action stability and cumulative reward bound. To tighten the certification, we also propose different partition and aggregation protocols to train robust policies.  We further prove that some of the proposed certification methods are theoretically tight and some are NP-Complete problems. We conduct thorough evaluation of COPA on different games trained with different offline RL algorithms: (1) The proposed temporal aggregation in COPA significantly improves the certified robustness; (2) Our certifications for both per-state action stability and  cumulative reward bound are efficient and tight; (3) The certification for different training algorithms and games are different, implying their intrinsic robustness properties.", "ratings": "[6.0, 6.0, 5.0, 5.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "rJvY_5OzoI", "title": "Multi-Critic Actor Learning: Teaching RL Policies to Act with Style", "keywords": "['Reinforcement Learning', 'Multi-Style Learning', 'Multi-Task Learning', 'Actor-Critic']", "abstract": "Using a single value function (critic) shared over multiple tasks in Actor-Critic multi-task reinforcement learning (MTRL) can result in negative interference between tasks, which can compromise learning performance. Multi-Critic Actor Learning (MultiCriticAL) proposes instead maintaining separate value-function estimators, i.e. critics, for each task being trained. This relaxes an assumption of continuity between task values and avoids interference between task-value estimates. Explicitly distinguishing between tasks also eliminates the need for critics to learn to do so. MultiCriticAL is tested in the context of multi-style learning, a special case of MTRL where agents are trained to behave with different distinct behavior styles, and yields up to 45% performance gains over the single-critic baselines and even successfully learns behavior styles in cases where single-critic approaches may simply fail to learn. As a further test of MultiCriticAL\u2019s utility, it is tested on a simulation of EA\u2019s UFC game, where our method enables a single policy function to learn and smoothly transition between multiple fighting styles.", "ratings": "[6.0, 8.0, 6.0, 6.0, 8.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "9SS69KwomAM", "title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "keywords": "compositional task, sparse reward, reinforcement learning, task reduction, imitation learning", "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures.", "ratings": "[7.0, 6.0, 5.0, 3.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "OHgnfSrn2jv", "title": "Efficient Wasserstein Natural Gradients for Reinforcement Learning", "keywords": "reinforcement learning, optimization", "abstract": "A novel optimization approach is proposed for application to policy gradient methods and evolution strategies for reinforcement learning (RL). The procedure uses a computationally efficient \\emph{Wasserstein natural gradient} (WNG) descent that takes advantage of the geometry induced by a Wasserstein penalty to speed optimization. This method follows the recent theme in RL of including divergence penalties in the objective to establish trust regions. Experiments on challenging tasks demonstrate improvements in both computational cost and performance over advanced baselines.", "ratings": "[5.0, 8.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "9xhgmsNVHu", "title": "Is High Variance Unavoidable in RL? A Case Study in Continuous Control", "keywords": "['reinforcement learning', 'continuous control']", "abstract": "Reinforcement learning (RL) experiments have notoriously high variance, and minor details can have disproportionately large effects on measured outcomes. This is problematic for creating reproducible research and also serves as an obstacle for real-world applications, where safety and predictability are paramount. In this paper, we investigate causes for this perceived instability. To allow for an in-depth analysis, we focus on a specifically popular setup with high variance -- continuous control from pixels with an actor-critic agent. In this setting, we demonstrate that variance mostly arises early in training as a result of poor \"outlier\" runs, but that weight initialization and initial exploration are not to blame. We show that one cause for early variance is numerical instability which leads to saturating nonlinearities. We investigate several fixes to this issue and find that one particular method is surprisingly effective and simple -- normalizing penultimate features. Addressing the learning instability allows for larger learning rates, and significantly decreases the variance of outcomes. This demonstrates that the perceived variance in RL is not necessarily inherent to the problem definition and may be addressed through simple architectural modifications.", "ratings": "[6.0, 10.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "7IWGzQ6gZ1D", "title": "Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates", "keywords": "['reinforcement learning', 'lifelong learning', 'transfer learning', 'successor features']", "abstract": "We study the problem of learning a good set of policies, so that when combined together, they can solve a wide variety of unseen reinforcement learning tasks with no or very little new data. Specifically, we consider the framework of generalized policy evaluation and improvement, in which the rewards for all tasks of interest are assumed to be expressible as a linear combination of a fixed set of features. We show theoretically that, under certain assumptions, having access to a specific set of diverse policies, which we call a set of independent policies, can allow for instantaneously achieving high-level performance on all possible downstream tasks, although these tasks are typically more complex than the ones on which the agent was trained. Based on this theoretical analysis, we propose a simple algorithm that iteratively constructs this set of policies. In addition to empirically validating our theoretical results, we compare our approach with recently proposed diverse policy set construction methods and show that, while others fail, our approach is able to build a behavior basis that enables instantaneous transfer to all possible downstream tasks. We also show empirically that having access to a set of independent policies can better bootstrap the learning process on downstream tasks where the new reward function cannot be described as a linear combination of the features. Finally, we demonstrate that this policy set can be useful in a realistic lifelong reinforcement learning setting.", "ratings": "[10.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "Q-neeWNVv1", "title": "Order Matters: Agent-by-agent Policy Optimization", "keywords": "['Multi-agent Reinforcement Learning']", "abstract": "While multi-agent trust region algorithms have achieved great success empirically in solving coordination tasks, most of them, however, suffer from a non-stationarity problem since agents update their policies simultaneously. In contrast, a sequential scheme that updates policies agent-by-agent provides another perspective and shows strong performance. However, sample inefficiency and lack of monotonic improvement guarantees for each agent are still the two significant challenges for the sequential scheme. In this paper, we propose the \\textbf{A}gent-by-\\textbf{a}gent \\textbf{P}olicy \\textbf{O}ptimization (A2PO) algorithm to improve the sample efficiency and retain the guarantees of monotonic improvement for each agent during training. We justify the tightness of the monotonic improvement bound compared with other trust region algorithms. From the perspective of sequentially updating agents, we further consider the effect of agent updating order and extend the theory of non-stationarity into the sequential update scheme. To evaluate A2PO, we conduct a comprehensive empirical study on four benchmarks: StarCraftII, Multi-agent MuJoCo, Multi-agent Particle Environment, and Google Research Football full game scenarios. A2PO consistently outperforms strong baselines.", "ratings": "[8, 6, 5, 6, 8]", "confidences": "[3, 4, 5, 4, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "gJYlaqL8i8", "title": "Learning to Sample with Local and Global Contexts in Experience Replay Buffer", "keywords": "reinforcement learning, experience replay buffer, off-policy RL", "abstract": "Experience replay, which enables the agents to remember and reuse experience from the past, has played a significant role in the success of off-policy reinforcement learning (RL). To utilize the experience replay efficiently, the existing sampling methods allow selecting out more meaningful experiences by imposing priorities on them based on certain metrics (e.g. TD-error). However, they may result in sampling highly biased, redundant transitions since they compute the sampling rate for each transition independently, without consideration of its importance in relation to other transitions. In this paper, we aim to address the issue by proposing a new learning-based sampling method that can compute the relative importance of transition. To this end, we design a novel permutation-equivariant neural architecture that takes contexts from not only features of each transition (local) but also those of others (global) as inputs. We validate our framework, which we refer to as Neural Experience Replay Sampler (NERS), on multiple benchmark tasks for both continuous and discrete control tasks and show that it can significantly improve the performance of various off-policy RL methods. Further analysis confirms that the improvements of the sample efficiency indeed are due to sampling diverse and meaningful transitions by NERS that considers both local and global contexts.", "ratings": "[7.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "QubpWYfdNry", "title": "Domain-Robust Visual Imitation Learning with Mutual Information Constraints", "keywords": "Imitation Learning, Reinforcement Learning, Observational Imitation, Third-Person Imitation, Mutual Information, Domain Adaption, Machine Learning", "abstract": "Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.", "ratings": "[7.0, 6.0, 7.0, 7.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "jHc8dCx6DDr", "title": "Memory Gym: Partially Observable Challenges to Memory-Based Agents", "keywords": "['Deep Reinforcement Learning', 'Memory', 'Benchmark', 'Proximal Policy Optimization', 'Gated Recurrent Unit', 'HELM']", "abstract": "Memory Gym is a novel benchmark for challenging Deep Reinforcement Learning agents to memorize events across long sequences, be robust to noise, and generalize. It consists of the partially observable 2D and discrete control environments Mortar Mayhem, Mystery Path, and Searing Spotlights. These environments are believed to be unsolvable by memory-less agents because they feature strong dependencies on memory and frequent agent-memory interactions. Empirical results based on Proximal Policy Optimization (PPO) and Gated Recurrent Unit (GRU) underline the strong memory dependency of the contributed environments. The hardness of these environments can be smoothly scaled, while different levels of difficulty (some of them unsolved yet) emerge for Mortar Mayhem and Mystery Path. Surprisingly, Searing Spotlights poses a tremendous challenge to GRU-PPO, which remains an open puzzle. Even though the randomly moving spotlights reveal parts of the environment\u2019s ground truth, environmental ablations hint that these pose a severe perturbation to agents that leverage recurrent model architectures as their memory. Source Code: https://github.com/MarcoMeter/drl-memory-gym/", "ratings": "[3, 6, 8, 5]", "confidences": "[4, 4, 4, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "NDWl9qcUpvy", "title": "Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward", "keywords": "['deep reinforcement learning', 'structured exploration']", "abstract": "We propose Structured Exploration with Achievements (SEA), a multi-stage reinforcement learning algorithm designed for achievement-based environments, a particular type of environment with an internal achievement set. SEA first uses offline data to learn a representation of the known achievements with a determinant loss function, then recovers the dependency graph of the learned achievements with a heuristic algorithm, and finally interacts with the environment online to learn policies that master known achievements and explore new ones with a controller built with the recovered dependency graph. We empirically demonstrate that SEA can recover the achievement structure accurately and improve exploration in hard domains such as Crafter that are procedurally generated with high-dimensional observations like images.", "ratings": "[5, 5, 8, 8]", "confidences": "[4, 4, 4, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "r8Mu7idxyF", "title": "Making Better Decision by Directly Planning in Continuous Control", "keywords": "['Model-based Reinforcement Learning', 'Reinforcement Learning', 'Planning', 'Policy Optimization']", "abstract": "By properly utilizing the learned environment model, model-based reinforcement learning methods can improve the sample efficiency for decision-making problems. Beyond using the learned environment model to train a policy, the success of MCTS-based methods shows that directly incorporating the learned environment model as a planner to make decisions might be more effective. However, when action space is of high dimension and continuous, directly planning according to the learned model is costly and non-trivial. Because of two challenges: (1) the infinite number of candidate actions and (2) the temporal dependency between actions in different timesteps. To address these challenges, inspired by Differential Dynamic Programming (DDP) in optimal control theory, we design a novel Policy Optimization with Model Planning (POMP) algorithm, which incorporates a carefully designed Deep Differential Dynamic Programming (D3P) planner into the model-based RL framework. In D3P planner, (1) to effectively plan in the continuous action space, we construct a locally quadratic programming problem that uses a gradient-based optimization process to replace search. (2) To take the temporal dependency of actions at different timesteps into account, we leverage the updated and latest actions of previous timesteps (i.e., step $1, \\cdots, h-1$) to update the action of the current step (i.e., step $h$), instead of updating all actions simultaneously. We theoretically prove the convergence rate for our D3P planner and analyze the effect of the feedback term. In practice, to effectively apply the neural network based D3P planner in reinforcement learning, we leverage the policy network to initialize the action sequence and keep the action update conservative in the planning process. Experiments demonstrate that POMP consistently improves sample efficiency on widely used continuous control tasks. Our code is released at https://github.com/POMP-D3P/POMP-D3P. ", "ratings": "[8, 6, 8, 8]", "confidences": "[3, 3, 4, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "chPj_I5KMHG", "title": "DECSTR: Learning Goal-Directed Abstract Behaviors using Pre-Verbal Spatial Predicates in Intrinsically Motivated Agents", "keywords": "Deep Reinforcement Learning", "abstract": "Past research showed that preverbal infants represent abstract spatial relations between physical objects and demonstrate goal-oriented sensorimotor behavior before they even start acquiring language. Inspired by these findings, we introduce DECSTR, an intrinsically motivated agent endowed with a high-level representation of spatial relations between objects that freely explores its environment and sets its own goals as target constraints on these spatial relations. In a robotic manipulation environment, DECSTR explores the corresponding representation space by manipulating objects, and efficiently learns to achieve any reachable configuration within it. It does so by leveraging an object-centered modular architecture, a symmetry inductive bias, and a new form of automatic curriculum learning for goal selection and policy learning. We explicitly compare the use of this abstract state and goal representations to the traditional goal-as-state framework and show that DECSTR can fulfill goals in an opportunistic manner, making profit of the current configuration of objects to achieve its goals. Additionally, by contrast with traditional instruction-following agents which are trained simultaneously to understand language and to act, DECSTR decouples goal-oriented sensorimotor learning from language acquisition, as in infants. This decoupling is facilitated by the abstract spatial relations, as they better match the natural semantics of linguistic inputs than traditional continuous representations. We show this behavior in a proof-of-concept setup, where trained agents learn to associate their behavioral repertoire with simple language commands via a language-conditioned goal generator. We show that this goal generator increases the behavioral diversity of a given language instruction and enables retry behaviors when the agent fails.", "ratings": "[4.0, 6.0, 6.0, 7.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "OzyXtIZAzFv", "title": "Task-Induced Representation Learning", "keywords": "['representation learning', 'reinforcement learning', 'transfer learning', 'visually complex observations']", "abstract": "A major bottleneck for applying deep reinforcement learning to real-world problems is its sample inefficiency, particularly when training policies from high-dimensional inputs such as images. A number of recent works use unsupervised representation learning approaches to improve sample efficiency. Yet, such unsupervised approaches are fundamentally unable to distinguish between task-relevant and irrelevant information. Thus, in visually complex scenes they learn representations that model lots of task-irrelevant details and hence lead to slower downstream task learning. Our insight: to determine which parts of the scene are important and should be modeled, we can exploit task information, such as rewards or demonstrations, from previous tasks. To this end, we formalize the problem of task-induced representation learning (TARP), which aims to leverage such task information in offline experience from prior tasks for learning compact representations that focus on modelling only task-relevant aspects. Through a series of experiments in visually complex environments we compare different approaches for leveraging task information within the TARP framework with prior unsupervised representation learning techniques and (1) find that task-induced representations allow for more sample efficient learning of unseen tasks and (2) formulate a set of best-practices for task-induced representation learning.", "ratings": "[6.0, 6.0, 6.0, 5.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "JQc2VowqCzz", "title": "Interaction-Based Disentanglement of Entities for Object-Centric World Models", "keywords": "['object-centric', 'object-oriented', 'world models', 'self-supervised learning', 'probabilistic deep learning', 'structured models', 'video prediction', 'physics prediction', 'planning', 'variational autoencoders', 'model-based reinforcement learning', 'VAEs', 'unsupervised']", "abstract": "Perceiving the world compositionally in terms of space and time is essential to understanding object dynamics and solving downstream tasks. Object-centric learning using generative models has improved in its ability to learn distinct representations of individual objects and predict their interactions, and how to utilize the learned representations to solve untrained, downstream tasks is a focal question. However, as models struggle to predict object interactions and track the objects accurately, especially for unseen configurations, using object-centric representations in downstream tasks is still a challenge. This paper proposes STEDIE, a new model that disentangles object representations, based on interactions, into interaction-relevant relational features and interaction-irrelevant global features without supervision. Empirical evaluation shows that the proposed model factorizes global features, unaffected by interactions from relational features that are necessary to predict outcome of interactions. We also show that STEDIE achieves better performance in planning tasks and understanding causal relationships. In both tasks, our model not only achieves better performance in terms of reconstruction ability but also utilizes the disentangled representations to solve the tasks in a structured manner.", "ratings": "[6, 5, 6, 6]", "confidences": "[4, 3, 2, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "RXQ-FPbQYVn", "title": "Anti-Concentrated Confidence Bonuses For Scalable Exploration", "keywords": "['deep reinforcement learning', 'reinforcement learning', 'bandits', 'exploration']", "abstract": "Intrinsic rewards play a central role in handling the exploration-exploitation tradeoff when designing sequential decision-making algorithms, in both foundational theory and state-of-the-art deep reinforcement learning. The LinUCB algorithm, a centerpiece of the stochastic linear bandits literature, prescribes an elliptical bonus which addresses the challenge of leveraging shared information in large action spaces. This bonus scheme cannot be directly transferred to high-dimensional exploration problems, however, due to the computational cost of maintaining the inverse covariance matrix of action features. We introduce anti-concentrated confidence bounds for efficiently approximating the elliptical bonus, using an ensemble of regressors trained to predict random noise from policy network-derived features. Using this approximation, we obtain stochastic linear bandit algorithms which obtain $\\tilde O(d \\sqrt{T})$ regret bounds for $\\mathsf{poly}(d)$ fixed actions. We develop a practical variant that is competitive with contemporary intrinsic reward heuristics on Atari benchmarks.", "ratings": "[8.0, 6.0, 5.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "QjQibO3scV_", "title": "Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching", "keywords": "['Graph Matching', 'Reinforcement Learning', 'Quadratic Assignment', 'Affinity Regularization', 'Combinatorial Optimization.']", "abstract": "Graph matching (GM) has been a building block in various areas including computer vision and pattern recognition. Despite recent impressive progress, existing deep GM methods often have obvious difficulty in handling outliers, which are ubiquitous in practice. We propose a deep reinforcement learning based approach RGM, whose sequential node matching scheme naturally fits the strategy for selective inlier matching against outliers. A revocable action framework is devised to improve the agent's flexibility against the complex constrained GM. Moreover, we propose a quadratic approximation technique to regularize the affinity score, in the presence of outliers. As such, the agent can finish inlier matching timely when the affinity score stops growing, for which otherwise an additional parameter i.e. the number of inliers is needed to avoid matching outliers. In this paper, we focus on learning the back-end solver under the most general form of GM: the Lawler's QAP, whose input is the affinity matrix. Especially, our approach can also boost existing GM methods that use such input. Experiments on multiple real-world datasets demonstrate its performance regarding both accuracy and robustness.", "ratings": "[5, 6, 8]", "confidences": "[5, 4, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "NEtep2C7yD", "title": "Learning Simultaneous Navigation and Construction in Grid Worlds ", "keywords": "['Navigation', 'Localization', 'Construction', 'Deep reinforcement learning', 'Representation learning']", "abstract": "We propose to study a new learning task, mobile construction, to enable an agent to build designed structures in 1/2/3D grid worlds while navigating in the same evolving environments. Unlike existing robot learning tasks such as visual navigation and object manipulation, this task is challenging because of the interdependence between accurate localization and strategic construction planning. In pursuit of generic and adaptive solutions to this partially observable Markov decision process (POMDP) based on deep reinforcement learning (RL), we design a Deep Recurrent Q-Network (DRQN) with explicit recurrent position estimation in this dynamic grid world. Our extensive experiments show that pre-training this position estimation module before Q-learning can significantly improve the construction performance measured by the intersection-over-union score, achieving the best results in our benchmark of various baselines including model-free and model-based RL, a handcrafted SLAM-based policy, and human players. Our code is available at: https://ai4ce.github.io/SNAC/.", "ratings": "[8, 8, 6, 6]", "confidences": "[4, 3, 4, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "chDrutUTs0K", "title": "POPGym: Benchmarking Partially Observable Reinforcement Learning", "keywords": "['partially observable', 'POMDP', 'reinforcement learning', 'memory']", "abstract": "Real world applications of Reinforcement Learning (RL) are often partially observable, thus requiring memory. Despite this, partial observability is still largely ignored by contemporary RL benchmarks and libraries. We introduce Partially Observable Process Gym (POPGym), a two-part library containing (1) a diverse collection of 15 partially observable environments, each with multiple difficulties and (2) implementations of 13 memory model baselines -- the most in a single RL library. Existing partially observable benchmarks tend to fixate on 3D visual navigation, which is computationally expensive and only one type of POMDP. In contrast, POPGym environments are diverse, produce smaller observations, use less memory, and often converge within two hours of training on a consumer-grade GPU. We implement our high-level memory API and memory baselines on top of the popular RLlib framework, providing plug-and-play compatibility with various training algorithms, exploration strategies, and distributed training paradigms. Using POPGym, we execute the largest comparison across RL memory models to date. POPGym is available at https://github.com/proroklab/popgym.", "ratings": "[3, 8, 8]", "confidences": "[4, 4, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "0WVNuEnqVu", "title": "Deep Reinforcement Learning for Cost-Effective Medical Diagnosis", "keywords": "['medical diagnostics', 'Pareto front', 'reinforcement learning', 'non-Markovian reward', 'semi-model-based policy optimization']", "abstract": "Dynamic diagnosis is desirable when medical tests are costly or time-consuming. In this work, we use reinforcement learning (RL) to find a dynamic policy that selects lab test panels sequentially based on previous observations, ensuring accurate testing at a low cost. Clinical diagnostic data are often highly imbalanced; therefore, we aim to maximize the F1 score instead of the error rate. However, optimizing the non-concave $F_1$ score is not a classic RL problem, thus invalidating standard RL methods. To remedy this issue, we develop a reward shaping approach, leveraging properties of the $F_1$ score and duality of policy optimization, to provably find the set of all Pareto-optimal policies for budget-constrained $F_1$ score maximization. To handle the combinatorially complex state space, we propose a Semi-Model-based Deep Diagnosis Policy Optimization (SM-DDPO) framework that is compatible with end-to-end training and online learning. SM-DDPO is tested on diverse clinical tasks: ferritin abnormality detection, sepsis mortality prediction, and acute kidney injury diagnosis. Experiments with real-world data validate that SM-DDPO trains efficiently and identify all Pareto-front solutions. Across all tasks, SM-DDPO is able to achieve state-of-the-art diagnosis accuracy (in some cases higher than conventional methods) with up to $85\\%$ reduction in testing cost. Core codes are available at https://github.com/Zheng321/Deep-Reinforcement-Learning-for-Cost-Effective-Medical-Diagnosis.", "ratings": "[6, 8, 6]", "confidences": "[3, 3, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "086pmarAris", "title": "Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems", "keywords": "['task-oriented dialogue', 'reinforcement learning', 'reward learning']", "abstract": "When learning task-oriented dialogue (ToD) agents, reinforcement learning (RL) techniques can naturally be utilized to train dialogue strategies to achieve user-specific goals. Prior works mainly focus on adopting advanced RL techniques to train the ToD agents, while the design of the reward function is not well studied. This paper aims at answering the question of how to efficiently learn and leverage a reward function for training end-to-end (E2E) ToD agents. Specifically, we introduce two generalized objectives for reward-function learning, inspired by the classical learning-to-rank literature. Further, we utilize the learned reward function to guide the training of the E2E ToD agent. With the proposed techniques, we achieve competitive results on the E2E response-generation task on the Multiwoz 2.0 dataset. Source code and checkpoints are publicly released at https://github.com/Shentao-YANG/Fantastic_Reward_ICLR2023.", "ratings": "[6, 6, 8, 6]", "confidences": "[3, 4, 3, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "vEZyTBRPP6o", "title": "Actor-critic is implicitly biased towards high entropy optimal policies", "keywords": "['implicit bias', 'reinforcement learning', 'actor-critic', 'policy gradient', 'mixing time', 'convergence rate', 'mirror ascent.']", "abstract": "We show that the simplest actor-critic method \u2014 a linear softmax policy updated with TD through interaction with a linear MDP, but featuring no explicit regularization or exploration \u2014 does not merely find an optimal policy, but moreover prefers high entropy optimal policies.  To demonstrate the strength of this bias, the algorithm not only has no regularization, no projections, and no exploration like $\\epsilon$-greedy, but is moreover trained on a single trajectory with no resets.  The key consequence of the high entropy bias is that uniform mixing assumptions on the MDP, which exist in some form in all prior work, can be dropped: the implicit regularization of the high entropy bias is enough to ensure that all chains mix and an optimal policy is reached with high probability.  As an auxiliary contribution, this work decouples concerns between the actor and critic by writing the actor update as an explicit mirror descent, and provides tools to analyze the critic's estimation in similar high entropy settings.", "ratings": "[6.0, 8.0, 8.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "NUl0ylt7SM", "title": "Simple Emergent Action Representations from Multi-Task Policy Training", "keywords": "['action representation', 'reinforcement learning', 'representation learning']", "abstract": "The low-level sensory and motor signals in deep reinforcement learning, which exist in high-dimensional spaces such as image observations or motor torques, are inherently challenging to understand or utilize directly for downstream tasks. While sensory representations have been extensively studied, the representations of motor actions are still an area of active exploration. Our work reveals that a space containing meaningful action representations emerges when a multi-task policy network takes as inputs both states and task embeddings. Moderate constraints are added to improve its representation ability. Therefore, interpolated or composed embeddings can function as a high-level interface within this space, providing instructions to the agent for executing meaningful action sequences. Empirical results demonstrate that the proposed action representations are effective for intra-action interpolation and inter-action composition with limited or no additional learning. Furthermore, our approach exhibits superior task adaptation ability compared to strong baselines in Mujoco locomotion tasks. Our work sheds light on the promising direction of learning action representations for efficient, adaptable, and composable RL, forming the basis of abstract action planning and the understanding of motor signal space. Project page: https://sites.google.com/view/emergent-action-representation/", "ratings": "[6, 5, 5, 6]", "confidences": "[3, 3, 4, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "TGFO0DbD_pk", "title": "Genetic Soft Updates for Policy Evolution in Deep Reinforcement Learning", "keywords": "Deep Reinforcement Learning, Machine Learning for Robotics", "abstract": "The combination of Evolutionary Strategies (ES) and Deep Reinforcement Learning (DRL) has been recently proposed to merge the benefits of both solutions. Existing mixed approaches, however, have been successfully applied only to actor-critic methods and present significant overhead. We address these issues by introducing a novel mixed framework that exploits a periodical genetic evaluation to soft update the weights of a DRL agent. The resulting approach is applicable with any DRL method and, in a worst-case scenario, it does not exhibit detrimental behaviours. Experiments in robotic applications and continuous control benchmarks demonstrate the versatility of our approach that significantly outperforms prior DRL, ES, and mixed approaches. Finally, we employ formal verification to confirm the policy improvement, mitigating the inefficient exploration and hyper-parameter sensitivity of DRL.", "ratings": "[7.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "gfwON7rAm4", "title": "Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games", "keywords": "['Multi-agent Reinforcement Learning', 'Markov Potential Games', 'Policy Gradient']", "abstract": "Potential games are arguably one of the most important and widely studied classes of normal form games. They define the archetypal setting of multi-agent coordination in which all agents utilities are perfectly aligned via a common potential function. Can this intuitive framework be transplanted in the setting of Markov games? What are the similarities and differences between multi-agent coordination with and without state dependence? To answer these questions, we study a natural class of Markov Potential Games (MPGs) that generalize prior attempts at capturing complex stateful multi-agent coordination. Counter-intuitively, insights from normal-form potential games do not carry over as MPGs involve settings where state-games can be zero-sum games. In the opposite direction, Markov games where every state-game is a potential game are not necessarily MPGs. Nevertheless, MPGs showcase standard desirable properties such as the existence of deterministic Nash policies. In our main technical result, we prove convergence of independent policy gradient and its stochastic counterpart to Nash policies (polynomially fast in the approximation error) by adapting recent gradient dominance property arguments developed for single-agent Markov decision processes to multi-agent learning settings.", "ratings": "[6.0, 8.0, 5.0, 8.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "Nc3TJqbcl3", "title": "Extracting Strong Policies for Robotics Tasks from zero-order trajectory optimizers", "keywords": "reinforcement learning, zero-order optimization, policy learning, model-based learning, robotics", "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "ratings": "[6.0, 6.0, 5.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "hSjxQ3B7GWq", "title": "Sample-Efficient Automated Deep Reinforcement Learning", "keywords": "AutoRL, Deep Reinforcement Learning, Hyperparameter Optimization, Neuroevolution", "abstract": "Despite significant progress in challenging problems across various domains, applying state-of-the-art deep reinforcement learning (RL) algorithms remains challenging due to their sensitivity to the choice of hyperparameters. This sensitivity can partly be attributed to the non-stationarity of the RL problem, potentially requiring different hyperparameter settings at various stages of the learning process. Additionally, in the RL setting, hyperparameter optimization (HPO) requires a large number of environment interactions, hindering the transfer of the successes in RL to real-world applications. In this work, we tackle the issues of sample-efficient and dynamic HPO in RL. We propose a population-based automated RL (AutoRL) framework to meta-optimize arbitrary off-policy RL algorithms. In this framework, we optimize the hyperparameters and also the neural architecture while simultaneously training the agent. By sharing the collected experience across the population, we substantially increase the sample efficiency of the meta-optimization. We demonstrate the capabilities of our sample-efficient AutoRL approach in a case study with the popular TD3 algorithm in the MuJoCo benchmark suite, where we reduce the number of environment interactions needed for meta-optimization by up to an order of magnitude compared to population-based training.", "ratings": "[6.0, 5.0, 7.0, 5.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "7_G8JySGecm", "title": "Monte-Carlo Planning and Learning with Language Action Value Estimates", "keywords": "natural language processing, Monte-Carlo tree search, reinforcement learning, interactive fiction", "abstract": "Interactive Fiction (IF) games provide a useful testbed for language-based reinforcement learning agents, posing significant challenges of natural language understanding, commonsense reasoning, and non-myopic planning in the combinatorial search space. Agents based on standard planning algorithms struggle to play IF games due to the massive search space of language actions. Thus, language-grounded planning is a key ability of such agents, since inferring the consequence of language action based on semantic understanding can drastically improve search. In this paper, we introduce Monte-Carlo planning with Language Action Value Estimates (MC-LAVE) that combines a Monte-Carlo tree search with language-driven exploration. MC-LAVE invests more search effort into semantically promising language actions using locally optimistic language value estimates, yielding a significant reduction in the effective search space of language actions. We then present a reinforcement learning approach via MC-LAVE, which alternates between MC-LAVE planning and supervised learning of the self-generated language actions. In the experiments, we demonstrate that our method significantly outperforms the state-of-the-art on the extensive set of IF games.", "ratings": "[7.0, 4.0, 6.0, 7.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "_BoPed4tYww", "title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints", "keywords": "['dynamic programming', 'impulse control', 'optimal stopping', 'reinforcement learning']", "abstract": "Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage. Determining when to act is crucial for achieving successful outcomes and yet, the challenge of efficiently learning to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we intro- duce a reinforcement learning (RL) framework named Learnable Impulse Control Reinforcement Algorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as impulse control which learns to maximise objectives when actions incur costs. We prove that LICRA, which seamlessly adopts any RL method, converges to policies that optimally select when to perform actions and their optimal magnitudes. We then augment LICRA to handle problems in which the agent can perform at most k < \u221e actions and more generally, faces a budget constraint. We show LICRA learns the optimal value function and ensures budget constraints are satisfied almost surely. We demonstrate empirically LICRA\u2019s superior performance against benchmark RL methods in OpenAI gym\u2019s Lunar Lander and in Highway environments and a variant of the Merton portfolio problem within finance.", "ratings": "[5, 8, 6, 6]", "confidences": "[4, 4, 3, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "35QyoZv8cKO", "title": "ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret", "keywords": "['game theory', 'two-player zero-sum', 'CFR', 'Reinforcement learning']", "abstract": "Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promis- ing line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art\u2014DREAM and neural fictitious self play (NFSP)\u2014on a number of games and the difference becomes dramatic as game size increases. In the very large game of dark chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over 90% of the time.", "ratings": "[6, 6, 6]", "confidences": "[3, 3, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "tc5qisoB-C", "title": "C-Learning: Learning to Achieve Goals via Recursive Classification", "keywords": "reinforcement learning, goal reaching, density estimation, Q-learning, hindsight relabeling", "abstract": "We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.", "ratings": "[4.0, 7.0, 7.0, 8.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "SK7A5pdrgov", "title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning", "keywords": "reinforcement learning, transfer learning, sim2real transfer, domain adaptation, causality, generalization, robotics", "abstract": "Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this, we propose CausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer.  Tasks consist of constructing 3D shapes from a given set of blocks - inspired by how children learn to build complex structures.  The key  strength of CausalWorld is that it provides a combinatorial family of such tasks with  common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows for fine-grained control over how similar different  tasks (or task distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level, targeting a specific form of generalization (e.g., only changes in appearance or object mass).  Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning and precise low-level motor control at the same time. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark.", "ratings": "[7.0, 8.0, 4.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "vIC-xLFuM6", "title": "Overcoming The Spectral Bias of Neural Value Approximation", "keywords": "['spectral bias', 'neural value approximation', 'Q learning', 'reinforcement learning', 'neural tangent kernels', 'kernel regression']", "abstract": "Value approximation using deep neural networks is a critical component of state-of-the-art reinforcement learning algorithms. While deep networks are universal function approximators, recent works have suggested the presence of a spectral bias during learning, where high-frequency components of the target function require more exponentially more gradient updates to learn than the low-frequency ones. Such dynamics contribute to slow convergence when fitting value functions that are intrinsically complex. To mitigate this issue, we propose Random Fourier Network (RFN) with a band-agnostic dimension-less initialization scheme, as a drop-in replacement of the standard multi-layer perceptrons. We demonstrate the advantage of our approach on the challenging high-dimensional continuous control domains including Ant and Humanoid, where the RFN leads to faster convergence and higher performance.", "ratings": "[3.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "H7HDG--DJF0", "title": "Multi-Agent MDP Homomorphic Networks", "keywords": "['multiagent systems', 'reinforcement learning', 'equivariance', 'symmetry']", "abstract": "This paper introduces Multi-Agent MDP Homomorphic Networks, a class of networks that allows distributed execution using only local information, yet is able to share experience between global symmetries in the joint state-action space of cooperative multi-agent systems. In cooperative multi-agent systems, complex symmetries arise between different configurations of the agents and their local observations. For example, consider a group of agents navigating: rotating the state globally results in a permutation of the optimal joint policy. Existing work on symmetries in single agent reinforcement learning can only be generalized to the fully centralized setting, because such approaches rely on the global symmetry in the full state-action spaces, and these can result in correspondences across agents. To encode such symmetries while still allowing distributed execution we propose a factorization that decomposes global symmetries into local transformations. Our proposed factorization allows for distributing the computation that enforces global symmetries  over  local agents and local  interactions. We introduce a multi-agent equivariant policy network based on this factorization. We show empirically on symmetric multi-agent problems that distributed execution of globally symmetric policies improves data efficiency compared to non-equivariant baselines.", "ratings": "[6.0, 6.0, 8.0, 5.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "DfUjyyRW90", "title": "Information Prioritization through Empowerment in Visual Model-based RL", "keywords": "['model-based reinforcement learning', 'visual distractors', 'empowerment']", "abstract": "Model-based reinforcement learning (RL) algorithms designed for handling complex visual observations typically learn some sort of latent state representation, either explicitly or implicitly. Standard methods of this sort do not distinguish between functionally relevant aspects of the state and irrelevant distractors, instead aiming to represent all available information equally. We propose a modified objective for model-based RL that, in combination with mutual information maximization, allows us to learn representations and dynamics for visual model-based RL without reconstruction in a way that explicitly prioritizes functionally relevant factors. The key principle behind our design is to integrate a term inspired by variational empowerment into a state-space learning model based on mutual information. This term prioritizes information that is correlated with action, thus ensuring that functionally relevant factors are captured first. Furthermore, the same empowerment term also promotes faster exploration during the RL process, especially for sparse-reward tasks where the reward signal is insufficient to drive exploration in the early stages of learning. We evaluate the approach on a suite of vision-based robot control tasks with natural video backgrounds, and show that the proposed prioritized information objective outperforms state-of-the-art model based RL approaches by an average of 20\\% in terms of episodic returns at 1M environment interactions with 30\\% higher sample efficiency at 100k interactions.", "ratings": "[8.0, 8.0, 8.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "SNwH0dDGl7_", "title": "Near-Optimal Deployment Efficiency in Reward-Free Reinforcement Learning with Linear Function Approximation", "keywords": "['Reinforcement Learning', 'Deployment efficiency', 'Reward free RL', 'Low adaptive RL']", "abstract": "We study the problem of deployment efficient reinforcement learning (RL) with linear function approximation under the \\emph{reward-free} exploration setting. This is a well-motivated problem because deploying new policies is costly in real-life RL applications. Under the linear MDP setting with feature dimension $d$ and planning horizon $H$, we propose a new algorithm that collects at most $\\widetilde{O}(\\frac{d^2H^5}{\\epsilon^2})$ trajectories within $H$ deployments to identify $\\epsilon$-optimal policy for any (possibly data-dependent) choice of reward functions. To the best of our knowledge, our approach is the first to achieve optimal deployment complexity and optimal $d$ dependence in sample complexity at the same time, even if the reward is known ahead of time. Our novel techniques include an exploration-preserving policy discretization and a generalized G-optimal experiment design, which could be of independent interest. Lastly, we analyze the related problem of regret minimization in low-adaptive RL and provide information-theoretic lower bounds for switching cost and batch complexity.", "ratings": "[6, 5, 6, 6]", "confidences": "[2, 4, 3, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "HIGSa_3kOx3", "title": "Reset-Free Lifelong Learning with Skill-Space Planning", "keywords": "reset-free, lifelong, reinforcement learning", "abstract": "The objective of \\textit{lifelong} reinforcement learning (RL) is to optimize agents which can continuously adapt and interact in changing environments. However, current RL approaches fail drastically when environments are non-stationary and interactions are non-episodic. We propose \\textit{Lifelong Skill Planning} (LiSP), an algorithmic framework for lifelong RL based on planning in an abstract space of higher-order skills. We learn the skills in an unsupervised manner using intrinsic rewards and plan over the learned skills using a learned dynamics model. Moreover, our framework permits skill discovery even from offline data, thereby reducing the need for excessive real-world interactions. We demonstrate empirically that LiSP successfully enables long-horizon planning and learns agents that can avoid catastrophic failures even in challenging non-stationary and non-episodic environments derived from gridworld and MuJoCo benchmarks.", "ratings": "[5.0, 7.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "6vkzF28Hur8", "title": "Training Transition Policies via Distribution Matching for Complex Tasks", "keywords": "['Reinforcement Learning', 'Hierarchical Reinforcement Learning', 'Inverse Reinforcement Learning']", "abstract": "Humans decompose novel complex tasks into simpler ones to exploit previously learned skills. Analogously, hierarchical reinforcement learning seeks to leverage lower-level policies for simple tasks to solve complex ones. However, because each lower-level policy induces a different distribution of states, transitioning from one lower-level policy to another may fail due to an unexpected starting state. We introduce transition policies that smoothly connect lower-level policies by producing a distribution of states and actions that matches what is expected by the next policy. Training transition policies is challenging because the natural reward signal---whether the next policy can execute its subtask successfully---is sparse. By training transition policies via adversarial inverse reinforcement learning to match the distribution of expected states and actions, we avoid relying on task-based reward. To further improve performance, we use deep Q-learning with a binary action space to determine when to switch from a transition policy to the next pre-trained policy, using the success or failure of the next subtask as the reward. Although the reward is still sparse, the problem is less severe due to the simple binary action space. We demonstrate our method on continuous bipedal locomotion and arm manipulation tasks that require diverse skills. We show that it smoothly connects the lower-level policies, achieving higher success rates than previous methods that search for successful trajectories based on a reward function, but do not match the state distribution.", "ratings": "[6.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "j1RMMKeP2gR", "title": "Acting in Delayed Environments with Non-Stationary Markov Policies", "keywords": "reinforcement learning, delay", "abstract": "The standard Markov Decision Process (MDP) formulation hinges on the assumption that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of $m$ steps. The brute-force state augmentation baseline where the state is concatenated to the last $m$ committed actions suffers from an exponential complexity in $m$, as we show for policy iteration. We then prove that with execution delay, Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks without resorting to state-augmentation. Experiments on tabular, physical, and Atari domains reveal that it converges quickly to high performance even for substantial delays, while standard approaches that either ignore the delay or rely on state-augmentation struggle or fail due to divergence. The code will be shared upon publication.", "ratings": "[5.0, 6.0, 6.0, 8.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "mwdfai8NBrJ", "title": "Policy Smoothing for Provably Robust Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Provable Adversarial Robustness', 'Randomized Smoothing']", "abstract": "The study of provable adversarial robustness for deep neural networks (DNNs) has mainly focused on $\\textit{static}$ supervised learning tasks such as image classification. However, DNNs have been used extensively in real-world $\\textit{adaptive}$ tasks such as reinforcement learning (RL), making such systems vulnerable to adversarial attacks as well. Prior works in provable robustness in RL seek to certify the behaviour of the victim policy at every time-step against a non-adaptive adversary using methods developed for the static setting. But in the real world, an RL adversary can infer the defense strategy used by the victim agent by observing the states, actions, etc. from previous time-steps and adapt itself to produce stronger attacks in future steps (e.g., by focusing more on states critical to the agent's performance). We present an efficient procedure, designed specifically to defend against an adaptive RL adversary, that can directly certify the total reward without requiring the policy to be robust at each time-step. Focusing on randomized smoothing based defenses, our main theoretical contribution is to prove an $\\textit{adaptive version}$ of the Neyman-Pearson Lemma -- a key lemma for smoothing-based certificates -- where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previous actions. Building on this result, we propose $\\textit{policy smoothing}$ where the agent adds a Gaussian noise to its observation at each time-step before passing it through the policy function. Our robustness certificates guarantee that the final total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time-steps may change under the attack. We show that our certificates are $\\textit{tight}$ by constructing a worst-case scenario that achieves the bounds derived in our analysis. Our experiments on various environments like Cartpole, Pong, Freeway and Mountain Car show that our method can yield meaningful robustness guarantees in practice.", "ratings": "[5.0, 6.0, 6.0, 8.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "9kBCMNb5mc", "title": "Optimistic Exploration with Learned Features Provably Solves Markov Decision Processes with Neural Dynamics", "keywords": "['Reinforcement Learning', 'Neural Network', 'Representation Learning.']", "abstract": "Incorporated with the recent advances in deep learning, deep reinforcement learning (DRL) has achieved tremendous success in empirical study. However, analyzing DRL is still challenging due to the complexity of the neural network class. In this paper, we address such a challenge by analyzing the Markov decision process (MDP) with neural dynamics, which covers several existing models as special cases, including the kernelized nonlinear regulator (KNR) model and the linear MDP. We propose a novel algorithm that designs exploration incentives via learnable representations of the dynamics model by embedding the neural dynamics into a kernel space induced by the system noise. We further establish an upper bound on the sample complexity of the algorithm, which demonstrates the sample efficiency of the algorithm. We highlight that, unlike previous analyses of RL algorithms with function approximation, our bound on the sample complexity does not depend on the Eluder dimension of the neural network class, which is known to be exponentially large (Dong et al., 2021).", "ratings": "[8, 6, 3]", "confidences": "[4, 2, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "-nm-rHXi5ga", "title": "On the Data-Efficiency with Contrastive Image Transformation in Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Data Augmentation', 'Self-Supervised Learning', 'Representation Learning']", "abstract": "Data-efficiency has always been an essential issue in pixel-based reinforcement learning (RL). As the agent not only learns decision-making but also meaningful representations from images. The line of reinforcement learning with data augmentation shows significant improvements in sample-efficiency. However, it is challenging to guarantee the optimality invariant transformation, that is, the augmented data are readily recognized as a completely different state by the agent. In the end, we propose a contrastive invariant transformation (CoIT), a simple yet promising learnable data augmentation combined with standard model-free algorithms to improve sample-efficiency. Concretely, the differentiable CoIT leverages original samples with augmented samples and hastens the state encoder for a contrastive invariant embedding. We evaluate our approach on DeepMind Control Suite and Atari100K. Empirical results verify advances using CoIT, enabling it to outperform the new state-of-the-art on various tasks. Source code is available at https://github.com/mooricAnna/CoIT.", "ratings": "[8, 6, 6, 6]", "confidences": "[4, 3, 3, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "sy4Kg_ZQmS7", "title": "Learning Deep Features in Instrumental Variable Regression", "keywords": "Causal Inference, Instrumental Variable Regression, Deep Learning, Reinforcement Learning", "abstract": "Instrumental variable (IV) regression is a standard strategy for learning causal relationships between the confounded treatment and outcome variables by utilizing an instrumental variable, which is conditionally independent of the outcome given the treatment. In classical IV regression, learning proceeds in two stages: stage 1 performs linear regression from the instrument to the treatment; and stage 2 performs linear regression from the treatment to outcome, conditioned on the instrument. We propose a novel method, deep feature instrumental variable regression (DFIV), to address the case where relations between instruments, treatments, and outcomes may be nonlinear. In this case, deep neural nets are trained to define informative nonlinear features on the instrument and treatments, for both the stage 1 and stage 2 regression problems. We propose an alternating training regime for these features to ensure good end-to-end performance when composing stages 1 and 2, thus obtaining highly flexible feature maps in a computationally efficient manner. DFIV outperforms recent state-of-the-art methods across several challenging IV benchmarks, including settings involving high dimensional image data. DFIV also exhibits competitive performance in policy evaluation for reinforcement learning, which can be understood as an IV regression task.", "ratings": "[5.0, 6.0, 8.0, 7.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "Ysuv-WOFeKR", "title": "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning", "keywords": "reinforcement learning, imitation learning", "abstract": "Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin.", "ratings": "[9.0, 6.0, 7.0, 8.0]", "decision": "Accept (Oral)", "year": "2021"}
{"paper_id": "6qeBuZSo7Pr", "title": "Planning Goals for Exploration", "keywords": "['model-based reinforcement learning', 'exploration', 'goal-conditioned reinforcement learning', 'planning', 'intrinsic motivation', 'reinforcement learning']", "abstract": "Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose \"Planning Exploratory Goals\" (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent's goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to \"plan goal commands\". In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables more efficient and effective training of goal-conditioned policies relative to baselines and ablations. Our ant successfully navigates a long maze, and the robot arm successfully builds a stack of three blocks upon command. Website: https://sites.google.com/view/exploratory-goals", "ratings": "[8, 8, 8, 8, 6]", "confidences": "[3, 4, 4, 5, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "UkU05GOH7_6", "title": "Generating Diverse Cooperative Agents by Learning Incompatible Policies", "keywords": "['multi-agent systems', 'cooperation', 'collaboration', 'reinforcement learning', 'diversity', 'robustness']", "abstract": "Training a robust cooperative agent requires diverse partner agents. However, obtaining those agents is difficult. Previous works aim to learn diverse behaviors by changing the state-action distribution of agents. But, without information about the task's goal, the diversified agents are not guided to find other important, albeit sub-optimal, solutions: the agents might learn only variations of the same solution. In this work, we propose to learn diverse behaviors via policy compatibility. Conceptually, policy compatibility measures whether policies of interest can coordinate effectively. We theoretically show that incompatible policies are not similar. Thus, policy compatibility\u2014which has been used exclusively as a measure of robustness\u2014can be used as a proxy for learning diverse behaviors. Then, we incorporate the proposed objective into a population-based training scheme to allow concurrent training of multiple agents. Additionally, we use state-action information to induce local variations of each policy. Empirically, the proposed method consistently discovers more solutions than baseline methods across various multi-goal cooperative environments. Finally, in multi-recipe Overcooked, we show that our method produces populations of behaviorally diverse agents, which enables generalist agents trained with such a population to be more robust. See our project page at https://bit.ly/marl-lipo ", "ratings": "[8, 8, 8, 8]", "confidences": "[4, 4, 4, 3]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "C2fsSj3ZGiU", "title": "Neural Episodic Control with State Abstraction", "keywords": "['Deep reinforcement learning', 'episodic control', 'sample efficiency', 'state abstraction']", "abstract": "Existing Deep Reinforcement Learning (DRL) algorithms suffer from sample inefficiency. Generally, episodic control-based approaches are solutions that leverage highly rewarded past experiences to improve sample efficiency of DRL algorithms. However, previous episodic control-based approaches fail to utilize the latent information from the historical behaviors (\\eg, state transitions, topological similarities, \\etc) and lack scalability during DRL training. This work introduces Neural Episodic Control with State Abstraction (NECSA), a simple but effective state abstraction-based episodic control containing a more comprehensive episodic memory, a novel state evaluation, and a multi-step state analysis. We evaluate our approach to the MuJoCo and Atari tasks in OpenAI gym domains. The experimental results indicate that NECSA achieves higher sample efficiency than the state-of-the-art episodic control-based approaches. Our data and code are available at the project website\\footnote{\\url{https://sites.google.com/view/drl-necsa}}. ", "ratings": "[8, 6, 8]", "confidences": "[4, 4, 3]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "Ovnwe_sDQW", "title": "BC-IRL: Learning Generalizable Reward Functions from Demonstrations", "keywords": "['inverse reinforcement learning', 'reward learning', 'reinforcement learning', 'imitation learning']", "abstract": "How well do reward functions learned with inverse reinforcement learning (IRL) generalize? We illustrate that state-of-the-art IRL algorithms, which maximize a maximum-entropy objective, learn rewards that overfit to the demonstrations. Such rewards struggle to provide meaningful rewards for states not covered by the demonstrations, a major detriment when using the reward to learn policies in new situations. We introduce BC-IRL a new inverse reinforcement learning method that learns reward functions that generalize better when compared to maximum-entropy IRL approaches. In contrast to the MaxEnt framework, which learns to maximize rewards around demonstrations, BC-IRL updates reward parameters such that the policy trained with the new reward matches the expert demonstrations better. We show that BC-IRL learns rewards that generalize better on an illustrative simple task and two continuous robotic control tasks, achieving over twice the success rate of baselines in challenging generalization settings.", "ratings": "[8, 8, 3]", "confidences": "[4, 2, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "TfBHFLgv77", "title": "Hyperbolic Deep Reinforcement Learning", "keywords": "['Reinforcement learning', 'Hyperbolic space', 'Representation learning', 'Machine learning']", "abstract": "In deep reinforcement learning (RL), useful information about the state is inherently tied to its possible future successors. Consequently, encoding features that capture the hierarchical relationships between states into the model's latent representations is often conducive to recovering effective policies. In this work, we study a new class of deep RL algorithms that promote encoding such relationships by using hyperbolic space to model latent representations. However, we find that a naive application of existing methodology from the hyperbolic deep learning literature leads to fatal instabilities due to the non-stationarity and variance characterizing common gradient estimators in RL. Hence, we design a new general method that directly addresses such optimization challenges and enables stable end-to-end learning with deep hyperbolic representations. We empirically validate our framework by applying it to popular on-policy and off-policy RL algorithms on the Procgen and Atari 100K benchmarks, attaining near universal performance and generalization benefits. Given its natural fit, we hope this work will inspire future RL research to consider hyperbolic representations as a standard tool.", "ratings": "[10, 10, 6]", "confidences": "[4, 4, 2]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "sKc6fgce1zs", "title": "Learning About Progress From Experts", "keywords": "['learning from demonstrations', 'reinforcement learning', 'exploration', 'nethack']", "abstract": "Many important tasks involve some notion of long-term progress in multiple phases: e.g. to clean a shelf it must be cleared of items, cleaning products applied, and then the items placed back on the shelf. In this work, we explore the use of expert demonstrations in long-horizon tasks to learn a monotonically increasing function that summarizes progress. This function can then be used to aid agent exploration in environments with sparse rewards. As a case study we consider the NetHack environment, which requires long-term progress at a variety of scales and is far from being solved by existing approaches. In this environment, we demonstrate that by learning a model of long-term progress from expert data containing only observations, we can achieve efficient exploration in challenging sparse tasks, well beyond what is possible with current state-of-the-art approaches. We have made the curated gameplay dataset used in this work available at https://github.com/deepmind/nao_top10.", "ratings": "[8, 6, 8]", "confidences": "[3, 3, 3]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "8aHzds2uUyB", "title": "Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization", "keywords": "['natural language processing', 'reinforcement learning', 'language models', 'feedback learning']", "abstract": "We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, $RL4LMs$ (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the $GRUE$ (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, $NLPO$ (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.", "ratings": "[8, 8, 6, 6]", "confidences": "[4, 3, 3, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "UuchYL8wSZo", "title": "Learning Flexible Visual Representations via Interactive Gameplay", "keywords": "representation learning, deep reinforcement learning, computer vision", "abstract": "A growing body of research suggests that embodied gameplay, prevalent not just in human cultures but across a variety of animal species including turtles and ravens, is critical in developing the neural flexibility for creative problem solving, decision making and socialization. Comparatively little is known regarding the impact of embodied gameplay upon artificial agents. While recent work has produced agents proficient in abstract games, these environments are far removed the real world and thus these agents can provide little insight into the advantages of embodied play. Hiding games, such as hide-and-seek, played universally, provide a rich ground for studying the impact of embodied gameplay on representation learning in the context of perspective taking, secret keeping, and false belief understanding. Here we are the first to show that embodied adversarial reinforcement learning agents playing Cache, a variant of hide-and-seek, in a high fidelity, interactive, environment, learn flexible representations of their observations encoding information such as object permanence, free space, and containment. Moving closer to biologically motivated learning strategies, our agents' representations, enhanced by intentionality and memory, are developed through interaction and play. These results serve as a model for studying how facets of vision develop through interaction, provide an experimental framework for assessing what is learned by artificial agents, and demonstrates the value of moving from large, static, datasets towards experiential, interactive, representation learning.", "ratings": "[9.0, 8.0, 8.0, 8.0]", "decision": "Accept (Oral)", "year": "2021"}
{"paper_id": "q3F0UBAruO", "title": "Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective", "keywords": "['game playing', 'multi-agent', 'human-ai communication', 'human-ai collaboration', 'deep reinforcement learning']", "abstract": "MOBA games, e.g., Dota2 and Honor of Kings, have been actively used as the testbed for the recent AI research on games, and various AI systems have been developed at the human level so far. However, these AI systems mainly focus on how to compete with humans, less on exploring how to collaborate with humans. To this end, this paper makes the first attempt to investigate human-agent collaboration in MOBA games. In this paper, we propose to enable humans and agents to collaborate through explicit communication by designing an efficient and interpretable Meta-Command Communication-based framework, dubbed MCC, for accomplishing effective human-agent collaboration in MOBA games. The MCC framework consists of two pivotal modules: 1) an interpretable communication protocol, i.e., the Meta-Command, to bridge the communication gap between humans and agents; 2) a meta-command value estimator, i.e., the Meta-Command Selector, to select a valuable meta-command for each agent to achieve effective human-agent collaboration. Experimental results in Honor of Kings demonstrate that MCC agents can collaborate reasonably well with human teammates and even generalize to collaborate with different levels and numbers of human teammates. Videos are available at https://sites.google.com/view/mcc-demo.", "ratings": "[6, 6, 8]", "confidences": "[3, 4, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "nIAxjsniDzg", "title": "What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study", "keywords": "Reinforcement learning, continuous control", "abstract": "In recent years, reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``\"choices\" in a unified on-policy deep actor-critic framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for the training of on-policy deep actor-critic RL agents.", "ratings": "[7.0, 9.0, 9.0, 7.0]", "decision": "Accept (Oral)", "year": "2021"}
{"paper_id": "P4MUGRM4Acu", "title": "The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry", "keywords": "['Equivariant Learning', 'Reinforcement Learning', 'Robotics']", "abstract": "Extensive work has demonstrated that equivariant neural networks can significantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the domain symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the input. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surprisingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model's performance, imposing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems.", "ratings": "[8, 8, 8, 8]", "confidences": "[2, 3, 3, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "0-uUGPbIjD", "title": "Human-Level Performance in No-Press Diplomacy via Equilibrium Search", "keywords": "multi-agent systems, regret minimization, no-regret learning, game theory, reinforcement learning", "abstract": "Prior AI breakthroughs in complex games have focused on either the purely adversarial or purely cooperative settings. In contrast, Diplomacy is a game of shifting alliances that involves both cooperation and competition. For this reason, Diplomacy has proven to be a formidable research challenge. In this paper we describe an agent for the no-press variant of Diplomacy that combines supervised learning on human data with one-step lookahead search via external regret minimization. External regret minimization techniques have been behind previous AI successes in adversarial games, most notably poker, but have not previously been shown to be successful in large-scale games involving cooperation. We show that our agent greatly exceeds the performance of past no-press Diplomacy bots, is unexploitable by expert humans, and achieves a rank of 23 out of 1,128 human players when playing anonymous games on a popular Diplomacy website.", "ratings": "[7.0, 8.0, 7.0, 8.0]", "decision": "Accept (Oral)", "year": "2021"}
{"paper_id": "AWZgXGmsbA", "title": "Powderworld: A Platform for Understanding Generalization via Rich Task Distributions", "keywords": "['reinforcement learning', 'environment', 'generalization', 'out-of-distribution', 'multi-task']", "abstract": "One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating task distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models, yet causes reinforcement learning agents to struggle. Powderworld aims to support the study of generalization by providing a source of diverse tasks arising from the same core rules.", "ratings": "[8, 8, 8, 8]", "confidences": "[4, 4, 4, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "rALA0Xo6yNJ", "title": "Learning to Reach Goals via Iterated Supervised Learning", "keywords": "goal reaching, reinforcement learning, behavior cloning, goal-conditioned RL", "abstract": "Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation learning provides a simple and stable alternative, it requires access to demonstrations from a human supervisor. In this paper, we study RL algorithms that use imitation learning to acquire goal reaching policies from scratch, without the need for expert demonstrations or a value function. In lieu of demonstrations, we leverage the property that any trajectory is a successful demonstration for reaching the final state in that same trajectory. We propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal-reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. We formally show that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy, and empirically demonstrate improved goal-reaching performance and robustness over current RL algorithms in several benchmark tasks.", "ratings": "[7.0, 8.0, 7.0, 8.0]", "decision": "Accept (Oral)", "year": "2021"}
{"paper_id": "eQzLwwGyQrb", "title": "Can We Find Nash Equilibria at a Linear Rate in Markov Games?", "keywords": "['Multi-Agent Reinforcement Learning', 'Markov Games', 'Linear Convergence', 'Policy Optimization']", "abstract": "We study decentralized learning in two-player zero-sum discounted Markov games where the goal is to design a policy optimization algorithm for either agent satisfying two properties. First, the player does not need to know the policy of the opponent to update its policy. Second, when both players adopt the algorithm, their joint policy converges to a Nash equilibrium of the game. To this end, we construct a meta-algorithm, dubbed as $\\texttt{Homotopy-PO}$, which provably finds a Nash equilibrium at a global linear rate. In particular, $\\texttt{Homotopy-PO}$ interweaves two base algorithms $\\texttt{Local-Fast}$ and $\\texttt{Global-Slow}$ via homotopy continuation. $\\texttt{Local-Fast}$ is an algorithm that enjoys local linear convergence while $\\texttt{Global-Slow}$ is an algorithm that converges globally but at a slower sublinear rate. By switching between these two base algorithms, $\\texttt{Global-Slow}$ essentially serves as a ``guide'' which identifies a benign neighborhood where $\\texttt{Local-Fast}$ enjoys fast convergence. However, since the exact size of such a neighborhood is unknown, we apply a doubling trick to switch between these two base algorithms. The switching scheme is delicately designed so that the aggregated performance of the algorithm is driven by $\\texttt{Local-Fast}$. Furthermore, we prove that $\\texttt{Local-Fast}$ and $\\texttt{Global-Slow}$ can both be instantiated by variants of optimistic gradient descent/ascent (OGDA) method, which is of independent interest.", "ratings": "[8, 8, 10, 8]", "confidences": "[3, 2, 3, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "DJEEqoAq7to", "title": "RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch", "keywords": "['Deep Reinforcement Learning', 'Lottery Ticket Hypothesis', 'Model Compression', 'Value Learning']", "abstract": "Training deep reinforcement learning (DRL) models usually requires high computation costs. Therefore, compressing DRL models possesses immense potential for training acceleration and model deployment. However, existing methods that generate small models mainly adopt the knowledge distillation-based approach by iteratively training a dense network. As a result, the training process still demands massive computing resources. Indeed, sparse training from scratch in DRL has not been well explored and is particularly challenging due to non-stationarity in bootstrap training. In this work, we propose a novel sparse DRL training framework, \u201cthe Rigged Reinforcement Learning Lottery\u201d (RLx2), which builds upon gradient-based topology evolution and is capable of training a sparse DRL model based entirely on a sparse network. Specifically, RLx2 introduces a novel multi-step TD target mechanism with a dynamic-capacity replay buffer to achieve robust value learning and efficient topology exploration in sparse models. It also reaches state-of-the-art sparse training performance in several tasks, showing $7.5\\times$-$20\\times$ model compression with less than $3\\%$ performance degradation and up to $20\\times$ and $50\\times$ FLOPs reduction for training and inference, respectively.", "ratings": "[8, 8, 8, 6]", "confidences": "[4, 4, 3, 3]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "Z3IClM_bzvP", "title": "Multi-skill Mobile Manipulation for Object Rearrangement", "keywords": "['mobile manipulation', 'reinforcement learning']", "abstract": "We study a modular approach to tackle long-horizon mobile manipulation tasks for object rearrangement, which decomposes a full task into a sequence of subtasks. To tackle the entire task, prior work chains multiple stationary manipulation skills with a point-goal navigation skill, which are learned individually on subtasks. Although more effective than monolithic end-to-end RL policies, this framework suffers from compounding errors in skill chaining, e.g., navigating to a bad location where a stationary manipulation skill can not reach its target to manipulate. To this end, we propose that the manipulation skills should include mobility to have flexibility in interacting with the target object from multiple locations and at the same time the navigation skill could have multiple end points which lead to successful manipulation. We operationalize these ideas by implementing mobile manipulation skills rather than stationary ones and training a navigation skill trained with region goal instead of point goal. We evaluate our multi-skill mobile manipulation method M3 on 3 challenging long-horizon mobile manipulation tasks in the Home Assistant Benchmark (HAB), and show superior performance as compared to the baselines.", "ratings": "[5, 6, 10, 8]", "confidences": "[4, 4, 5, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "O5rKg7IRQIO", "title": "Guarded Policy Optimization with Imperfect Online Demonstrations", "keywords": "['reinforcement learning', 'guarded policy optimization', 'imperfect demonstrations', 'shared control', 'metadrive simulator']", "abstract": "The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C.", "ratings": "[8, 5, 6, 8]", "confidences": "[4, 3, 3, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "UKr0MwZM6fL", "title": "Building a Subspace of Policies for Scalable Continual Learning", "keywords": "['continual learning', 'deep reinforcement learning']", "abstract": "The ability to continuously acquire new knowledge and skills is crucial for autonomous agents. Existing methods are typically based on either fixed-size models that struggle to learn a large number of diverse behaviors, or growing-size models that scale poorly with the number of tasks. In this work, we aim to strike a better balance between scalability and performance by designing a method whose size grows adaptively depending on the task sequence. We introduce Continual Subspace of Policies (CSP), a new approach that incrementally builds a subspace of policies for training a reinforcement learning agent on a sequence of tasks. The subspace's high expressivity allows CSP to perform well for many different tasks while growing more slowly than the number of tasks. Our method does not suffer from forgetting and also displays positive transfer to new tasks. CSP outperforms a number of popular baselines on a wide range of scenarios from two challenging domains, Brax (locomotion) and Continual World (robotic manipulation). Interactive visualizations of the subspace can be found at https://share.streamlit.io/continual-subspace/policies/main.", "ratings": "[6, 6, 8, 8, 8]", "confidences": "[4, 3, 4, 4, 2]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "688hNNMigVX", "title": "Learning a Data-Driven Policy Network for Pre-Training Automated Feature Engineering", "keywords": "['Automated Feature Engineering', 'Reinforcement Learning', 'Tabular Data', 'Data-Driven', 'Pre-Training']", "abstract": "Feature engineering is widely acknowledged to be pivotal in tabular data analysis and prediction. Automated feature engineering (AutoFE) emerged to automate this process managed by experienced data scientists and engineers conventionally. In this area, most \u2014 if not all \u2014 prior work adopted an identical framework from the neural architecture search (NAS) method. While feasible, we posit that the NAS framework very much contradicts the way how human experts cope with the data since the inherent Markov decision process (MDP) setup differs. We point out that its data-unobserved setup consequentially results in an incapability to generalize across different datasets as well as also high computational cost. This paper proposes a novel AutoFE framework Feature Set Data-Driven Search (FETCH), a pipeline mainly for feature generation and selection. Notably, FETCH is built on a brand-new data-driven MDP setup using the tabular dataset as the state fed into the policy network. Further, we posit that the crucial merit of FETCH is its transferability where the yielded policy network trained on a variety of datasets is indeed capable to enact feature engineering on unseen data, without requiring additional exploration. To the best of our knowledge, this is a pioneer attempt to build a tabular data pre-training paradigm via AutoFE. Extensive experiments show that FETCH systematically surpasses the current state-of-the-art AutoFE methods and validates the transferability of AutoFE pre-training.", "ratings": "[8, 8, 8]", "confidences": "[4, 5, 3]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "UcDUxjPYWSr", "title": "Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design", "keywords": "['Agent Design', 'Morphology Optimization', 'Reinforcement Learning']", "abstract": "An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables first-order optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency tremendously.  Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Our project website is at https://sites.google.com/view/transform2act.", "ratings": "[8.0, 8.0, 8.0, 8.0]", "decision": "Accept (Oral)", "year": "2022"}
{"paper_id": "m5Qsh0kBQG", "title": "Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients", "keywords": "symbolic regression, reinforcement learning, automated machine learning", "abstract": "Discovering the underlying mathematical expressions describing a dataset is a core challenge for artificial intelligence. This is the problem of symbolic regression. Despite recent advances in training neural networks to solve complex tasks, deep learning approaches to symbolic regression are underexplored. We propose a framework that leverages deep learning for symbolic regression via a simple idea: use a large model to search the space of small models. Specifically, we use a recurrent neural network to emit a distribution over tractable mathematical expressions and employ a novel risk-seeking policy gradient to train the network to generate better-fitting expressions. Our algorithm outperforms several baseline methods (including Eureqa, the gold standard for symbolic regression) in its ability to exactly recover symbolic expressions on a series of benchmark problems, both with and without added noise. More broadly, our contributions include a framework that can be applied to optimize hierarchical, variable-length objects under a black-box performance metric, with the ability to incorporate constraints in situ, and a risk-seeking policy gradient formulation that optimizes for best-case performance instead of expected performance.", "ratings": "[8.0, 7.0, 8.0, 9.0]", "decision": "Accept (Oral)", "year": "2021"}
{"paper_id": "hQ9V5QN27eS", "title": "Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning", "keywords": "['reinforcement learning', 'exploration', 'action noise', 'continuous control']", "abstract": "In off-policy deep reinforcement learning with continuous action spaces, exploration is often implemented by injecting action noise into the action selection process. Popular algorithms based on stochastic policies, such as SAC or MPO, inject white noise by sampling actions from uncorrelated Gaussian distributions. In many tasks, however, white noise does not provide sufficient exploration, and temporally correlated noise is used instead. A common choice is Ornstein-Uhlenbeck (OU) noise, which is closely related to Brownian motion (red noise). Both red noise and white noise belong to the broad family of colored noise. In this work, we perform a comprehensive experimental evaluation on MPO and SAC to explore the effectiveness of other colors of noise as action noise. We find that pink noise, which is halfway between white and red noise, significantly outperforms white noise, OU noise, and other alternatives on a wide range of environments. Thus, we recommend it as the default choice for action noise in continuous control. ", "ratings": "[8, 8, 8]", "confidences": "[4, 4, 5]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "uLE3WF3-H_5", "title": "Adversarial Diversity in Hanabi", "keywords": "['coordination', 'diversity', 'multi-agent reinforcement learning']", "abstract": "Many Dec-POMDPs admit a qualitatively diverse set of ''reasonable'' joint policies, where reasonableness is indicated by symmetry equivariance, non-sabotaging behaviour and the graceful degradation of performance when paired with ad-hoc partners. Some of the work in diversity literature is concerned with generating these policies. Unfortunately, existing methods fail to produce teams of agents that are simultaneously diverse, high performing, and reasonable. In this work, we propose a novel approach, adversarial diversity (ADVERSITY), which is designed for turn-based Dec-POMDPs with public actions. ADVERSITY relies on off-belief learning to encourage reasonableness and skill, and on ''repulsive'' fictitious transitions to encourage diversity. We use this approach to generate new agents with distinct but reasonable play styles for the card game Hanabi and open-source our agents to be used for future research on (ad-hoc) coordination.", "ratings": "[6, 8, 6]", "confidences": "[2, 4, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "0XXpJ4OtjW", "title": "Evolving Reinforcement Learning Algorithms", "keywords": "reinforcement learning, evolutionary algorithms, meta-learning, genetic programming", "abstract": "We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithm is domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, enabling interpretable modifications which obtain superior performance. We highlight two learned algorithms which obtain good generalization performance over a set of classical control tasks and gridworld type tasks. Our analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that have been designed manually.", "ratings": "[7.0, 6.0, 9.0]", "decision": "Accept (Oral)", "year": "2021"}
{"paper_id": "hWwY_Jq0xsN", "title": "Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes", "keywords": "['Interpretable Machine Learning', 'Deep Reinforcement Learning', 'Prototypes', 'User Study']", "abstract": "Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an \"interpretable-by-design\" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes. ", "ratings": "[8, 8, 8, 6]", "confidences": "[4, 5, 4, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "HcUf-QwZeFh", "title": "A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation", "keywords": "['Morphology-Task Generalization', 'Behavior Distillation', 'Supervised RL', 'Reinforcement Learning']", "abstract": "The rise of generalist large-scale models in natural language and vision has made us expect that a massive data-driven approach could achieve broader generalization in other domains such as continuous control. In this work, we explore a method for learning a single policy that manipulates various forms of agents to solve various tasks by distilling a large amount of proficient behavioral data. In order to align input-output (IO) interface among multiple tasks and diverse agent morphologies while preserving essential 3D geometric relations, we introduce morphology-task graph, which treats observations, actions and goals/task in a unified graph representation. We also develop MxT-Bench for fast large-scale behavior generation, which supports procedural generation of diverse morphology-task combinations with a minimal blueprint and hardware-accelerated simulator. Through efficient representation and architecture selection on MxT-Bench, we find out that a morphology-task graph representation coupled with Transformer architecture improves the multi-task performances compared to other baselines including recent discrete tokenization, and provides better prior knowledge for zero-shot transfer or sample efficiency in downstream multi-task imitation learning. Our work suggests large diverse offline datasets, unified IO representation, and policy representation and architecture selection through supervised learning form a promising approach for studying and advancing morphology-task generalization.", "ratings": "[5, 8, 8, 8]", "confidences": "[4, 2, 5, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "cPZOyoDloxl", "title": "SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments", "keywords": "Reinforcement learning", "abstract": "Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning.", "ratings": "[7.0, 8.0, 7.0, 7.0]", "decision": "Accept (Oral)", "year": "2021"}
{"paper_id": "C-xa_D3oTj6", "title": "DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems", "keywords": "['reinforcement learning', 'musculoskeletal', 'correlated exploration']", "abstract": "Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.", "ratings": "[8, 8, 8, 10]", "confidences": "[2, 4, 4, 4]", "decision": "Accept (Oral)", "year": "2023"}
{"paper_id": "vrW3tvDfOJQ", "title": "Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation", "keywords": "['Deep reinforcement learning', 'uncertainty estimation', 'inverse-variance', 'heteroscedastic']", "abstract": "In model-free deep reinforcement learning (RL) algorithms, using noisy value estimates to supervise policy evaluation and optimization is detrimental to the sample efficiency. As this noise is heteroscedastic, its effects can be mitigated using uncertainty-based weights in the optimization process. Previous methods rely on sampled ensembles, which do not capture all aspects of uncertainty. We provide a systematic analysis of the sources of uncertainty in the noisy supervision that occurs in RL, and introduce inverse-variance RL, a Bayesian framework which combines probabilistic ensembles and Batch Inverse Variance weighting. We propose a method whereby two complementary uncertainty estimation methods account for both the Q-value and the environment stochasticity to better mitigate the negative impacts of noisy supervision. Our results show significant improvement in terms of sample efficiency on discrete and continuous control tasks.", "ratings": "[10.0, 8.0, 8.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "LmUJqB1Cz8", "title": "Winning the L2RPN Challenge: Power Grid Management via Semi-Markov Afterstate Actor-Critic", "keywords": "power grid management, deep reinforcement learning, graph neural network", "abstract": "Safe and reliable electricity transmission in power grids is crucial for modern society. It is thus quite natural that there has been a growing interest in the automatic management of power grids, exempli\ufb01ed by the Learning to Run a Power Network Challenge (L2RPN), modeling the problem as a reinforcement learning (RL) task. However, it is highly challenging to manage a real-world scale power grid, mostly due to the massive scale of its state and action space. In this paper, we present an off-policy actor-critic approach that effectively tackles the unique challenges in power grid management by RL, adopting the hierarchical policy together with the afterstate representation. Our agent ranked \ufb01rst in the latest challenge (L2RPN WCCI 2020), being able to avoid disastrous situations while maintaining the highest level of operational ef\ufb01ciency in every test scenarios. This paper provides a formal description of the algorithmic aspect of our approach, as well as further experimental studies on diverse power grids.", "ratings": "[7.0, 7.0, 7.0, 9.0]", "decision": "Accept (Spotlight)", "year": "2021"}
{"paper_id": "qTHBE7E9iej", "title": "Learning transferable motor skills with hierarchical latent mixture policies", "keywords": "['Robotics', 'Reinforcement Learning', 'Hierarchical', 'Latent Variable Models', 'Skills', 'Transfer']", "abstract": "For robots operating in the real world, it is desirable to learn reusable abstract behaviours that can effectively be transferred across numerous tasks and scenarios.         We propose an approach to learn skills from data using a hierarchical mixture latent variable model.         Our method exploits a multi-level hierarchy of both discrete and continuous latent variables, to model a discrete set of abstract high-level behaviours while allowing for variance in how they are executed.         We demonstrate in manipulation domains that the method can effectively cluster offline data into distinct, executable behaviours, while retaining the flexibility of a continuous latent variable model.         The resulting skills can be transferred to new tasks, unseen objects, and from state to vision-based policies, yielding significantly better sample efficiency and asymptotic performance compared to existing skill- and imitation-based methods.         We also perform further analysis showing how and when the skills are most beneficial: they encourage directed exploration to cover large regions of the state space relevant to the task, making them most effective in challenging sparse-reward settings.", "ratings": "[8.0, 8.0, 8.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "zz9hXVhf40", "title": "Revisiting Design Choices in Offline Model Based Reinforcement Learning", "keywords": "['Model-Based Reinforcement Learning', 'Offline Reinforcement Learning', 'Uncertainty Quantification']", "abstract": "Offline reinforcement learning enables agents to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. Significant progress has been made recently in offline model-based reinforcement learning, approaches which leverage a learned dynamics model. This typically involves constructing a probabilistic model, and using the model uncertainty to penalize rewards where there is insufficient data, solving for a pessimistic MDP that lower bounds the true MDP. Existing methods, however, exhibit a breakdown between theory and practice, whereby pessimistic return ought to be bounded by the total variation distance of the model from the true dynamics, but is instead implemented through a penalty based on estimated model uncertainty. This has spawned a variety of uncertainty heuristics, with little to no comparison between differing approaches. In this paper, we compare these heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. Using these insights, we show that selecting these key hyperparameters using Bayesian Optimization produces superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance.", "ratings": "[6.0, 6.0, 8.0, 6.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "o_V-MjyyGV_", "title": "Self-Supervised Policy Adaptation during Deployment", "keywords": "reinforcement learning, robotics, self-supervised learning, generalization, sim2real", "abstract": "In most real world scenarios, a policy trained by reinforcement learning in one environment needs to be deployed in another, potentially quite different environment. However, generalization across different environments is known to be hard. A natural solution would be to keep training after deployment in the new environment, but this cannot be done if the new environment offers no reward signal. Our work explores the use of self-supervision to allow the policy to continue training after deployment without using any rewards. While previous methods explicitly anticipate changes in the new environment, we assume no prior knowledge of those changes yet still obtain significant improvements. Empirical evaluations are performed on diverse simulation environments from DeepMind Control suite and ViZDoom, as well as real robotic manipulation tasks in  continuously changing environments, taking observations from an uncalibrated camera. Our method improves generalization in 28 out of 32 environments across various tasks and outperforms domain randomization on a majority of environments. Videos are available at https://iclr2021submission.github.io/ICLR2021_Anonymized_PAD/.", "ratings": "[7.0, 7.0, 7.0, 7.0]", "decision": "Accept (Spotlight)", "year": "2021"}
{"paper_id": "CAjxVodl_v", "title": "Distributional Decision Transformer for Hindsight Information Matching", "keywords": "['Hindsight Information Matching', 'Decision Transformer', 'State-Marginal Matching', 'Hindsight Experience Replay', 'Reinforcement Learning']", "abstract": "How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for practical applications. Recent works have shown that using expressive policy function approximators and conditioning on future trajectory information -- such as future states in hindsight experience replay (HER) or returns-to-go in Decision Transformer (DT) -- enables efficient learning of context-conditioned policies, where at times online RL can be fully replaced by offline behavioral cloning (BC), e.g. sequence modeling. Inspired by distributional and state-marginal matching literatures in RL, we demonstrate that all these approaches are essentially doing hindsight information matching (HIM) -- training policies that can output the rest of trajectory that matches a given future state information statistics. We first present Distributional Decision Transformer (DDT) and its two practical instantiations, Categorical and Gaussian DTs, and show that these simple modifications to DT can enable effective offline state-marginal matching that generalizes well to unseen, even synthetic multi-modal, reward or state-feature distributions. We perform experiments on Gym's MuJoCo continuous control benchmarks and empirically validate performances. Additionally, we present and test another simple modification to DT called Unsupervised DT (UDT), show its connection to distribution matching, inverse RL and representation learning, and empirically demonstrate their effectiveness for offline imitation learning. To the best of our knowledge, DDT and UDT together constitute the first successes for offline state-marginal matching and inverse-RL imitation learning, allowing us to propose first benchmarks for these two important subfields and greatly expand the role of powerful sequence modeling architectures in modern RL.", "ratings": "[6.0, 8.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "xvxPuCkCNPO", "title": "Correcting experience replay for multi-agent communication", "keywords": "multi-agent reinforcement learning, experience replay, communication, relabelling", "abstract": "We consider the problem of learning to communicate using multi-agent reinforcement learning (MARL). A common approach is to learn off-policy, using data sampled from a replay buffer. However, messages received in the past may not accurately reflect the current communication policy of each agent, and this complicates learning. We therefore introduce a 'communication correction' which accounts for the non-stationarity of observed communication induced by multi-agent learning. It works by relabelling the received message to make it likely under the communicator's current policy, and thus be a better reflection of the receiver's current environment. To account for cases in which agents are both senders and receivers, we introduce an ordered relabelling scheme. Our correction is computationally efficient and can be integrated with a range of off-policy algorithms. It substantially improves the ability of communicating MARL systems to learn across a variety of cooperative and competitive tasks.", "ratings": "[8.0, 8.0, 7.0, 7.0]", "decision": "Accept (Spotlight)", "year": "2021"}
{"paper_id": "dEwfxt14bca", "title": "When should agents explore?", "keywords": "['exploration', 'mode-switching', 'reinforcement learning', 'Atari']", "abstract": "Exploration remains a central challenge for reinforcement learning (RL). Virtually all existing methods share the feature of a *monolithic* behaviour policy that changes only gradually (at best). In contrast, the exploratory behaviours of animals and humans exhibit a rich diversity, namely including forms of *switching* between modes. This paper presents an initial study of mode-switching, non-monolithic exploration for RL. We investigate different modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. We also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. Finally, we report a promising initial study on Atari, using two-mode exploration and switching at sub-episodic time-scales.", "ratings": "[6.0, 8.0, 8.0, 6.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "yr1mzrH3IC", "title": "Regularization Matters in Policy Optimization - An Empirical Study on Continuous Control", "keywords": "Policy Optimization, Regularization, Continuous Control, Deep Reinforcement Learning", "abstract": "Deep Reinforcement Learning (Deep RL) has been receiving increasingly more attention  thanks to its encouraging performance on a variety of control tasks. Yet, conventional regularization techniques in training neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs. In this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/anonymouscode114/iclr2021_rlreg .", "ratings": "[7.0, 6.0, 7.0, 7.0]", "decision": "Accept (Spotlight)", "year": "2021"}
{"paper_id": "FeWvD0L_a4", "title": "Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection", "keywords": "['Deep Reinforcement Learning', 'The Arcade Learning Environment', 'Human World Records', 'Behavioral Control']", "abstract": "The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity. In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10077.52% mean human normalized score and surpassed 24 human world records within 1B training frames in the Arcade Learning Environment, which demonstrates our significant state-of-the-art (SOTA) performance without degrading the sample efficiency.", "ratings": "[8, 8, 10]", "confidences": "[4, 4, 5]", "decision": "Accept (Spotlight)", "year": "2023"}
{"paper_id": "wQfgfb8VKTn", "title": "Context-Aware Sparse Deep Coordination Graphs", "keywords": "['Multi-agent reinforcement learning', 'Sparse coordination graphs', 'Deep coordination graphs']", "abstract": "Learning sparse coordination graphs adaptive to the coordination dynamics among agents is a long-standing problem in cooperative multi-agent learning. This paper studies this problem and proposes a novel method using the variance of payoff functions to construct context-aware sparse coordination topologies. We theoretically consolidate our method by proving that the smaller the variance of payoff functions is, the less likely action selection will change after removing the corresponding edge. Moreover, we propose to learn action representations to effectively reduce the influence of payoff functions' estimation errors on graph construction. To empirically evaluate our method, we present the Multi-Agent COordination (MACO) benchmark by collecting classic coordination problems in the literature, increasing their difficulty, and classifying them into different types. We carry out a case study and experiments on the MACO and StarCraft II micromanagement benchmark to demonstrate the dynamics of sparse graph learning, the influence of graph sparseness, and the learning performance of our method.", "ratings": "[8.0, 6.0, 6.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "R4aWTjmrEKM", "title": "Iterative Empirical Game Solving via Single Policy Best Response", "keywords": "Empirical Game Theory, Reinforcement Learning, Multiagent Learning", "abstract": "Policy-Space Response Oracles (PSRO) is a general algorithmic framework for learning policies in multiagent systems by interleaving empirical game analysis with deep reinforcement learning (DRL).       At each iteration, DRL is invoked to train a best response to a mixture of opponent policies.       The repeated application of DRL poses an expensive computational burden as we look to apply this algorithm to more complex domains.       We introduce two variations of PSRO designed to reduce the amount of simulation required during DRL training.       Both algorithms modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy.       The first, Mixed-Oracles, transfers knowledge from previous iterations of DRL, requiring training only against the opponent's newest policy.       The second, Mixed-Opponents, constructs a pure-strategy opponent by mixing existing strategy's action-value estimates, instead of their policies.       Learning against a single policy mitigates conflicting experiences on behalf of a learner facing an unobserved distribution of opponents.       We empirically demonstrate that these algorithms substantially reduce the amount of simulation during training required by PSRO, while producing equivalent or better solutions to the game.", "ratings": "[7.0, 7.0, 7.0, 7.0]", "decision": "Accept (Spotlight)", "year": "2021"}
{"paper_id": "PLDOnFoVm4", "title": "Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory", "keywords": "['Multi-agent Reinforcement Learning', 'Predictive State Representation', 'Dynamic Interaction Graph']", "abstract": "This paper proposes a new algorithm for learning the optimal policies under a novel multi-agent predictive state representation reinforcement learning model. Compared to the state-of-the-art methods, the most striking feature of our approach is the introduction of a dynamic interaction graph to the model, which allows us to represent each agent's predictive state by considering the behaviors of its ``neighborhood'' agents. Methodologically, we develop an online algorithm that simultaneously learns the predictive state representation and agent policies.          Theoretically, we provide an upper bound of the $L_2$-norm of the learned predictive state representation. Empirically, to demonstrate the efficacy of the proposed method, we provide thorough numerical results on both a MAMuJoCo robotic learning experiment and a multi-agent particle learning environment.", "ratings": "[8.0, 8.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "SidzxAb9k30", "title": "Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver", "keywords": "['reward-free exploration', 'model-based reinforcement learning', 'learning theory']", "abstract": "Although model-based reinforcement learning (RL) approaches are considered more sample efficient, existing algorithms are usually relying on sophisticated planning algorithm to couple tightly with the model-learning procedure. Hence the learned models may lack the ability of being re-used with more specialized planners. In this paper we address this issue and provide approaches to learn an RL model efficiently without the guidance of a reward signal. In particular, we take a plug-in solver approach, where we focus on learning a model in the exploration phase and demand that \\emph{any planning algorithm} on the learned model can give a near-optimal policy. Specicially, we focus on the linear mixture MDP setting, where the probability transition matrix is a (unknown) convex combination of a set of existing models. We show that, by establishing a novel exploration algorithm, the plug-in approach learns a model by taking $\\tilde{O}(d^2H^3/\\epsilon^2)$ interactions with the environment and \\emph{any} $\\epsilon$-optimal planner on the model gives an $O(\\epsilon)$-optimal policy on the original model. This sample complexity matches lower bounds for non-plug-in approaches and is \\emph{statistically optimal}. We achieve this result by leveraging a careful maximum total-variance bound using Bernstein inequality and properties specified to linear mixture MDP.", "ratings": "[8.0, 6.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "CALFyKVs87", "title": "Dynamics-Aware Comparison of Learned Reward Functions", "keywords": "['Reward Learning', 'Inverse Reinforcement Learning', 'Reinforcement Learning', 'Comparing Reward Functions']", "abstract": "The ability to learn reward functions plays an important role in enabling the deployment of intelligent agents in the real world. However, $\\textit{comparing}$ reward functions, for example as a means of evaluating reward learning methods, presents a challenge. Reward functions are typically compared by considering the behavior of optimized policies, but this approach conflates deficiencies in the reward function with those of the policy search algorithm used to optimize it. To address this challenge, Gleave et al. (2020) propose the Equivalent-Policy Invariant Comparison (EPIC) distance. EPIC avoids policy optimization, but in doing so requires computing reward values at transitions that may be impossible under the system dynamics. This is problematic for learned reward functions because it entails evaluating them outside of their training distribution, resulting in inaccurate reward values that we show can render EPIC ineffective at comparing rewards. To address this problem, we propose the Dynamics-Aware Reward Distance (DARD), a new reward pseudometric. DARD uses an approximate transition model of the environment to transform reward functions into a form that allows for comparisons that are invariant to reward shaping while only evaluating reward functions on transitions close to their training distribution. Experiments in simulated physical domains demonstrate that DARD enables reliable reward comparisons without policy optimization and is significantly more predictive than baseline methods of downstream policy performance when dealing with learned reward functions.", "ratings": "[6.0, 8.0, 5.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "3OR2tbtnYC-", "title": "Near-optimal Policy Identification in Active Reinforcement Learning", "keywords": "['reinforcement learning', 'contextual bayesian optimization', 'kernelized least-squares value iteration']", "abstract": "Many real-world reinforcement learning tasks require control of complex dynamical systems that involve both costly data acquisition processes and large state spaces. In cases where the expensive transition dynamics can be readily evaluated at specified states (e.g., via a simulator), agents can operate in what is often referred to as planning with a \\emph{generative model}. We propose the AE-LSVI algorithm for best policy identification, a novel variant of the kernelized least-squares value iteration (LSVI) algorithm that combines optimism with pessimism for active exploration (AE). AE-LSVI provably identifies a near-optimal policy \\emph{uniformly} over an entire state space and achieves polynomial sample complexity guarantees that are independent of the number of states. When specialized to the recently introduced offline contextual Bayesian optimization setting, our algorithm achieves improved sample complexity bounds. Experimentally, we demonstrate that AE-LSVI outperforms other RL algorithms in a variety of environments when robustness to the initial state is required. ", "ratings": "[8, 8, 8]", "confidences": "[3, 4, 3]", "decision": "Accept (Spotlight)", "year": "2023"}
{"paper_id": "OthEq8I5v1", "title": "Mutual Information State Intrinsic Control", "keywords": "Intrinsically Motivated Reinforcement Learning, Intrinsic Reward, Intrinsic Motivation, Deep Reinforcement Learning, Reinforcement Learning", "abstract": "Reinforcement learning has been shown to be highly successful at many challenging tasks. However, success heavily relies on well-shaped rewards. Intrinsically motivated RL attempts to remove this constraint by defining an intrinsic reward function. Motivated by the self-consciousness concept in psychology, we make a natural assumption that the agent knows what constitutes itself, and propose a new intrinsic objective that encourages the agent to have maximum control on the environment. We mathematically formalize this reward as the mutual information between the agent state and the surrounding state under the current agent policy. With this new intrinsic motivation, we are able to outperform previous methods, including being able to complete the pick-and-place task for the first time without using any task reward. A video showing experimental results is available in the supplementary material.", "ratings": "[7.0, 7.0, 7.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2021"}
{"paper_id": "UcoXdfrORC", "title": "Model-Based Visual Planning with Self-Supervised Functional Distances", "keywords": "planning, model learning, distance learning, reinforcement learning, robotics", "abstract": "A generalist robot must be able to complete a variety of tasks in its environment. One appealing way to specify each task is in terms of a goal observation. However, learning goal-reaching policies with reinforcement learning remains a challenging problem, particularly when rewards are not provided and distances in the observation space are not meaningful. Learned dynamics models are a promising approach for learning about the environment without rewards or task-directed data, but planning to reach goals with such a model requires a notion of functional similarity between observations and goal states. We present a self-supervised method for model-based visual goal reaching, which uses both a visual dynamics model as well as a dynamical distance function learned using model-free reinforcement learning. This approach trains entirely using offline, unlabeled data, making it practical to scale to large and diverse datasets. On several challenging robotic manipulation tasks with only offline, unlabeled data, we find that our algorithm compares favorably to prior model-based and model-free reinforcement learning methods. In ablation experiments, we additionally identify important factors for learning effective distances.", "ratings": "[7.0, 7.0, 7.0, 7.0]", "decision": "Accept (Spotlight)", "year": "2021"}
{"paper_id": "YJ1WzgMVsMt", "title": "Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration", "keywords": "['Reinforcement Learning', 'Sparse Rewards', 'Learning from Demonstrations']", "abstract": "A major challenge in real-world reinforcement learning (RL) is the sparsity of reward feedback.  Often, what is available is an intuitive but sparse reward function that only indicates whether the task is completed partially or fully.  However, the lack of carefully designed, fine grain feedback implies that most existing RL algorithms fail to learn an acceptable policy in a reasonable time frame.  This is because of the large number of exploration actions that the policy has to perform before it gets any useful feedback that it can learn from.  In this work, we address this challenging problem by developing an algorithm that exploits the offline demonstration data generated by {a sub-optimal behavior policy} for faster and efficient online RL in such sparse reward settings.  The proposed algorithm, which we call the Learning Online with Guidance Offline (LOGO) algorithm, merges a policy improvement step with an additional policy guidance step by using the offline demonstration data.  The key idea is that by obtaining guidance from - not imitating - the offline {data}, LOGO orients its policy in the manner of the sub-optimal {policy}, while yet being able to learn beyond and approach optimality.  We provide a theoretical analysis of our algorithm, and provide a lower bound on the performance improvement in each learning episode.  We also extend our algorithm to the even more challenging incomplete observation setting, where the demonstration data contains only a censored version of the true state observation.  We demonstrate the superior performance of our algorithm over state-of-the-art approaches on a number of  benchmark environments with sparse rewards {and censored state}.  Further, we demonstrate the value of our approach via implementing LOGO on a mobile robot for trajectory tracking and obstacle avoidance, where it shows excellent performance.", "ratings": "[8.0, 8.0, 6.0, 6.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "OpC-9aBBVJe", "title": "Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier", "keywords": "['reinforcement learning', 'sample efficiency', 'resets']", "abstract": "Increasing the replay ratio, the number of updates of an agent's parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms. In this work, we show that fully or partially resetting the parameters of deep reinforcement learning agents causes better replay ratio scaling capabilities to emerge. We push the limits of the sample efficiency of carefully-modified algorithms by training them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k and DeepMind Control Suite benchmarks. We then provide an analysis of the design choices required for favorable replay ratio scaling to be possible and discuss inherent limits and tradeoffs.", "ratings": "[8, 8, 8]", "confidences": "[4, 4, 5]", "decision": "Accept (Spotlight)", "year": "2023"}
{"paper_id": "HgLO8yalfwc", "title": "Regularized Inverse Reinforcement Learning", "keywords": "inverse reinforcement learning, reward learning, regularized markov decision processes, reinforcement learning", "abstract": "Inverse Reinforcement Learning (IRL) aims to facilitate a learner's ability to imitate expert behavior by acquiring reward functions that explain the expert's decisions. Regularized IRL applies convex regularizers to the learner's policy in order to avoid degenerate solutions, i.e., whereby any constant reward rationalizes the expert behavior. We propose analytical solutions, and practical methods to obtain them, for regularized IRL. Current methods are restricted to the maximum-entropy IRL framework, limiting them to Shannon-entropy regularizers, as well as proposing functional-form solutions that are generally intractable. We present theoretical backing for our proposed IRL method's applicability for both discrete and continuous controls, empirically validating our performance on a variety of tasks.", "ratings": "[7.0, 8.0, 6.0, 7.0, 6.0]", "decision": "Accept (Spotlight)", "year": "2021"}
{"paper_id": "sRZ3GhmegS", "title": "CoBERL: Contrastive BERT for Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Contrastive Learning', 'Representation Learning', 'Transformer', 'Deep Reinforcement Learning']", "abstract": "Many reinforcement learning (RL) agents require a large amount of experience to solve tasks. We propose Contrastive BERT for RL (COBERL), an agent that combines a new contrastive loss and a hybrid LSTM-transformer architecture to tackle the challenge of improving data efficiency. COBERL enables efficient and robust learning from pixels across a wide variety of domains. We use bidirectional masked prediction in combination with a generalization of a recent contrastive method to learn better representations for RL, without the need of hand engineered data augmentations. We find that COBERL consistently improves data efficiency across the full Atari suite, a set of control tasks and a challenging 3D environment, and often it also increases final score performance.", "ratings": "[8.0, 8.0, 6.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "KxbhdyiPHE", "title": "Learning Altruistic Behaviours in Reinforcement Learning without External Rewards", "keywords": "['reinforcement learning', 'altruistic behavior in AI', 'multi-agent systems']", "abstract": "Can artificial agents learn to assist others in achieving their goals without knowing what those goals are? Generic reinforcement learning agents could be trained to behave altruistically towards others by rewarding them for altruistic behaviour, i.e., rewarding them for benefiting other agents in a given situation. Such an approach assumes that other agents' goals are known so that the altruistic agent can cooperate in achieving those goals. However, explicit knowledge of other agents' goals is often difficult to acquire. In the case of human agents, their goals and preferences may be difficult to express fully, may be ambiguous or even contradictory. Thus, it is beneficial to develop agents that do not depend on external supervision and can learn altruistic behaviour in a task-agnostic manner. We propose to act altruistically towards other agents by giving them more choice and thereby allowing them to better achieve their goals. Some concrete examples include opening a door for others or safeguarding them to pursue their objectives without interference. We formalize this concept and propose an altruistic agent that learns to increase the choices another agent has by preferring to maximize the number of states that the other agent can reach in its future. We evaluate our approach on three different multi-agent environments where another agent's success depends on the altruistic agent's behaviour. Finally, we show that our unsupervised agents can perform comparably to agents explicitly trained to work cooperatively,         in some cases even outperforming them.", "ratings": "[6.0, 6.0, 8.0, 6.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "v9c7hr9ADKx", "title": "UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers", "keywords": "Multi-agent Reinforcement Learning, Transfer Learning", "abstract": "Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).", "ratings": "[6.0, 9.0, 7.0]", "decision": "Accept (Spotlight)", "year": "2021"}
{"paper_id": "7b4zxUnrO2N", "title": "Possibility Before Utility: Learning And Using Hierarchical Affordances", "keywords": "['RL', 'HRL', 'reinforcement learning', 'hierarchical reinforcement learning', 'affordances', 'hierarchical affordances']", "abstract": "Reinforcement learning algorithms struggle on tasks with complex hierarchical dependency structures. Humans and other intelligent agents do not waste time assessing the utility of every high-level action in existence, but instead consider only ones they deem possible in the first place. By focusing only on what is feasible, or \"afforded'', at the present moment, an agent can spend more time both evaluating the utility of and acting on what matters. To this end, we present Hierarchical Affordance Learning (HAL), a method that learns a model of hierarchical affordances in order to prune impossible subtasks for more effective learning. Existing works in hierarchical reinforcement learning provide agents with structural representations of subtasks but are not affordance-aware, and by grounding our definition of hierarchical affordances in the present state, our approach is more flexible than the multitude of approaches that ground their subtask dependencies in a symbolic history. While these logic-based methods often require complete knowledge of the subtask hierarchy, our approach is able to utilize incomplete and varying symbolic specifications. Furthermore, we demonstrate that relative to non-affordance-aware methods, HAL agents are better able to efficiently learn complex tasks, navigate environment stochasticity, and acquire diverse skills in the absence of extrinsic supervision---all of which are hallmarks of human learning.", "ratings": "[8.0, 8.0, 8.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "hy0a5MMPUv", "title": "In-context Reinforcement Learning with Algorithm Distillation", "keywords": "['Reinforcement Learning', 'Transformers', 'Learning to Learn', 'Large Language Models']", "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.", "ratings": "[5, 6, 8, 10]", "confidences": "[5, 4, 5, 4]", "decision": "Accept (Spotlight)", "year": "2023"}
{"paper_id": "7F9cOhdvfk_", "title": "$\\mathrm{SO}(2)$-Equivariant Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Equivariance', 'Robotic Manipulation']", "abstract": "Equivariant neural networks enforce symmetry within the structure of their convolutional layers, resulting in a substantial improvement in sample efficiency when learning an equivariant or invariant function. Such models are applicable to robotic manipulation learning which can often be formulated as a rotationally symmetric problem. This paper studies equivariant model architectures in the context of $Q$-learning and actor-critic reinforcement learning. We identify equivariant and invariant characteristics of the optimal $Q$-function and the optimal policy and propose equivariant DQN and SAC algorithms that leverage this structure. We present experiments that demonstrate that our equivariant versions of DQN and SAC can be significantly more sample efficient than competing algorithms on an important class of robotic manipulation problems.", "ratings": "[8.0, 8.0, 8.0, 6.0, 5.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "AUGBfDIV9rL", "title": "Emergent Communication at Scale", "keywords": "['emergent communication', 'multi-agent reinforcement learning', 'representation learning']", "abstract": "Emergent communication aims for a better understanding of human language evolution and building more efficient representations. We posit that reaching these goals will require scaling up, in contrast to a significant amount of literature that focuses on setting up small-scale problems to tease out desired properties of the emergent languages. We focus on three independent aspects to scale up, namely the dataset, task complexity, and population size. We provide a first set of results for large populations solving complex tasks on realistic large-scale datasets, as well as an easy-to-use codebase to enable further experimentation. In more complex tasks and datasets, we find that RL training can become unstable, but responds well to established stabilization techniques.         We also identify the need for a different metric than topographic similarity, which does not correlate with the generalization performances when working with natural images. In this context, we probe ease-of-learnability and transfer methods to assess emergent languages. Finally, we observe that larger populations do not induce robust emergent protocols with high generalization performance, leading us to explore different ways to leverage population, through voting and imitation learning.", "ratings": "[8.0, 8.0, 8.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "Zeb5mTuqT5", "title": "Confidence-Conditioned Value Functions for Offline Reinforcement Learning", "keywords": "['reinforcement learning', 'offline reinforcement learning', 'ensembles', 'adaptation']", "abstract": "Offline reinforcement learning (RL) promises the ability to learn effective policies solely using existing, static datasets, without any costly online interaction. To do so, offline RL methods must handle distributional shift between the dataset and the learned policy. The most common approach is to learn conservative, or lower-bound, value functions, which underestimate the return of OOD actions. However, such methods exhibit one notable drawback: policies optimized on such value functions can only behave according to a fixed, possibly suboptimal, degree of conservatism. However, this can be alleviated if we instead are able to learn policies for varying degrees of conservatism at training time and devise a method to dynamically choose one of them during evaluation. To do so, in this work, we propose learning value functions that additionally condition on the degree of conservatism, which we dub confidence-conditioned value functions. We derive a new form of a Bellman backup that simultaneously learns Q-values for any degree of confidence with high probability. By conditioning on confidence, our value functions enable adaptive strategies during online evaluation by controlling for confidence level using the history of observations thus far. This approach can be implemented in practice by conditioning the Q-function from existing conservative algorithms on the confidence. We theoretically show that our learned value functions produce conservative estimates of the true value at any desired confidence. Finally, we empirically show that our algorithm outperforms existing conservative offline RL algorithms on multiple discrete control domains. ", "ratings": "[5, 6, 8, 6]", "confidences": "[5, 2, 3, 3]", "decision": "Accept (Spotlight)", "year": "2023"}
{"paper_id": "ZkC8wKoLbQ7", "title": "Understanding and Preventing Capacity Loss in Reinforcement Learning", "keywords": "['Reinforcement learning', 'representation learning']", "abstract": "The reinforcement learning (RL) problem is rife with sources of non-stationarity that can destabilize or inhibit learning progress.         We identify a key mechanism by which this occurs in agents using neural networks as function approximators: \\textit{capacity loss}, whereby networks trained to predict a sequence of target values lose their ability to quickly fit new functions over time.         We demonstrate that capacity loss occurs in a broad range of RL agents and environments, and is particularly damaging to learning progress in sparse-reward tasks. We then present a simple regularizer, Initial Feature Regularization (InFeR), that mitigates this phenomenon by regressing a subspace of features towards its value at initialization, improving performance over a state-of-the-art model-free algorithm in the Atari 2600 suite. Finally, we study how this regularization affects different notions of capacity and evaluate other mechanisms by which it may improve performance.", "ratings": "[8.0, 8.0, 6.0, 3.0]", "decision": "Accept (Spotlight)", "year": "2022"}
