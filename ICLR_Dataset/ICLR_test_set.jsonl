{"paper_id": "fUhxuop_Q1r", "title": "Disentangling Generalization in Reinforcement Learning", "keywords": "['Reinforcement learning', 'generalization']", "abstract": "  Generalization in Reinforcement Learning (RL) is usually measured according to           concepts from supervised learning. Unlike a supervised learning model however,           an RL agent must generalize across states, actions and observations from           limited reward-based feedback. We propose to measure an RL agent's capacity to           generalize by evaluating it in a contextual decision process that combines a           tabular environment with observations from a supervised learning dataset. The           resulting environment, while simple, necessitates function approximation for           state abstraction and provides ground-truth labels for optimal policies and           value functions. The ground truth labels provided by our environment enable us           to characterize generalization in RL across different axes: state-space,           observation-space and action-space. Putting this method to work, we combine           the MNIST dataset with various gridworld environments to rigorously evaluate           generalization of DQN and QR-DQN in state, observation and action spaces for           both online and offline learning. Contrary to previous reports about common           regularization methods, we find that dropout does not improve observation           generalization. We find, however, that dropout improves action generalization.           Our results also corroborate recent findings that QR-DQN is able to generalize           to new observations better than DQN in the offline setting. This success does           not extend to state generalization, where DQN is able to generalize better           than QR-DQN. These findings demonstrate the need for careful consideration           of generalization in RL, and we hope that this line of research will continue           to shed light on generalization claims in the literature.", "ratings": "[5.0, 3.0, 5.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "kB8DkEKSDH", "title": "Hellinger Distance Constrained Regression", "keywords": "offline, Reinforcement Learning, off-policy, control", "abstract": "This paper introduces the off-policy reinforcement learning method that uses the Hellinger distance between sampling policy and current policy as a constraint. Hellinger distance squared multiplied by two is greater than or equal to total variation distance squared and less than or equal to Kullback-Leibler divergence, therefore lower bound for expected discounted return for the new policy is improved comparing to KL. Also, Hellinger distance is less than or equal to 1, so there is a policy-independent lower bound for expected discounted return. HDCR is capable of training with Experience Replay, a common setting for distributed RL when collecting trajectories using different policies and learning from this data centralized. HDCR shows results comparable to or better than Advantage-weighted Behavior Model and Advantage-Weighted Regression on MuJoCo tasks using offline datasets collected by random agents and datasets obtained during the first iterations of online training of HDCR agent.", "ratings": "[5.0, 4.0, 3.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "nVYND1kLOug", "title": "Example-based Planning via Dual Gradient Fields", "keywords": "['Example-based Planning', 'Score-Matching', 'Path Planning', 'Reinforcement Learning']", "abstract": "Path planning is one of the key abilities of an intelligent agent. However, both the learning-based and sample-based planners remain to require explicitly defining the task by manually designing the reward function or optimisation objectives, which limits the scope of implementation. Formulating the path planning problem from a new perspective, Example-based planning is to find the most efficient path to increase the likelihood of the target distribution by giving a set of target examples. In this work, we introduce Dual Gradient Fields (DualGFs), an offline-learning example-based planning framework built upon score matching. There are two gradient fields in DualGFs: a target gradient field that guides task completion and a support gradient field that ensures moving with environmental constraints. In the learning process, instead of interacting with the environment, the agents are trained with two offline examples, i.e., the target gradients and support gradients are trained by target examples and support examples, respectively. The support examples are randomly sampled from free space, e.g., states without collisions. DualGF is a weighted mixture of the two fields, combining the merits of the two fields together. To update the mixing ratio adaptively, we further propose a fields-balancing mechanism based on Lagrangian-Relaxation. Experimental results across four tasks (navigation, tracking, particle rearrangement, and room rearrangement) demonstrate the scalability and effectiveness of our method.", "ratings": "[6, 5, 8, 3]", "confidences": "[4, 3, 3, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "jcN7a3yZeQc", "title": "Decorrelated Double Q-learning", "keywords": "q-learning, control variates, reinforcement learning", "abstract": "Q-learning with value function approximation may have the poor performance because of overestimation bias and imprecise estimate. Specifically, overestimation bias is from the maximum operator over noise estimate, which is exaggerated using the estimate of a subsequent state. Inspired by the recent advance of deep reinforcement learning and Double Q-learning, we introduce the decorrelated double Q-learning (D2Q). Specifically, we introduce the decorrelated regularization item to reduce the correlation between value function approximators, which can lead to less biased estimation and low variance. The experimental results on a suite of MuJoCo continuous control tasks demonstrate that our decorrelated double Q-learning can effectively improve the performance.", "ratings": "[5.0, 3.0, 3.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "nhnJ3oo6AB", "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers", "keywords": "['Reinforcement Learning', 'Robotics', 'Locomotion Control', 'Multi-Modal Transformer']", "abstract": "We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method that leverages both proprioceptive states and visual observations for locomotion control. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We transfer our learned policy from simulation to a real robot by running it indoor and in-the-wild with unseen obstacles and terrain. Our method not only significantly improves over baselines, but also achieves far better generalization performance, especially when transferred to the real robot. Our project page with videos is at https://LocoTransformer.github.io/.", "ratings": "[8.0, 6.0, 8.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "B6YDcqpMk30", "title": "PRIMA: Planner-Reasoner Inside a Multi-task Reasoning Agent", "keywords": "['inductive logic programming', 'logic reasoning', 'first-order logic', 'reinforcement learning', 'Monte Carlo tree search']", "abstract": "In multi-task reasoning (MTR), an agent can solve multiple tasks via (first-order) logic reasoning. This capability is essential for human-like intelligence due to its strong generalizability and simplicity for handling multiple tasks. However, a major challenge in developing effective MTR is the intrinsic conflict between reasoning capability and efficiency. An MTR-capable agent must master a large set of \"skills'' to perform diverse tasks, but executing a particular task at the inference stage requires only a small subset of immediately relevant skills. How can we maintain broad reasoning capability yet efficient specific-task performance? To address this problem, we propose a Planner-Reasoner framework capable of state-of-the-art MTR capability and high efficiency. The Reasoner models shareable (first-order) logic deduction rules, from which the Planner selects a subset to compose into efficient reasoning paths. The entire model is trained in an end-to-end manner using deep reinforcement learning, and experimental studies over various domains validate its effectiveness.", "ratings": "[5.0, 6.0, 6.0, 6.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "0GhVG1de-Iv", "title": "Stability and Generalisation in Batch Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Algorithmic Stability', 'Generalisation', 'Overfitting', 'Target Network', 'Fitted TD', 'Off-Policy', 'Batch Reinforcement Learning']", "abstract": "Overfitting has been recently acknowledged as a key limiting factor in the capabilities of reinforcement learning algorithms, despite little theoretical characterisation. We provide a theoretical examination of overfitting in the context of batch reinforcement learning, through the fundamental relationship between algorithmic stability (Bousquet & Elisseeff, 2002)\u2013which characterises the effect of a change at a single data point\u2013and the generalisation gap\u2013which quantifies overfitting. Examining a popular fitted policy evaluation method with linear value function approximation, we characterise the dynamics of overfitting in the RL context. We provide finite sample, finite time, polynomial bounds on the generalisation gap in RL. In addition, our approach applies to a class of algorithms which only partially fit to temporal difference errors, as is common in deep RL, rather than perfectly optimising at each step. As such, our results characterise an unexplored bias-variance trade-off in the frequency of target network updates. To do so, our work extends the stochastic gradient-based approach of Hardt et al. (2016) to the iterative methods more common in RL. We find that under regimes where learning requires few iterations, the expected temporal difference error over the dataset is representative of the true performance on the MDP, indicating that, as is the case in supervised learning, good generalisation in RL can be ensured through the use of algorithms that learn quickly.", "ratings": "[3.0, 3.0, 3.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "OJiM1R3jAtZ", "title": "AWAC: Accelerating Online Reinforcement Learning with Offline Datasets", "keywords": "reinforcement learning", "abstract": "Reinforcement learning provides an appealing formalism for learning control policies from experience. However, the classic active formulation of reinforcement learning necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings. If we can instead allow reinforcement learning to effectively use previously collected data to aid the online learning process, where the data could be expert demonstrations or more generally any prior experience, we could make reinforcement learning a substantially more practical tool. While a number of recent methods have sought to learn offline from previously collected data, it remains exceptionally difficult to train a policy with offline data and improve it further with online reinforcement learning. In this paper we systematically analyze why this problem is so challenging, and propose an algorithm that combines sample-efficient dynamic programming with maximum likelihood policy updates, providing a simple and effective framework that is able to leverage large amounts of offline data and then quickly perform online fine-tuning of reinforcement learning policies. We show that our method enables rapid learning of skills with a combination of prior demonstration data and online experience across a suite of difficult dexterous manipulation and benchmark tasks.", "ratings": "[4.0, 6.0, 6.0, 3.0, 6.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "UXPrt1ffxYD", "title": "AsymQ: Asymmetric Q-loss to mitigate overestimation bias in off-policy reinforcement learning", "keywords": "['reinforcement learning', 'estimation bias']", "abstract": "It is well-known that off-policy deep reinforcement learning algorithms suffer from overestimation bias in value function approximation. Existing methods to reduce overestimation bias often utilize multiple value function estimators. Consequently, these methods have a larger time and memory consumption. In this work, we propose a new class of policy evaluation algorithms dubbed, \\textbf{AsymQ}, that use asymmetric loss functions to train the Q-value network. Departing from the symmetric loss functions such as mean squared error~(MSE) and Huber loss on the Temporal difference~(TD) error, we adopt asymmetric loss functions of the TD-error to impose a higher penalty on overestimation error. We present one such AsymQ loss called \\textbf{Softmax MSE~(SMSE)} that can be implemented with minimal modifications to the standard policy evaluation. Empirically, we show that using SMSE loss helps reduce estimation bias, and subsequently improves policy performance when combined with standard reinforcement learning algorithms. With SMSE, even the Deep Deterministic Policy Gradients~(DDPG) algorithm can achieve performance comparable to that of state-of-the-art methods such as the Twin-Delayed DDPG (TD3) and Soft Actor Critic~(SAC) on challenging environments in the OpenAI Gym MuJoCo benchmark. We additionally demonstrate that the proposed SMSE loss can also boost the performance of Deep Q learning (DQN) in Atari games with discrete action spaces.", "ratings": "[3, 8, 5, 5]", "confidences": "[4, 4, 4, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "nO5caZwFwYu", "title": "Efficient Active Search for Combinatorial Optimization Problems", "keywords": "['heuristic search', 'combinatorial optimization', 'learning to optimize', 'reinforcement learning', 'traveling salesperson problem', 'vehicle routing problem', 'job shop scheduling problem']", "abstract": "Recently numerous machine learning based methods for combinatorial optimization problems have been proposed that learn to construct solutions in a sequential decision process via reinforcement learning. While these methods can be easily combined with search strategies like sampling and beam search, it is not straightforward to integrate them into a high-level search procedure offering strong search guidance. Bello et al. (2016) propose active search, which adjusts the weights of a (trained) model with respect to a single instance at test time using reinforcement learning. While active search is simple to implement, it is not competitive with state-of-the-art methods because adjusting all model weights for each test instance is very time and memory intensive. Instead of updating all model weights, we propose and evaluate three efficient active search strategies that only update a subset of parameters during the search. The proposed methods offer a simple way to significantly improve the search performance of a given model and outperform state-of-the-art machine learning based methods on combinatorial problems, even surpassing the well-known heuristic solver LKH3 on the capacitated vehicle routing problem. Finally, we show that (efficient) active search enables learned models to effectively solve instances that are much larger than those seen during training.", "ratings": "[8.0, 8.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "sKWlRDzPfd7", "title": "MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement Learning", "keywords": "['reinforcement learning', 'multi-agent learning', 'unsupervised environment design']", "abstract": "Open-ended learning methods that automatically generate a curriculum of increasingly challenging tasks serve as a promising avenue toward generally capable reinforcement learning agents. Existing methods adapt curricula independently over either environment parameters (in single-agent settings) or co-player policies (in multi-agent settings). However, the strengths and weaknesses of co-players can manifest themselves differently depending on environmental features. It is thus crucial to consider the dependency between the environment and co-player when shaping a curriculum in multi-agent domains. In this work, we use this insight and extend Unsupervised Environment Design (UED) to multi-agent environments. We then introduce Multi-Agent Environment Design Strategist for Open-Ended Learning (MAESTRO), the first multi-agent UED approach for two-player zero-sum settings. MAESTRO efficiently produces adversarial, joint curricula over both environments and co-players and attains minimax-regret guarantees at Nash equilibrium. Our experiments show that MAESTRO outperforms a number of strong baselines on competitive two-player games, spanning discrete and continuous control settings.", "ratings": "[6, 8, 6]", "confidences": "[3, 4, 3]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "4-D6CZkRXxI", "title": "Value Gradient weighted Model-Based Reinforcement Learning", "keywords": "['model-based reinforcement learning', 'reinforcment learning', 'objective mismatch', 'value function', 'sensitivity']", "abstract": "Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the \\algoNameFull (\\algoName), a novel method for value-aware model learning which improves the performance of MBRL in challenging settings, such as small model capacity and the presence of distracting state dimensions. We analyze both MLE and value-aware approaches and demonstrate how they fail to account for exploration and the behavior of function approximation when learning value-aware models and highlight the additional goals that must be met to stabilize optimization in the deep learning setting. We verify our analysis by showing that our loss function is able to achieve high returns on the Mujoco benchmark suite while being more robust than maximum likelihood based approaches.", "ratings": "[6.0, 8.0, 6.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "cwiFbXPW4G0", "title": "Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees", "keywords": "['Robustness', 'online bipartite matching', 'reinforcement learning']", "abstract": "Many real-world problems, such as online ad display, can be formulated as online bipartite matching. The crucial challenge lies in the nature of sequentially-revealed online item information, based on which we make irreversible matching decisions at each step. While numerous expert online algorithms have been proposed with bounded worst-case competitive ratios, they may not offer satisfactory performance in average cases. On the other hand, reinforcement learning (RL) has been applied to improve the average performance, but they lack robustness and can perform arbitrarily badly. In this paper, we propose a novel RL-based approach to edge-weighted online bipartite matching with robustness guarantees (LOMAR), achieving both good average-case and good worst-case performance. The key novelty of LOMAR is a new online switching operation which, based on a judiciously-designed condition to hedge against future uncertainties, decides whether to follow the expert's decision or the RL decision for each online item arrival. We prove that for any $\\rho \\in [0,1]$, LOMAR is $\\rho$-competitive against any given expert online algorithm. To improve the average performance, we train the RL policy by explicitly considering the online switching operation. Finally, we run empirical experiments to demonstrate the advantages of LOMAR compared to existing baselines.", "ratings": "[3, 5, 5]", "confidences": "[3, 5, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "LtgEkhLScK3", "title": "Probabilistic Mixture-of-Experts for Efficient Deep Reinforcement Learning", "keywords": "Deep Reinforcement Learning, Sample Efficiency, Gaussian Mixture Models, Mixture-of-Experts", "abstract": "Deep reinforcement learning (DRL) has successfully solved various problems recently, typically with a unimodal policy representation. However, grasping the decomposable and hierarchical structures within a complex task can be essential for further improving its learning efficiency and performance, which may lead to a multimodal policy or a mixture-of-experts (MOE). To our best knowledge, present DRL algorithms for general utility do not deploy MOE methods as policy function approximators due to the lack of differentiability, or without explicit probabilistic representation. In this work, we propose a differentiable probabilistic mixture-of-experts (PMOE) embedded in the end-to-end training scheme for generic off-policy and on-policy algorithms using stochastic policies, e.g., Soft Actor-Critic (SAC) and Proximal Policy Optimisation (PPO). Experimental results testify the advantageous performance of our method over unimodal polices and three different MOE methods, as well as a method of option frameworks, based on two types of DRL algorithms. We also demonstrate the distinguishable primitives learned with PMOE in different environments.", "ratings": "[6.0, 3.0, 6.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "JiNvAGORcMW", "title": "Cross-State Self-Constraint for Feature Generalization in Deep Reinforcement Learning", "keywords": "reinforcement learning, generalization, regularization", "abstract": "Representation learning on visualized input is an important yet challenging task for deep reinforcement learning (RL). The feature space learned from visualized input not only dominates the agent's generalization ability in new environments but also affect the data efficiency during training. To help the RL agent learn general and discriminative representation among various states, we present cross-state self-constraint(CSSC), a novel constraint that regularizes the representation feature space by comparing similarity of different pairs of representations. Based on the representation-behavior connection derived from the agent's experience, this constraint helps reinforce the general feature recognition during the learning process and thus enhance the generalization to unseen environment. We test our proposed method on the OpenAI ProcGen benchmark and see significant improvement on generalization performance across most of ProcGen games.", "ratings": "[5.0, 5.0, 6.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "QpT9Q_NNfQL", "title": "NeurWIN: Neural Whittle Index Network for Restless Bandits via Deep RL", "keywords": "deep reinforcement learning, restless bandits, Whittle index", "abstract": "Whittle index policy is a powerful tool to obtain asymptotically optimal solutions for the notoriously intractable problem of restless bandits. However, finding the Whittle indices remains a difficult problem for many practical restless bandits with convoluted transition kernels. This paper proposes NeurWIN, a neural Whittle index network that seeks to learn the Whittle indices for any restless bandits by leveraging mathematical properties of the Whittle indices. We show that a neural network that produces the Whittle index is also one that produces the optimal control for a set of Markov decision problems. This property motivates using deep reinforcement learning for the training of NeurWIN. We demonstrate the utility of NeurWIN by evaluating its performance for three recently studied restless bandit problems. Our experiment results show that the performance of NeurWIN is either better than, or as good as, state-of-the-art policies for all three problems.", "ratings": "[4.0, 7.0, 7.0, 4.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "lpkGn3k2YdD", "title": "Learning Long-Term Reward Redistribution via Randomized Return Decomposition", "keywords": "['Reinforcement Learning', 'Long-Term Credit Assignment', 'Reward Redistribution', 'Return Decomposition']", "abstract": "Many practical applications of reinforcement learning require agents to learn from sparse and delayed rewards. It highlights a fundamental problem called long-term temporal credit assignment that challenges the ability of agents to tackle delayed environmental feedback. In this paper, we consider the problem formulation of episodic reinforcement learning with trajectory feedback. It refers to an extreme delay of reward signals, in which the agent can only obtain one reward signal at the end of each trajectory. A popular paradigm for this problem setting is learning with a designed auxiliary dense reward function, namely proxy reward, instead of sparse environmental signals. Based on this framework, this paper proposes a novel reward redistribution algorithm, randomized return decomposition (RRD), to learn a proxy reward function for episodic reinforcement learning. We establish a surrogate problem by Monte-Carlo sampling that scales up regression-based reward redistribution to long-horizon problems. We analyze our surrogate loss function by connection with existing methods in the literature, which illustrates the algorithmic properties of our approach. In experiments, we extensively evaluate our proposed method on a variety of benchmark tasks with episodic rewards and demonstrate substantial improvement over baseline algorithms.", "ratings": "[8.0, 8.0, 5.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "twv2QlJhXzo", "title": "Imitation Learning from Observations under Transition Model Disparity", "keywords": "['Imitation Learning', 'Deep Reinforcement Learning']", "abstract": "Learning to perform tasks by leveraging a dataset of expert observations, also known as imitation learning from observations (ILO), is an important paradigm for learning skills without access to the expert reward function or the expert actions. We consider ILO in the setting where the expert and the learner agents operate in different environments, with the source of the discrepancy being the transition dynamics model. Recent methods for scalable ILO utilize adversarial learning to match the state-transition distributions of the expert and the learner, an approach that becomes challenging when the dynamics are dissimilar. In this work, we propose an algorithm that trains an intermediary policy in the learner environment and uses it as a surrogate expert for the learner. The intermediary policy is learned such that the state transitions generated by it are close to the state transitions in the expert dataset. To derive a practical and scalable algorithm, we employ concepts from prior work on estimating the support of a probability distribution. Experiments using MuJoCo locomotion tasks highlight that our method compares favorably to the baselines for ILO with transition dynamics mismatch.", "ratings": "[5.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "wiSgdeJ29ee", "title": "Fine-Tuning Offline Reinforcement Learning with Model-Based Policy Optimization", "keywords": "Offline Reinforcement Learning, Model-Based Reinforcement Learning, Off-policy Reinforcement Learning, uncertainty estimation", "abstract": "In offline reinforcement learning (RL), we attempt to learn a control policy from a fixed dataset of environment interactions. This setting has the potential benefit of allowing us to learn effective policies without needing to collect additional interactive data, which can be expensive or dangerous in real-world systems. However, traditional off-policy RL methods tend to perform poorly in this setting due to the distributional shift between the fixed data set and the learned policy. In particular, they tend to extrapolate optimistically and overestimate the action-values outside of the dataset distribution. Recently, two major avenues have been explored to address this issue. First, behavior-regularized methods that penalize actions that deviate from the demonstrated action distribution. Second, uncertainty-aware model-based (MB) methods that discourage state-actions where the dynamics are uncertain. In this work, we propose an algorithmic framework that consists of two stages. In the first stage, we train a policy using behavior-regularized model-free RL on the offline dataset. Then, we can optionally enter a second stage where we fine-tune the policy using our novel Model-Based Behavior-Regularized Policy Optimization (MB2PO) algorithm. We demonstrate that for certain tasks and dataset distributions our conservative model-based fine tuning can greatly increase performance and allow the agent to generalize and outperform the demonstrated behavior. We evaluate our method on a variety of the Gym-MuJoCo tasks in the D4RL benchmark and demonstrate that our method is competitive and in some cases superior to the state of the art for most of the evaluated tasks.", "ratings": "[4.0, 5.0, 4.0, 5.0, 3.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "rYgeBuEHlh", "title": "Adversarial Cheap Talk", "keywords": "['Meta-Learning', 'Reinforcement Learning', 'Meta-Reinforcement Learning']", "abstract": "Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim\u2019s parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim\u2019s observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim\u2019s actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT can still significantly influence the Victim\u2019s training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorithms. More specifically, we show that an ACT Adversary is capable of harming performance by interfering with the learner\u2019s function approximation, or instead helping the Victim\u2019s performance by outputting useful features. Finally, we show that an ACT Adversary can manipulate messages during train-time to directly and arbitrarily control the Victim at test-time.", "ratings": "[6, 6, 5, 8]", "confidences": "[4, 3, 5, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "Ud3DSz72nYR", "title": "Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions", "keywords": "Explainable AI, Deep Reinforcement Learning", "abstract": "We investigate a deep reinforcement learning (RL) architecture that supports explaining why a learned agent prefers one action over another. The key idea is to learn action-values that are directly represented via human-understandable properties of expected futures. This is realized via the embedded self-prediction (ESP) model, which learns said properties in terms of human provided features. Action preferences can then be explained by contrasting the future properties predicted for each action. To address cases where there are a large number of features, we develop a novel method for computing minimal sufficient explanations from an ESP. Our case studies in three domains, including a complex strategy game, show that ESP models can be effectively learned and support insightful explanations.", "ratings": "[7.0, 8.0, 7.0]", "decision": "Accept (Oral)", "year": "2021"}
{"paper_id": "3M3t3tUbA2Y", "title": "DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations", "keywords": "['model-based reinforcement learning', 'representation learning']", "abstract": "In model-based reinforcement learning (MBRL) such as Dreamer, the approaches based on observation reconstruction         often fail to discard task-irrelevant details, thus struggling to handle visual distractions or generalize to unseen distractions. To address this issue, previous work has proposed to contrastively learn the latent representations and its temporal dynamics, but showed inconsistent performance, often worse than Dreamer. Although, in computer vision, an alternative prototypical approach has often shown to be more accurate and robust, it is elusive how this approach can be combined best with the temporal dynamics learning in MBRL. In this work, we propose a reconstruction-free MBRL agent, called DreamerPro, to achieve this goal. Similar to SwAV, by encouraging uniform cluster assignment across the batch, we implicitly push apart the embeddings of different observations. Additionally, we let the temporal latent state to 'reconstruct' the cluster assignment of the observation, thereby relieving the world model from modeling low-level details. We evaluate our model on the standard setting of DeepMind Control Suite, and also on a natural background setting, where the background is replaced by natural videos irrelevant to the task. The results show that the proposed model is consistently better than the previous models.", "ratings": "[3.0, 5.0, 6.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "VMtftZqMruq", "title": "Towards Understanding Linear Value Decomposition in Cooperative Multi-Agent Q-Learning", "keywords": "Multi-agent reinforcement learning, Fitted Q-iteration, Value factorization, Credit assignment", "abstract": "Value decomposition is a popular and promising approach to scaling up multi-agent reinforcement learning in cooperative settings. However, the theoretical understanding of such methods is limited. In this paper, we introduce a variant of the fitted Q-iteration framework for analyzing multi-agent Q-learning with value decomposition. Based on this framework, we derive a closed-form solution to the empirical Bellman error minimization with linear value decomposition. With this novel solution, we further reveal two interesting insights: 1) linear value decomposition implicitly implements a classical multi-agent credit assignment called counterfactual difference rewards; and 2) On-policy data distribution or richer Q function classes can improve the training stability of multi-agent Q-learning. In the empirical study, our experiments demonstrate the realizability of our theoretical closed-form formulation and implications in the didactic examples and a broad set of StarCraft II unit micromanagement tasks, respectively.", "ratings": "[5.0, 5.0, 6.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "7qmQNB6Wn_B", "title": "Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration", "keywords": "Reinforcement Learning, Entropy Regularization, Exploration", "abstract": "Policy entropy regularization is commonly used for better exploration in deep reinforcement learning (RL). However, policy entropy regularization is sample-inefficient in off-policy learning since it does not take the distribution of previous samples stored in the replay buffer into account. In order to take advantage of the previous sample distribution from the replay buffer for sample-efficient exploration, we propose sample-aware entropy regularization which maximizes the entropy of weighted sum of the policy action distribution and the sample action distribution from the replay buffer. We formulate the problem of sample-aware entropy regularized policy iteration,  prove its convergence, and provide a practical algorithm named diversity actor-critic (DAC) which is a generalization of soft actor-critic (SAC). Numerical results show that DAC significantly outperforms SAC baselines and other state-of-the-art RL algorithms.", "ratings": "[5.0, 5.0, 6.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "LGkmUauBUL", "title": "Distributional Meta-Gradient Reinforcement Learning", "keywords": "['Reinforcement Learning', 'Meta Learning']", "abstract": "Meta-gradient reinforcement learning (RL) algorithms have substantially boosted the performance of RL agents by learning an adaptive return. All the existing algorithms adhere to the same reward learning principle, where the adaptive return is simply formulated in the form of expected cumulative rewards, upon which the policy and critic update rules are specified under well-adopted distance metrics. In this paper, we present a novel algorithm that builds on the success of meta-gradient RL algorithms and effectively improves such algorithms by following a simple recipe, i.e., going beyond the expected return to formulate and learn the return in a more expressive form, value distributions. To this end, we first formulate a distributional return that could effectively capture bootstrapping and discounting behaviors over distributions, to form an informative distributional return target in value update. Then we derive an efficient meta update rule to learn the adaptive distributional return with meta-gradients. For empirical evaluation, we first present an illustrative example on a toy two-color grid-world domain, which validates the benefit of learning distributional return over expectation; then we conduct extensive comparisons on a large-scale RL benchmark Atari 2600, where we confirm that our proposed method with distributional return works seamlessly well with the actor-critic framework and leads to state-of-the-art median human normalized score among meta-gradient RL literature.", "ratings": "[6, 6, 8, 6]", "confidences": "[4, 4, 4, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "T_8wHvOkEi9", "title": "Self-Organized Polynomial-time Coordination Graphs", "keywords": "['Multi-Agent Reinforcement Learning', 'Coordination Graphs', 'Polynomial-time DCOP']", "abstract": "Coordination graph is a promising approach to model agent collaboration in multi-agent reinforcement learning. It factorizes a large multi-agent system into a suite of overlapping groups that represent the underlying coordination dependencies. One critical challenge in this paradigm is the complexity of computing maximum-value actions for a graph-based value factorization. It refers to the decentralized constraint optimization problem (DCOP), which and whose constant-ratio approximation are NP-hard problems. To bypass this fundamental hardness, this paper proposes a novel method, named Self-Organized Polynomial-time Coordination Graphs (SOP-CG), which uses structured graph classes to guarantee the optimality of the induced DCOPs with sufficient function expressiveness. We extend the graph topology to be state-dependent, formulate the graph selection as an imaginary agent, and finally derive an end-to-end learning paradigm from the unified Bellman optimality equation. In experiments, we show that our approach learns interpretable graph topologies, induces effective coordination, and improves performance across a variety of cooperative multi-agent tasks.", "ratings": "[3.0, 3.0, 8.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "HumfPzF2yeI", "title": "Learning Rewards and Skills to Follow Commands with a Data Efficient Visual-Audio Representation", "keywords": "['Robotics', 'Representation Learning', 'Reinforcement Learning']", "abstract": "Based on the recent advancements in representation learning, we propose a novel framework for command-following robots with raw sensor inputs. Previous RL-based methods are either difficult to continuously improve after the deployment or require a large number of new labels during the fine-tuning. Motivated by (self-)supervised contrastive learning literature, we propose a novel representation, named VAR++, that generates an intrinsic reward function for command-following robot tasks by associating images with sound commands. After the robot is deployed in a new domain, the representation can be updated intuitively and data-efficiently by non-expert, and the robots are able to fulfill sound commands without any hand-crafted reward functions. We demonstrate our approach to various sound types and robotic tasks, including navigation and manipulation with raw sensor inputs. In the simulated experiments, we show that our system can continually self-improve in previously unseen scenarios given fewer new labeled data, yet achieves better performance, compared with previous methods. ", "ratings": "[6, 6, 5]", "confidences": "[3, 3, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "Cx1xYn6vVm2", "title": "A Mutual Information Duality Algorithm for Multi-Agent Specialization", "keywords": "['Multi-agent', 'Reinforcement Learning', 'Mutual Information', 'Duality', 'Policy Gradient', 'Social Graph']", "abstract": "The social behavior change in a population has long been studied as an essential component of multi-agent learning. The learning of behavioral change not only involves reinforcement learning (RL), but also be measured against the general population with mutual information (MI). The combination of RL and MI led us to derive MI optimizations from policy gradient. With MI as multi-agent's optimization objective, we discover that the dual properties of MI can result in distinctly different population behaviors. From MI maximization that maximizes the stability of a population to MI minimization that enables specialization among the agents, the dual of MI creates a significant change in a population's behavioral properties. In this paper, we propose a minimax formulation of MI (M\\&M) that enables agents specialization with stable regularization. Empirically we evaluated M\\&M against the prior SOTA MARL framework, and analyze the social behavior change in performance, diversity, and the stability of their social graphs. ", "ratings": "[3, 6, 5, 6, 6, 5, 3, 3]", "confidences": "[3, 2, 4, 3, 2, 3, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "-HSOjDPfhBJ", "title": "PER-ETD: A Polynomially Efficient Emphatic Temporal Difference Learning Method", "keywords": "['emphatic temporal difference', 'finite-time analysis', 'off-policy evaluation', 'reinforcement learning']", "abstract": "Emphatic temporal difference (ETD) learning (Sutton et al., 2016) is a successful method to conduct the off-policy value function evaluation with function approximation. Although ETD has been shown to converge asymptotically to a desirable value function, it is well-known that ETD often encounters a large variance so that its sample complexity can increase exponentially fast with the number of iterations. In this work, we propose a new ETD method, called PER-ETD (i.e., PEriodically Restarted-ETD), which restarts and updates the follow-on trace only for a finite period for each iteration of the evaluation parameter. Further, PER-ETD features a design of the logarithmical increase of the restart period with the number of iterations, which guarantees the best trade-off between the variance and bias and keeps both vanishing sublinearly. We show that PER-ETD converges to the same desirable fixed point as ETD, but improves the exponential sample complexity of ETD to be polynomials. Our experiments validate the superior performance of PER-ETD and its advantage over ETD.", "ratings": "[8.0, 8.0, 8.0, 8.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "bMzj6hXL2VJ", "title": "Ordering-Based Causal Discovery with Reinforcement Learning", "keywords": "Causal Discovery, Reinforcement Learning, Ordering Search", "abstract": "It is a long-standing question to discover causal relations among a set of variables in many empirical sciences. Recently, Reinforcement Learning (RL) has achieved promising results in causal discovery. However, searching the space of directed graphs directly and enforcing acyclicity by implicit penalties tend to be inefficient and restrict the method to the small problems. In this work, we alternatively consider searching an ordering by RL from the variable ordering space that is much smaller than that of directed graphs, which also helps avoid dealing with acyclicity. Specifically, we formulate the ordering search problem as a Markov decision process, and then use different reward designs to optimize the ordering generating model. A generated ordering is then processed using variable selection methods to obtain the final directed acyclic graph. In contrast to other causal discovery methods, our method can also utilize a pretrained model to accelerate training. We conduct experiments on both synthetic and real-world datasets, and show that the proposed method outperforms other baselines on important metrics even on large graph tasks.", "ratings": "[5.0, 5.0, 5.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "pXmtZdDW16", "title": "Embedding a random graph via GNN: mean-field inference theory and RL applications to NP-Hard multi-robot/machine scheduling", "keywords": "Graph neural network, graph embedding, multi-robot/machine scheduling, Reinforcement learning, Mean-field inference", "abstract": "We develop a theory for embedding a random graph using graph neural networks (GNN) and illustrate its capability to solve NP-hard scheduling problems. We apply the theory to address the challenge of developing a near-optimal learning algorithm to solve the NP-hard problem of scheduling multiple robots/machines with time-varying rewards. In particular, we consider a class of reward collection problems called Multi-Robot Reward Collection (MRRC). Such MRRC problems well model ride-sharing, pickup-and-delivery, and a variety of related problems. We consider the classic identical parallel machine scheduling problem (IPMS) in the Appendix.                For the theory, we first observe that MRRC system state can be represented as an extension of probabilistic graphical models (PGMs), which we refer to as random PGMs. We then develop a mean-field inference method for random PGMs.        We prove that a simple modification of a typical GNN embedding is sufficient to embed a random graph even when the edge presence probabilities are interdependent.              Our theory enables a two-step hierarchical inference for precise and transferable Q-function estimation for MRRC and IPMS. For scalable computation, we show that the transferability of Q-function estimation enables us to design a polynomial-time algorithm with 1-1/e optimality bound.        Experimental results on solving NP-hard MRRC problems (and IMPS in the Appendix) highlight the near-optimality and transferability of the proposed methods.", "ratings": "[7.0, 5.0, 6.0, 7.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "jEYKjPE1xYN", "title": "Symmetry-Aware Actor-Critic for 3D Molecular Design", "keywords": "deep reinforcement learning, molecular design, covariant neural networks", "abstract": "Automating molecular design using deep reinforcement learning (RL) has the potential to greatly accelerate the search for novel materials. Despite recent progress on leveraging graph representations to design molecules, such methods are fundamentally limited by the lack of three-dimensional (3D) information. In light of this, we propose a novel actor-critic architecture for 3D molecular design that can generate geometrically complex molecular structures unattainable with previous approaches. This is achieved by exploiting the symmetries of the design process through a rotationally covariant state-action representation based on a spherical harmonics series expansion. We demonstrate the benefits of our approach on several 3D molecular design tasks, where we find that building in such symmetries significantly improves generalization and the quality of generated molecules.", "ratings": "[8.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "aYSlxlHKEA", "title": "Fully Decentralized Model-based Policy Optimization with Networked Agents", "keywords": "['Reinforcement learning', 'model-based', 'multi-agent', 'deep learning', 'networked system control.']", "abstract": "Model-based RL is an effective approach for reducing sample complexity. However, when it comes to multi-agent setting where the number of agent is large, the model estimation can be problematic due to the exponential increased interactions. In this paper, we propose a decentralized model-based reinforcement learning algorithm for networked multi-agent systems, where agents are cooperative and communicate locally with their neighbors. We analyze our algorithm theoretically and derive an upper bound of performance discrepancy caused by model usage, and provide a sufficient condition of monotonic policy improvement. In our experiments, we compare our algorithm against other strong multi-agent baselines and demonstrate that our algorithm not only matches the asymptotic performance of model-free methods but also largely increases its sample efficiency.", "ratings": "[5.0, 5.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "OhUAblg27z", "title": "Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting", "keywords": "['offline reinforcement learning', 'reinforcement learning', 'sampling', 'experience replay']", "abstract": "Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any offline RL algorithm. We further analyze that the opportunity for performance improvement over the behavior policy correlates with the positive-sided variance of the returns of the trajectories in the dataset. We empirically show that while CQL, IQL, and TD3+BC achieve only a part of this potential policy improvement, these same algorithms combined with our reweighted sampling strategy fully exploit the dataset. Furthermore, we empirically demonstrate that, despite its theoretical limitation, the approach may still be efficient in stochastic environments. ", "ratings": "[8, 6, 6, 6]", "confidences": "[4, 4, 4, 4]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "8eb12UQYxrG", "title": "The Role of Pretrained Representations for the OOD Generalization of RL Agents", "keywords": "['representations', 'out-of-distribution', 'generalization', 'deep learning', 'reinforcement learning']", "abstract": "Building sample-efficient agents that generalize out-of-distribution (OOD) in real-world settings remains a fundamental unsolved problem on the path towards achieving higher-level cognition. One particularly promising approach is to begin with low-dimensional, pretrained representations of our world, which should facilitate efficient downstream learning and generalization. By training 240 representations and over 10,000 reinforcement learning policies on a simulated robotic setup, we evaluate to what extent different properties of pretrained VAE-based representations affect the OOD generalization of downstream agents. We observe that many agents are surprisingly robust to realistic distribution shifts, including the challenging sim-to-real case. In addition, we find that the generalization performance of a simple downstream proxy task reliably predicts the generalization performance of our reinforcement learning control tasks under a wide range of practically relevant OOD settings. Such proxy tasks can thus be used to select pretrained representations that will lead to agents that generalize out-of-distribution.", "ratings": "[8.0, 6.0, 3.0, 5.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "9hgEG-k57Zj", "title": "Addressing Distribution Shift in Online Reinforcement Learning with Offline Datasets", "keywords": "reinforcement learning, offline reinforcement learning, control, distribution shift", "abstract": "Recent progress in offline reinforcement learning (RL) has made it possible to train strong RL agents from previously-collected, static datasets. However, depending on the quality of the trained agents and the application being considered, it is often desirable to improve such offline RL agents with further online interaction. As it turns out, fine-tuning offline RL agents is a non-trivial challenge, due to distribution shift \u2013 the agent encounters out-of-distribution samples during online interaction, which may cause bootstrapping error in Q-learning and instability during fine-tuning. In order to address the issue, we present a simple yet effective framework, which incorporates a balanced replay scheme and an ensemble distillation scheme. First, we propose to keep separate offline and online replay buffers, and carefully balance the number of samples from each buffer during updates. By utilizing samples from a wider distribution, i.e., both online and offline samples, we stabilize the Q-learning. Next, we present an ensemble distillation scheme, where we train an ensemble of independent actor-critic agents, then distill the policies into a single policy. In turn, we improve the policy using the Q-ensemble during fine-tuning, which allows the policy updates to be more robust to error in each individual Q-function. We demonstrate the superiority of our method on MuJoCo datasets from the recently proposed D4RL benchmark suite.", "ratings": "[3.0, 5.0, 4.0, 6.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "CpTuR2ECuW", "title": "LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning", "keywords": "['multi-agent', 'reinforcement learning', 'intrinsic rewards', 'exploration']", "abstract": "Efficient exploration is important for reinforcement learners (RL) to achieve high         rewards. In multi-agent systems, coordinated exploration and behaviour is critical         for agents to jointly achieve optimal outcomes. In this paper, we introduce a new         general framework for improving coordination and performance of multi-agent reinforcement learners (MARL). Our framework, named Learnable Intrinsic-Reward         Generation Selection algorithm (LIGS) introduces an adaptive learner, Generator         that observes the agents and learns to construct intrinsic rewards online that coordinate the agents\u2019 joint exploration and joint behaviour. Using a novel combination         of reinforcement learning (RL) and switching controls, LIGS determines the best         states to learn to add intrinsic rewards which leads to a highly efficient learning process. LIGS can subdivide complex tasks making them easier to solve and enables         systems of RL agents to quickly solve environments with sparse rewards. LIGS         can seamlessly adopt existing multi-agent RL algorithms and our theory shows that         it ensures convergence to joint policies that deliver higher system performance. We         demonstrate the superior performance of the LIGS framework in challenging tasks         in Foraging and StarCraft II and show LIGS is capable of tackling tasks previously         unsolvable by MARL methods.", "ratings": "[5.0, 6.0, 8.0, 5.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "t02FF6Fj5mH", "title": "Best Possible Q-Learning", "keywords": "['multi-agent reinforcement learning']", "abstract": "Fully decentralized learning, where the global information, i.e., the actions of other agents, is inaccessible, is a fundamental challenge in multi-agent reinforcement learning. However, the convergence and optimality of most decentralized algorithms are not theoretically guaranteed, since the transition probabilities are non-stationary as all agents are updating policies simultaneously. To tackle this challenge, we propose \\textit{best possible operator}, a novel decentralized operator, and prove that the policies of agents will converge to the optimal joint policy if each agent independently updates its individual state-action value by the operator. Further, to make the update more efficient and practical, we simplify the operator and prove that the convergence and optimality still hold with the simplified one. By instantiating the simplified operator, the derived fully decentralized algorithm, best possible Q-learning (BQL), does not suffer from non-stationarity. Empirically, we show that BQL achieves remarkable improvement over baselines in a variety of cooperative multi-agent tasks.", "ratings": "[3, 6, 6, 3]", "confidences": "[4, 3, 2, 4]", "decision": "Reject", "year": "2023"}
{"paper_id": "9pA3oXBwYh7", "title": "Towards biologically plausible Dreaming and Planning", "keywords": "['Reinforcement Learning', 'Model based', 'Biologically Plausible']", "abstract": "Humans and animals can learn new skills after practicing for a few hours, while current reinforcement learning algorithms require a large amount of data to achieve good performances. Recent model-based approaches show promising results by reducing the number of necessary interactions with the environment to learn a desirable policy. However, these methods require biological implausible ingredients, such as the detailed storage of older experiences, and long periods of offline learning. The optimal way to learn and exploit word-models is still an open question. Taking inspiration from biology, we suggest that dreaming might be an efficient expedient to use an inner model. We propose a two-module (agent and model) neural network in which \"dreaming\" (living new experiences in a model-based simulated environment) significantly boosts learning. We also explore \"planning\", an online alternative to dreaming, that shows comparable performances. Importantly, our model does not require the detailed storage of experiences, and learns online the world-model. This is a key ingredient for biological plausibility and implementability (e.g., in neuromorphic hardware).", "ratings": "[3, 6, 3, 1]", "confidences": "[3, 3, 3, 3]", "decision": "Reject", "year": "2023"}
{"paper_id": "kWSeGEeHvF8", "title": "Benchmarks for Deep Off-Policy Evaluation", "keywords": "reinforcement learning, off-policy evaluation, benchmarks", "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both obtaining and selecting complex policies for decision making. The ability to perform evaluation offline is particularly important in many real-world domains, such as healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between works is difficult because there is currently a lack of a comprehensive and unified benchmark. Moreover, it is difficult to measure how far algorithms have progressed, due to the lack of challenging evaluation tasks. In order to address this gap, we propose a new benchmark for off-policy evaluation which includes tasks on a range of challenging, high-dimensional control problems, with wide selections of datasets and policies for performing policy selection. The goal of of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform a comprehensive evaluation of state-of-the-art algorithms, and we will provide open-source access to all data and code to foster future research in this area.", "ratings": "[6.0, 6.0, 7.0, 7.0]", "decision": "Accept (Poster)", "year": "2021"}
{"paper_id": "Jf24xdaAwF9", "title": "Self-Activating Neural Ensembles for Continual Reinforcement Learning", "keywords": "continual reinforcement learning, lifelong learning, deep reinforcement learning", "abstract": "The ability for an agent to continuously learn new skills without catastrophically forgetting existing knowledge is of critical importance for the development of generally intelligent agents. Most methods devised to address this problem depend heavily on well-defined task boundaries which simplify the problem considerably. Our task-agnostic method, Self-Activating Neural Ensembles (SANE), uses a hierarchical modular architecture designed to avoid catastrophic forgetting without making any such assumptions. At each timestep a path through the SANE tree is activated; during training only activated nodes are updated, ensuring that unused nodes do not undergo catastrophic forgetting. Additionally, new nodes are created as needed, allowing the system to leverage and retain old skills while growing and learning new ones. We demonstrate our approach on MNIST and a set of grid world environments, demonstrating that SANE does not undergo catastrophic forgetting where existing methods do.", "ratings": "[6.0, 4.0, 5.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "6Tk2noBdvxt", "title": "Programmatic Reinforcement Learning without Oracles", "keywords": "['Reinforcement Learning', 'Programmatic Reinforcement Learning', 'Compositional Reinforcement Learning', 'Program Synthesis', 'Differentiable Architecture Search']", "abstract": "Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difficulty of identifying how the model's control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two shortcomings remain: First, synthesizing programmatic policies requires optimizing over the discrete and non-differentiable search space of program architectures. Previous works are suboptimal because they only enumerate program architectures greedily guided by a pretrained RL oracle. Second, these works do not exploit compositionality, an important programming concept, to reuse and compose primitive functions to form a complex function for new tasks. Our first contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space defined by programming language grammar rules. Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efficient policy-gradient methods, and thus does not require a pretrained oracle. Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems. Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable.", "ratings": "[8.0, 8.0, 8.0]", "decision": "Accept (Spotlight)", "year": "2022"}
{"paper_id": "5y35LXrRMMz", "title": "Exploiting Minimum-Variance Policy Evaluation for Policy Optimization", "keywords": "['Reinforcement Learning', 'Policy Optimization', 'Importance Sampling', 'Variance Reduction']", "abstract": "Off-policy methods are the basis of a large number of effective Policy Optimization (PO) algorithms. In this setting, Importance Sampling (IS) is typically employed as a what-if analysis tool, with the goal of estimating the performance of a target policy, given samples collected with a different behavioral policy. However, in Monte Carlo simulation, IS represents a variance minimization approach. In this field, a suitable behavioral distribution is employed for sampling, allowing diminishing the variance of the estimator below the one achievable when sampling from the target distribution. In this paper, we analyze IS in these two guises, showing the connections between the two objectives. We illustrate that variance minimization can be used as a performance improvement tool, with the advantage, compared with direct off-policy learning, of implicitly enforcing a trust region. We make use of these theoretical findings to build a PO algorithm, Policy Optimization via Optimal Policy Evaluation (PO2PE), that employs variance minimization as an inner loop. Finally, we present empirical evaluations on continuous RL benchmarks, with a particular focus on the robustness to small batch sizes.", "ratings": "[6.0, 3.0, 10.0, 5.0]", "decision": "Reject", "year": "2022"}
{"paper_id": "4CxsUBDQJqv", "title": "Learning Intrinsic Symbolic Rewards in Reinforcement Learning", "keywords": "Reinforcement Learning, Intrinsic Rewards, Symbolic Regression", "abstract": "Learning effective policies for sparse objectives is a key challenge in Deep Reinforcement Learning (RL). A common approach is to design task-related dense rewards to improve task learnability. While such rewards are easily interpreted, they rely on heuristics and domain expertise. Alternate approaches that train neural networks to discover dense surrogate rewards avoid heuristics, but are high-dimensional, black-box solutions offering little interpretability. In this paper, we present a method that discovers dense rewards in the form of low-dimensional symbolic trees - thus making them more tractable for analysis. The trees use simple functional operators to map an agent's observations to a scalar reward, which then supervises the policy gradient learning of a neural network policy. We test our method on continuous action spaces in Mujoco and discrete action spaces in Atari and Pygames environments. We show that the discovered dense rewards are an effective signal for an RL policy to solve the benchmark tasks. Notably, we significantly outperform a widely used, contemporary neural-network based reward-discovery algorithm in all environments considered.", "ratings": "[5.0, 4.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "JtC6yOHRoJJ", "title": "Human-level Atari 200x faster", "keywords": "['Reinforcement Learning', 'Data-efficiency', 'Exploration', 'Off-policy']", "abstract": "The task of building general agents that perform well over a wide range of tasks has been an important goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 billion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline, within our novel agent MEME. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. Our contributions aim to achieve faster propagation of learning signals related to rare events, stabilize learning under differing value scales, improve the neural network architecture, and make updates more robust under a rapidly-changing policy.", "ratings": "[8, 8, 3]", "confidences": "[3, 4, 5]", "decision": "Accept (Poster)", "year": "2023"}
{"paper_id": "kavTY__jxp", "title": "Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery", "keywords": "['reinforcement learning', 'graph neural network', 'molecule generation', 'drug discovery', 'curiosity-driven policy']", "abstract": "We developed Distilled Graph Attention Policy Networks (DGAPNs), a reinforcement learning model to generate novel graph-structured chemical representations that optimize user-defined objectives by efficiently navigating a physically constrained domain. The framework is examined on the task of generating molecules that are designed to bind, noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial Graph Attention (sGAT) mechanism that leverages self-attention over both node and edge attributes as well as encoding the spatial structure --- this capability is of considerable interest in synthetic biology and drug discovery. An attentional policy network is introduced to learn the decision rules for a dynamic, fragment-based chemical environment, and state-of-the-art policy gradient techniques are employed to train the network with stability. Exploration is driven by the stochasticity of the action space design and the innovation reward bonuses learned and proposed by random network distillation. In experiments, our framework achieved outstanding results compared to state-of-the-art algorithms, while reducing the complexity of paths to chemical synthesis.", "ratings": "[8.0, 6.0, 6.0]", "decision": "Accept (Poster)", "year": "2022"}
{"paper_id": "RJkAHKp7kNZ", "title": "Vision-Based Manipulators Need to Also See from Their Hands", "keywords": "['reinforcement learning', 'observation space', 'out-of-distribution generalization', 'visuomotor control', 'robotics', 'manipulation']", "abstract": "We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits require no algorithmic changes other than the perspective and hold across a variety of learning algorithms, experimental settings, and distribution shifts, albeit only when hand-centric observability is sufficient. When this is not the case, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this simple and broadly applicable principle results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task.", "ratings": "[8.0, 8.0, 8.0]", "decision": "Accept (Oral)", "year": "2022"}
{"paper_id": "P1MaSJlwdT4", "title": "Go-Explore with a guide: Speeding up search in sparse reward settings with goal-directed intrinsic rewards", "keywords": "['reinforcement learning', 'intrinsic motivation', 'goal-directed rewards', 'hippocampal replay', 'hard-exploration', 'sparse rewards']", "abstract": "Reinforcement Learning (RL) agents have traditionally been very sample-intensive to train, especially in environments with sparse rewards. Seeking inspiration from neuroscience experiments of rats learning the structure of a maze without needing extrinsic rewards, we seek to incorporate additional intrinsic rewards to guide behavior. We propose a potential-based goal-directed intrinsic reward (GDIR), which provides a reward signal regardless of whether the task is achieved, and ensures that learning can always take place. While GDIR may be similar to approaches such as reward shaping in incorporating goal-based rewards, we highlight that GDIR is innate to the agent and hence applicable across a wide range of environments without needing to rely on a properly shaped environment reward. We also note that GDIR is different from curiosity-based intrinsic motivation, which can diminish over time and lead to inefficient exploration. Go-Explore is a well-known state-of-the-art algorithm for sparse reward domains, and we demonstrate that by incorporating GDIR in the ``Go\" and ``Explore\" phases, we can improve Go-Explore's performance and enable it to learn faster across multiple environments, for both discrete (2D grid maze environments, Towers of Hanoi, Game of Nim) and continuous (Cart Pole and Mountain Car) state spaces. Furthermore, to consolidate learnt trajectories better, our method also incorporates a novel approach of hippocampal replay to update the values of GDIR and reset state visit and selection counts of states along the successful trajectory. As a benchmark, we also show that our proposed approaches learn significantly faster than traditional extrinsic-reward-based RL algorithms such as Proximal Policy Optimization, TD-learning, and Q-learning.", "ratings": "[3, 3, 3, 1]", "confidences": "[3, 4, 2, 5]", "decision": "Reject", "year": "2023"}
{"paper_id": "8iW8HOidj1_", "title": "Dream and Search to Control: Latent Space Planning for Continuous Control", "keywords": "Reinforcement Learning, Model Based RL, Continuous Control, Search, Planning, MCTS", "abstract": "Learning and planning with latent space dynamics has been shown to be useful for sample efficiency in model-based reinforcement learning (MBRL) for discrete and continuous control tasks. In particular, recent work, for discrete action spaces, demonstrated the effectiveness of latent-space planning via Monte-Carlo Tree Search (MCTS) for bootstrapping MBRL during learning and at test time. However, the potential gains from latent-space tree search have not yet been demonstrated for environments with continuous action spaces.  In this work, we propose and explore an MBRL approach for continuous action spaces based on tree-based planning over learned latent dynamics.  We show that it is possible to demonstrate the types of bootstrapping benefits as previously shown for discrete spaces. In particular, the approach achieves improved sample efficiency and performance on a majority of challenging continuous-control benchmarks compared to the state-of-the-art.", "ratings": "[4.0, 6.0, 4.0, 5.0]", "decision": "Reject", "year": "2021"}
{"paper_id": "_xxbJ7oSJXX", "title": "Offline Reinforcement Learning with Resource Constrained Online Deployment", "keywords": "['Offline Reinforcement Learning', 'Reinforcement Learning', 'Transfer Learning', 'Knowledge Transfer', 'Resource Constraints']", "abstract": "Offline reinforcement learning is used to train policies in scenarios where real-time access to the environment is expensive or impossible.         As a natural consequence of these harsh conditions, an agent may lack the resources to fully observe the online environment before taking an action. We dub this situation the resource-constrained setting. This leads to situations where the offline dataset (available for training) can contain fully processed features (using powerful language models, image models, complex sensors, etc.) which are not available when actions are actually taken online.         This disconnect leads to an interesting and unexplored problem in offline RL: Is it possible to use a richly processed offline dataset to train a policy which has access to fewer features in the online environment?          In this work, we introduce and formalize this novel resource-constrained problem setting. We highlight the performance gap between policies trained using the full offline dataset and policies trained using limited features.          We address this performance gap with a policy transfer algorithm which first trains a teacher agent using the offline dataset where features are fully available, and then transfers this knowledge to a student agent that only uses the resource-constrained features. To better capture the challenge of this setting, we propose a data collection procedure: Resource Constrained-Datasets for RL (RC-D4RL). We evaluate our transfer algorithm on RC-D4RL and the popular D4RL benchmarks and observe consistent improvement over the baseline (TD3+BC without transfer).", "ratings": "[5.0, 6.0, 5.0, 5.0]", "decision": "Reject", "year": "2022"}
